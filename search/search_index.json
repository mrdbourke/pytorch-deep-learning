{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Learn PyTorch for Deep Learning: Zero to Mastery book","text":"<p>Welcome to the second best place on the internet to learn PyTorch (the first being the PyTorch documentation).</p> <p>This is the online book version of the Learn PyTorch for Deep Learning: Zero to Mastery course.</p> <p>This course will teach you the foundations of machine learning and deep learning with PyTorch (a machine learning framework written in Python).</p> <p>The course is video based. However, the videos are based on the contents of this online book.</p> <p>For full code and resources see the course GitHub.</p> <p>Otherwise, you can find more about the course below.</p>"},{"location":"#does-this-course-cover-pytorch-20","title":"Does this course cover PyTorch 2.0?","text":"<p>Yes. PyTorch 2.0 is an additive release to previous versions of PyTorch.</p> <p>This means it adds new features on top of the existing baseline features of PyTorch.</p> <p>This course focuses on the baseline features of PyTorch (e.g. you're a beginner wanting to get into deep learning/AI).</p> <p>Once you know the fundamentals of PyTorch, PyTorch 2.0 is a quick upgrade, there's a tutorial on this website which runs through the new features. </p>"},{"location":"#status","title":"Status","text":"<p>Course launched on ZTM Academy!</p> <ul> <li>Last update: April 16 2023</li> <li>Videos are done for chapters: 00, 01, 02, 03, 04, 05, 06, 07, 08, 09 (all chapters!)</li> <li>Currently working on: PyTorch 2.0 Tutorial</li> <li>See progress on the course GitHub Project.</li> </ul> <p>Get updates: Follow the <code>pytorch-deep-learning</code> repo log or sign up for emails.</p>"},{"location":"#course-materialsoutline","title":"Course materials/outline","text":"<ul> <li>\ud83d\udcbb Code on GitHub: All of course materials are available open-source on GitHub.</li> <li>\ud83c\udfa5 First five sections on YouTube: Learn Pytorch in a day by watching the first 25-hours of material.</li> <li>\ud83d\udd2c Course focus: code, code, code, experiment, experiment, experiment.</li> <li>\ud83c\udfc3\u200d\u2642\ufe0f Teaching style: https://sive.rs/kimo.</li> <li>\ud83e\udd14 Ask a question: See the course GitHub Discussions page for existing questions/ask your own.</li> </ul> Section What does it cover? Exercises &amp; Extra-curriculum Slides 00 - PyTorch Fundamentals Many fundamental PyTorch operations used for deep learning and neural networks. Go to exercises &amp; extra-curriculum Go to slides 01 - PyTorch Workflow Provides an outline for approaching deep learning problems and building neural networks with PyTorch. Go to exercises &amp; extra-curriculum Go to slides 02 - PyTorch Neural Network Classification Uses the PyTorch workflow from 01 to go through a neural network classification problem. Go to exercises &amp; extra-curriculum Go to slides 03 - PyTorch Computer Vision Let's see how PyTorch can be used for computer vision problems using the same workflow from 01 &amp; 02. Go to exercises &amp; extra-curriculum Go to slides 04 - PyTorch Custom Datasets How do you load a custom dataset into PyTorch? Also we'll be laying the foundations in this notebook for our modular code (covered in 05). Go to exercises &amp; extra-curriculum Go to slides 05 - PyTorch Going Modular PyTorch is designed to be modular, let's turn what we've created into a series of Python scripts (this is how you'll often find PyTorch code in the wild). Go to exercises &amp; extra-curriculum Go to slides 06 - PyTorch Transfer Learning Let's take a well performing pre-trained model and adjust it to one of our own problems. Go to exercises &amp; extra-curriculum Go to slides 07 - Milestone Project 1: PyTorch Experiment Tracking We've built a bunch of models... wouldn't it be good to track how they're all going? Go to exercises &amp; extra-curriculum Go to slides 08 - Milestone Project 2: PyTorch Paper Replicating PyTorch is the most popular deep learning framework for machine learning research, let's see why by replicating a machine learning paper. Go to exercises &amp; extra-curriculum Go to slides 09 - Milestone Project 3: Model Deployment So we've built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet. Go to exercises &amp; extra-curriculum Go to slides PyTorch Extra Resources This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you'll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more. - - PyTorch Cheatsheet A very quick overview of some of the main features of PyTorch plus links to various resources where more can be found in the course and in the PyTorch documentation. - - Three Most Common Errors in PyTorch An overview of the three most common errors in PyTorch (shape, device and datatype errors), how they happen and how to fix them. - - A Quick PyTorch 2.0 Tutorial A fasssssst introduction to PyTorch 2.0, what's new and how to get started along with resources to learn more. - -"},{"location":"#about-this-course","title":"About this course","text":""},{"location":"#who-is-this-course-for","title":"Who is this course for?","text":"<p>You: Are a beginner in the field of machine learning or deep learning or AI and would like to learn PyTorch.</p> <p>This course: Teaches you PyTorch and many machine learning, deep learning and AI concepts in a hands-on, code-first way.</p> <p>If you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.</p>"},{"location":"#what-are-the-prerequisites","title":"What are the prerequisites?","text":"<ol> <li>3-6 months coding Python.</li> <li>At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).</li> <li>Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).</li> <li>A willingness to learn (most important).</li> </ol> <p>For 1 &amp; 2, I'd recommend the Zero to Mastery Data Science and Machine Learning Bootcamp, it'll teach you the fundamentals of machine learning and Python (I'm biased though, I also teach that course).</p>"},{"location":"#how-is-the-course-taught","title":"How is the course taught?","text":"<p>All of the course materials are available for free in an online book at learnpytorch.io. If you like to read, I'd recommend going through the resources there.</p> <p>If you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.</p> <p>There's a reason the course motto's include if in doubt, run the code and experiment, experiment, experiment!.</p> <p>My whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.</p> <p>The code is all written via Google Colab Notebooks (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.</p>"},{"location":"#what-will-i-get-if-i-finish-the-course","title":"What will I get if I finish the course?","text":"<p>There's certificates and all that jazz if you go through the videos.</p> <p>But certificates are meh.</p> <p>You can consider this course a machine learning momentum builder.</p> <p>By the end, you'll have written hundreds of lines of PyTorch code.</p> <p>And will have been exposed to many of the most important concepts in machine learning.</p> <p>So when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it'll feel familiar and if it doesn't, at least you'll know where to look.</p>"},{"location":"#what-will-i-build-in-the-course","title":"What will I build in the course?","text":"<p>We start with the barebone fundamentals of PyTorch and machine learning, so even if you're new to machine learning you'll be caught up to speed.</p> <p>Then we\u2019ll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!</p> <p>Along the way, you\u2019ll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food. </p> <p>These milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say \"here's what I've done\".</p>"},{"location":"#how-do-i-approach-this-course","title":"How do I approach this course?","text":"<p>As mentioned, the video version of the course is taught apprenticeship style.</p> <p>Meaning I write PyTorch code, you write PyTorch code.</p> <p>But here's what I recommend:</p> <ol> <li>Code along (if in doubt, run the code) - Follow along with code and try to write as much of it as you can yourself, keep doing so until you find yourself writing PyTorch code in your subconscious that's when you can stop writing the same code over and over again. </li> <li>Explore and experiment (experiment, experiment, experiment!) - Machine learning (and deep learning) is very experimental. So if you find yourself wanting to try something on your own and ignoring the materials, do it.</li> <li>Visualize what you don't understand (visualize, visualize, visualize!) - Numbers on a page can get confusing. So make things colourful, see what the inputs and outputs of your code looks like.</li> <li>Ask questions - If you're stuck with something, ask a question, trying searching for it or if nothing comes up, the course GitHub Discussions page will be the place to go.</li> <li>Do the exercises - Each module of the course comes with a dedicated exercises section. It's important to try these on your own. You will get stuck. But that's the nature of learning something new: everyone gets stuck.</li> <li>Share your work - If you've learned something cool or even better, made something cool, share it. It could be with the course Discord group or on the course GitHub page or on your own website. The benefits of sharing your work is you get to practice communicating as well as others can help you out if you're not sure of something.</li> </ol>"},{"location":"#do-i-need-to-take-things-in-order","title":"Do I need to take things in order?","text":"<p>The notebooks/chapters build upon each other sequentially but feel free to jump around.</p>"},{"location":"#how-do-i-get-started","title":"How do I get started?","text":"<p>You can read the materials on any device but this course is best viewed and coded along within a desktop browser.</p> <p>The course uses a free tool called Google Colab. If you've got no experience with it, I'd go through the free Introduction to Google Colab tutorial and then come back here.</p> <p>To start:</p> <ol> <li>Click on one of the notebook or section links like \"00. PyTorch Fundamentals\". </li> <li>Click the \"Open in Colab\" button up the top.</li> <li>Press SHIFT+Enter a few times and see what happens.</li> </ol> <p>Happy machine learning!</p>"},{"location":"00_pytorch_fundamentals/","title":"00. PyTorch Fundamentals","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>import torch\ntorch.__version__\n</pre> import torch torch.__version__ Out[1]: <pre>'1.13.1+cu116'</pre> <p>Wonderful, it looks like we've got PyTorch 1.10.0+.</p> <p>This means if you're going through these materials, you'll see most compatability with PyTorch 1.10.0+, however if your version number is far higher than that, you might notice some inconsistencies.</p> <p>And if you do have any issues, please post on the course GitHub Discussions page.</p> In\u00a0[2]: Copied! <pre># Scalar\nscalar = torch.tensor(7)\nscalar\n</pre> # Scalar scalar = torch.tensor(7) scalar Out[2]: <pre>tensor(7)</pre> <p>See how the above printed out <code>tensor(7)</code>?</p> <p>That means although <code>scalar</code> is a single number, it's of type <code>torch.Tensor</code>.</p> <p>We can check the dimensions of a tensor using the <code>ndim</code> attribute.</p> In\u00a0[3]: Copied! <pre>scalar.ndim\n</pre> scalar.ndim Out[3]: <pre>0</pre> <p>What if we wanted to retrieve the number from the tensor?</p> <p>As in, turn it from <code>torch.Tensor</code> to a Python integer?</p> <p>To do we can use the <code>item()</code> method.</p> In\u00a0[4]: Copied! <pre># Get the Python number within a tensor (only works with one-element tensors)\nscalar.item()\n</pre> # Get the Python number within a tensor (only works with one-element tensors) scalar.item() Out[4]: <pre>7</pre> <p>Okay, now let's see a vector.</p> <p>A vector is a single dimension tensor but can contain many numbers.</p> <p>As in, you could have a vector <code>[3, 2]</code> to describe <code>[bedrooms, bathrooms]</code> in your house. Or you could have <code>[3, 2, 2]</code> to describe <code>[bedrooms, bathrooms, car_parks]</code> in your house.</p> <p>The important trend here is that a vector is flexible in what it can represent (the same with tensors).</p> In\u00a0[5]: Copied! <pre># Vector\nvector = torch.tensor([7, 7])\nvector\n</pre> # Vector vector = torch.tensor([7, 7]) vector Out[5]: <pre>tensor([7, 7])</pre> <p>Wonderful, <code>vector</code> now contains two 7's, my favourite number.</p> <p>How many dimensions do you think it'll have?</p> In\u00a0[6]: Copied! <pre># Check the number of dimensions of vector\nvector.ndim\n</pre> # Check the number of dimensions of vector vector.ndim Out[6]: <pre>1</pre> <p>Hmm, that's strange, <code>vector</code> contains two numbers but only has a single dimension.</p> <p>I'll let you in on a trick.</p> <p>You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside (<code>[</code>) and you only need to count one side.</p> <p>How many square brackets does <code>vector</code> have?</p> <p>Another important concept for tensors is their <code>shape</code> attribute. The shape tells you how the elements inside them are arranged.</p> <p>Let's check out the shape of <code>vector</code>.</p> In\u00a0[7]: Copied! <pre># Check shape of vector\nvector.shape\n</pre> # Check shape of vector vector.shape Out[7]: <pre>torch.Size([2])</pre> <p>The above returns <code>torch.Size([2])</code> which means our vector has a shape of <code>[2]</code>. This is because of the two elements we placed inside the square brackets (<code>[7, 7]</code>).</p> <p>Let's now see a matrix.</p> In\u00a0[8]: Copied! <pre># Matrix\nMATRIX = torch.tensor([[7, 8], \n                       [9, 10]])\nMATRIX\n</pre> # Matrix MATRIX = torch.tensor([[7, 8],                         [9, 10]]) MATRIX Out[8]: <pre>tensor([[ 7,  8],\n        [ 9, 10]])</pre> <p>Wow! More numbers! Matrices are as flexible as vectors, except they've got an extra dimension.</p> In\u00a0[9]: Copied! <pre># Check number of dimensions\nMATRIX.ndim\n</pre> # Check number of dimensions MATRIX.ndim Out[9]: <pre>2</pre> <p><code>MATRIX</code> has two dimensions (did you count the number of square brakcets on the outside of one side?).</p> <p>What <code>shape</code> do you think it will have?</p> In\u00a0[10]: Copied! <pre>MATRIX.shape\n</pre> MATRIX.shape Out[10]: <pre>torch.Size([2, 2])</pre> <p>We get the output <code>torch.Size([2, 2])</code> because <code>MATRIX</code> is two elements deep and two elements wide.</p> <p>How about we create a tensor?</p> In\u00a0[11]: Copied! <pre># Tensor\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n</pre> # Tensor TENSOR = torch.tensor([[[1, 2, 3],                         [3, 6, 9],                         [2, 4, 5]]]) TENSOR Out[11]: <pre>tensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])</pre> <p>Woah! What a nice looking tensor.</p> <p>I want to stress that tensors can represent almost anything.</p> <p>The one we just created could be the sales numbers for a steak and almond butter store (two of my favourite foods).</p> <p></p> <p>How many dimensions do you think it has? (hint: use the square bracket counting trick)</p> In\u00a0[12]: Copied! <pre># Check number of dimensions for TENSOR\nTENSOR.ndim\n</pre> # Check number of dimensions for TENSOR TENSOR.ndim Out[12]: <pre>3</pre> <p>And what about its shape?</p> In\u00a0[13]: Copied! <pre># Check shape of TENSOR\nTENSOR.shape\n</pre> # Check shape of TENSOR TENSOR.shape Out[13]: <pre>torch.Size([1, 3, 3])</pre> <p>Alright, it outputs <code>torch.Size([1, 3, 3])</code>.</p> <p>The dimensions go outer to inner.</p> <p>That means there's 1 dimension of 3 by 3.</p> <p></p> <p>Note: You might've noticed me using lowercase letters for <code>scalar</code> and <code>vector</code> and uppercase letters for <code>MATRIX</code> and <code>TENSOR</code>. This was on purpose. In practice, you'll often see scalars and vectors denoted as lowercase letters such as <code>y</code> or <code>a</code>. And matrices and tensors denoted as uppercase letters such as <code>X</code> or <code>W</code>.</p> <p>You also might notice the names martrix and tensor used interchangably. This is common. Since in PyTorch you're often dealing with <code>torch.Tensor</code>s (hence the tensor name), however, the shape and dimensions of what's inside will dictate what it actually is.</p> <p>Let's summarise.</p> Name What is it? Number of dimensions Lower or upper (usually/example) scalar a single number 0 Lower (<code>a</code>) vector a number with direction (e.g. wind speed with direction) but can also have many other numbers 1 Lower (<code>y</code>) matrix a 2-dimensional array of numbers 2 Upper (<code>Q</code>) tensor an n-dimensional array of numbers can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector Upper (<code>X</code>) <p></p> In\u00a0[14]: Copied! <pre># Create a random tensor of size (3, 4)\nrandom_tensor = torch.rand(size=(3, 4))\nrandom_tensor, random_tensor.dtype\n</pre> # Create a random tensor of size (3, 4) random_tensor = torch.rand(size=(3, 4)) random_tensor, random_tensor.dtype Out[14]: <pre>(tensor([[0.6541, 0.4807, 0.2162, 0.6168],\n         [0.4428, 0.6608, 0.6194, 0.8620],\n         [0.2795, 0.6055, 0.4958, 0.5483]]),\n torch.float32)</pre> <p>The flexibility of <code>torch.rand()</code> is that we can adjust the <code>size</code> to be whatever we want.</p> <p>For example, say you wanted a random tensor in the common image shape of <code>[224, 224, 3]</code> (<code>[height, width, color_channels</code>]).</p> In\u00a0[15]: Copied! <pre># Create a random tensor of size (224, 224, 3)\nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n</pre> # Create a random tensor of size (224, 224, 3) random_image_size_tensor = torch.rand(size=(224, 224, 3)) random_image_size_tensor.shape, random_image_size_tensor.ndim Out[15]: <pre>(torch.Size([224, 224, 3]), 3)</pre> In\u00a0[16]: Copied! <pre># Create a tensor of all zeros\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n</pre> # Create a tensor of all zeros zeros = torch.zeros(size=(3, 4)) zeros, zeros.dtype Out[16]: <pre>(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n torch.float32)</pre> <p>We can do the same to create a tensor of all ones except using <code>torch.ones()</code>  instead.</p> In\u00a0[17]: Copied! <pre># Create a tensor of all ones\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n</pre> # Create a tensor of all ones ones = torch.ones(size=(3, 4)) ones, ones.dtype Out[17]: <pre>(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n torch.float32)</pre> In\u00a0[18]: Copied! <pre># Use torch.arange(), torch.range() is deprecated \nzero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n\n# Create a range of values 0 to 10\nzero_to_ten = torch.arange(start=0, end=10, step=1)\nzero_to_ten\n</pre> # Use torch.arange(), torch.range() is deprecated  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future  # Create a range of values 0 to 10 zero_to_ten = torch.arange(start=0, end=10, step=1) zero_to_ten <pre>/tmp/ipykernel_3695928/193451495.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n</pre> Out[18]: <pre>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> <p>Sometimes you might want one tensor of a certain type with the same shape as another tensor.</p> <p>For example, a tensor of all zeros with the same shape as a previous tensor.</p> <p>To do so you can use <code>torch.zeros_like(input)</code> or <code>torch.ones_like(input)</code> which return a tensor filled with zeros or ones in the same shape as the <code>input</code> respectively.</p> In\u00a0[19]: Copied! <pre># Can also create a tensor of zeros similar to another tensor\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n</pre> # Can also create a tensor of zeros similar to another tensor ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape ten_zeros Out[19]: <pre>tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</pre> In\u00a0[20]: Copied! <pre># Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n</pre> # Default datatype for tensors is float32 float_32_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed                                device=None, # defaults to None, which uses the default tensor type                                requires_grad=False) # if True, operations performed on the tensor are recorded   float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device Out[20]: <pre>(torch.Size([3]), torch.float32, device(type='cpu'))</pre> <p>Aside from shape issues (tensor shapes don't match up), two of the other most common issues you'll come across in PyTorch are datatype and device issues.</p> <p>For example, one of tensors is <code>torch.float32</code> and the other is <code>torch.float16</code> (PyTorch often likes tensors to be the same format).</p> <p>Or one of your tensors is on the CPU and the other is on the GPU (PyTorch likes calculations between tensors to be on the same device).</p> <p>We'll see more of this device talk later on.</p> <p>For now let's create a tensor with <code>dtype=torch.float16</code>.</p> In\u00a0[21]: Copied! <pre>float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n</pre> float_16_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=torch.float16) # torch.half would also work  float_16_tensor.dtype Out[21]: <pre>torch.float16</pre> In\u00a0[22]: Copied! <pre># Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n</pre> # Create a tensor some_tensor = torch.rand(3, 4)  # Find out details about it print(some_tensor) print(f\"Shape of tensor: {some_tensor.shape}\") print(f\"Datatype of tensor: {some_tensor.dtype}\") print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU <pre>tensor([[0.4688, 0.0055, 0.8551, 0.0646],\n        [0.6538, 0.5157, 0.4071, 0.2109],\n        [0.9960, 0.3061, 0.9369, 0.7008]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n</pre> <p>Note: When you run into issues in PyTorch, it's very often one to do with one of the three attributes above. So when the error messages show up, sing yourself a little song called \"what, what, where\":</p> <ul> <li>\"what shape are my tensors? what datatype are they and where are they stored? what shape, what datatype, where where where\"</li> </ul> In\u00a0[23]: Copied! <pre># Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n</pre> # Create a tensor of values and add a number to it tensor = torch.tensor([1, 2, 3]) tensor + 10 Out[23]: <pre>tensor([11, 12, 13])</pre> In\u00a0[24]: Copied! <pre># Multiply it by 10\ntensor * 10\n</pre> # Multiply it by 10 tensor * 10 Out[24]: <pre>tensor([10, 20, 30])</pre> <p>Notice how the tensor values above didn't end up being <code>tensor([110, 120, 130])</code>, this is because the values inside the tensor don't change unless they're reassigned.</p> In\u00a0[25]: Copied! <pre># Tensors don't change unless reassigned\ntensor\n</pre> # Tensors don't change unless reassigned tensor Out[25]: <pre>tensor([1, 2, 3])</pre> <p>Let's subtract a number and this time we'll reassign the <code>tensor</code> variable.</p> In\u00a0[26]: Copied! <pre># Subtract and reassign\ntensor = tensor - 10\ntensor\n</pre> # Subtract and reassign tensor = tensor - 10 tensor Out[26]: <pre>tensor([-9, -8, -7])</pre> In\u00a0[27]: Copied! <pre># Add and reassign\ntensor = tensor + 10\ntensor\n</pre> # Add and reassign tensor = tensor + 10 tensor Out[27]: <pre>tensor([1, 2, 3])</pre> <p>PyTorch also has a bunch of built-in functions like <code>torch.mul()</code> (short for multiplication) and <code>torch.add()</code> to perform basic operations.</p> In\u00a0[28]: Copied! <pre># Can also use torch functions\ntorch.multiply(tensor, 10)\n</pre> # Can also use torch functions torch.multiply(tensor, 10) Out[28]: <pre>tensor([10, 20, 30])</pre> In\u00a0[29]: Copied! <pre># Original tensor is still unchanged \ntensor\n</pre> # Original tensor is still unchanged  tensor Out[29]: <pre>tensor([1, 2, 3])</pre> <p>However, it's more common to use the operator symbols like <code>*</code> instead of <code>torch.mul()</code></p> In\u00a0[30]: Copied! <pre># Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n</pre> # Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2) print(tensor, \"*\", tensor) print(\"Equals:\", tensor * tensor) <pre>tensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n</pre> In\u00a0[31]: Copied! <pre>import torch\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n</pre> import torch tensor = torch.tensor([1, 2, 3]) tensor.shape Out[31]: <pre>torch.Size([3])</pre> <p>The difference between element-wise multiplication and matrix multiplication is the addition of values.</p> <p>For our <code>tensor</code> variable with values <code>[1, 2, 3]</code>:</p> Operation Calculation Code Element-wise multiplication <code>[1*1, 2*2, 3*3]</code> = <code>[1, 4, 9]</code> <code>tensor * tensor</code> Matrix multiplication <code>[1*1 + 2*2 + 3*3]</code> = <code>[14]</code> <code>tensor.matmul(tensor)</code> In\u00a0[32]: Copied! <pre># Element-wise matrix multiplication\ntensor * tensor\n</pre> # Element-wise matrix multiplication tensor * tensor Out[32]: <pre>tensor([1, 4, 9])</pre> In\u00a0[33]: Copied! <pre># Matrix multiplication\ntorch.matmul(tensor, tensor)\n</pre> # Matrix multiplication torch.matmul(tensor, tensor) Out[33]: <pre>tensor(14)</pre> In\u00a0[34]: Copied! <pre># Can also use the \"@\" symbol for matrix multiplication, though not recommended\ntensor @ tensor\n</pre> # Can also use the \"@\" symbol for matrix multiplication, though not recommended tensor @ tensor Out[34]: <pre>tensor(14)</pre> <p>You can do matrix multiplication by hand but it's not recommended.</p> <p>The in-built <code>torch.matmul()</code> method is faster.</p> In\u00a0[35]: Copied! <pre>%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n</pre> %%time # Matrix multiplication by hand  # (avoid doing operations with for loops at all cost, they are computationally expensive) value = 0 for i in range(len(tensor)):   value += tensor[i] * tensor[i] value <pre>CPU times: user 773 \u00b5s, sys: 0 ns, total: 773 \u00b5s\nWall time: 499 \u00b5s\n</pre> Out[35]: <pre>tensor(14)</pre> In\u00a0[36]: Copied! <pre>%%time\ntorch.matmul(tensor, tensor)\n</pre> %%time torch.matmul(tensor, tensor) <pre>CPU times: user 146 \u00b5s, sys: 83 \u00b5s, total: 229 \u00b5s\nWall time: 171 \u00b5s\n</pre> Out[36]: <pre>tensor(14)</pre> In\u00a0[37]: Copied! <pre># Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n</pre> # Shapes need to be in the right way   tensor_A = torch.tensor([[1, 2],                          [3, 4],                          [5, 6]], dtype=torch.float32)  tensor_B = torch.tensor([[7, 10],                          [8, 11],                           [9, 12]], dtype=torch.float32)  torch.matmul(tensor_A, tensor_B) # (this will error) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 75 in &lt;cell line: 10&gt;()\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'&gt;2&lt;/a&gt; tensor_A = torch.tensor([[1, 2],\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'&gt;3&lt;/a&gt;                          [3, 4],\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'&gt;4&lt;/a&gt;                          [5, 6]], dtype=torch.float32)\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'&gt;6&lt;/a&gt; tensor_B = torch.tensor([[7, 10],\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'&gt;7&lt;/a&gt;                          [8, 11], \n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'&gt;8&lt;/a&gt;                          [9, 12]], dtype=torch.float32)\n---&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'&gt;10&lt;/a&gt; torch.matmul(tensor_A, tensor_B)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)</pre> <p>We can make matrix multiplication work between <code>tensor_A</code> and <code>tensor_B</code> by making their inner dimensions match.</p> <p>One of the ways to do this is with a transpose (switch the dimensions of a given tensor).</p> <p>You can perform transposes in PyTorch using either:</p> <ul> <li><code>torch.transpose(input, dim0, dim1)</code> - where <code>input</code> is the desired tensor to transpose and <code>dim0</code> and <code>dim1</code> are the dimensions to be swapped.</li> <li><code>tensor.T</code> - where <code>tensor</code> is the desired tensor to transpose.</li> </ul> <p>Let's try the latter.</p> In\u00a0[38]: Copied! <pre># View tensor_A and tensor_B\nprint(tensor_A)\nprint(tensor_B)\n</pre> # View tensor_A and tensor_B print(tensor_A) print(tensor_B) <pre>tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7., 10.],\n        [ 8., 11.],\n        [ 9., 12.]])\n</pre> In\u00a0[39]: Copied! <pre># View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n</pre> # View tensor_A and tensor_B.T print(tensor_A) print(tensor_B.T) <pre>tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n</pre> In\u00a0[40]: Copied! <pre># The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n</pre> # The operation works when tensor_B is transposed print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\") print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\") print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\") print(\"Output:\\n\") output = torch.matmul(tensor_A, tensor_B.T) print(output)  print(f\"\\nOutput shape: {output.shape}\") <pre>Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n</pre> <p>You can also use <code>torch.mm()</code> which is a short for <code>torch.matmul()</code>.</p> In\u00a0[41]: Copied! <pre># torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n</pre> # torch.mm is a shortcut for matmul torch.mm(tensor_A, tensor_B.T) Out[41]: <pre>tensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])</pre> <p>Without the transpose, the rules of matrix mulitplication aren't fulfilled and we get an error like above.</p> <p>How about a visual?</p> <p></p> <p>You can create your own matrix multiplication visuals like this at http://matrixmultiplication.xyz/.</p> <p>Note: A matrix multiplication like this is also referred to as the dot product of two matrices.</p> <p>Neural networks are full of matrix multiplications and dot products.</p> <p>The <code>torch.nn.Linear()</code> module (we'll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input <code>x</code> and a weights matrix <code>A</code>.</p> $$ y = x\\cdot{A^T} + b $$<p>Where:</p> <ul> <li><code>x</code> is the input to the layer (deep learning is a stack of layers like <code>torch.nn.Linear()</code> and others on top of each other).</li> <li><code>A</code> is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"<code>T</code>\", that's because the weights matrix gets transposed).<ul> <li>Note: You might also often see <code>W</code> or another letter like <code>X</code> used to showcase the weights matrix.</li> </ul> </li> <li><code>b</code> is the bias term used to slightly offset the weights and inputs.</li> <li><code>y</code> is the output (a manipulation of the input in the hopes to discover patterns in it).</li> </ul> <p>This is a linear function (you may have seen something like $y = mx+b$ in high school or elsewhere), and can be used to draw a straight line!</p> <p>Let's play around with a linear layer.</p> <p>Try changing the values of <code>in_features</code> and <code>out_features</code> below and see what happens.</p> <p>Do you notice anything to do with the shapes?</p> In\u00a0[42]: Copied! <pre># Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n</pre> # Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later) torch.manual_seed(42) # This uses matrix multiplication linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input                           out_features=6) # out_features = describes outer value  x = tensor_A output = linear(x) print(f\"Input shape: {x.shape}\\n\") print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\") <pre>Input shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n</pre> <p>Question: What happens if you change <code>in_features</code> from 2 to 3 above? Does it error? How could you change the shape of the input (<code>x</code>) to accomodate to the error? Hint: what did we have to do to <code>tensor_B</code> above?</p> <p>If you've never done it before, matrix multiplication can be a confusing topic at first.</p> <p>But after you've played around with it a few times and even cracked open a few neural networks, you'll notice it's everywhere.</p> <p>Remember, matrix multiplication is all you need.</p> <p></p> <p>When you start digging into neural network layers and building your own, you'll find matrix multiplications everywhere. Source:* https://marksaroufim.substack.com/p/working-class-deep-learner*</p> In\u00a0[43]: Copied! <pre># Create a tensor\nx = torch.arange(0, 100, 10)\nx\n</pre> # Create a tensor x = torch.arange(0, 100, 10) x Out[43]: <pre>tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])</pre> <p>Now let's perform some aggregation.</p> In\u00a0[44]: Copied! <pre>print(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n</pre> print(f\"Minimum: {x.min()}\") print(f\"Maximum: {x.max()}\") # print(f\"Mean: {x.mean()}\") # this will error print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype print(f\"Sum: {x.sum()}\") <pre>Minimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n</pre> <p>Note: You may find some methods such as <code>torch.mean()</code> require tensors to be in <code>torch.float32</code> (the most common) or another specific datatype, otherwise the operation will fail.</p> <p>You can also do the same as above with <code>torch</code> methods.</p> In\u00a0[45]: Copied! <pre>torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n</pre> torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x) Out[45]: <pre>(tensor(90), tensor(0), tensor(45.), tensor(450))</pre> In\u00a0[46]: Copied! <pre># Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n</pre> # Create a tensor tensor = torch.arange(10, 100, 10) print(f\"Tensor: {tensor}\")  # Returns index of max and min values print(f\"Index where max value occurs: {tensor.argmax()}\") print(f\"Index where min value occurs: {tensor.argmin()}\") <pre>Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n</pre> In\u00a0[47]: Copied! <pre># Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n</pre> # Create a tensor and check its datatype tensor = torch.arange(10., 100., 10.) tensor.dtype Out[47]: <pre>torch.float32</pre> <p>Now we'll create another tensor the same as before but change its datatype to <code>torch.float16</code>.</p> In\u00a0[48]: Copied! <pre># Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n</pre> # Create a float16 tensor tensor_float16 = tensor.type(torch.float16) tensor_float16 Out[48]: <pre>tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)</pre> <p>And we can do something similar to make a <code>torch.int8</code> tensor.</p> In\u00a0[49]: Copied! <pre># Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n</pre> # Create a int8 tensor tensor_int8 = tensor.type(torch.int8) tensor_int8 Out[49]: <pre>tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)</pre> <p>Note: Different datatypes can be confusing to begin with. But think of it like this, the lower the number (e.g. 32, 16, 8), the less precise a computer stores the value. And with a lower amount of storage, this generally results in faster computation and a smaller overall model. Mobile-based neural networks often operate with 8-bit integers, smaller and faster to run but less accurate than their float32 counterparts. For more on this, I'd read up about precision in computing.</p> <p>Exercise: So far we've covered a fair few tensor methods but there's a bunch more in the <code>torch.Tensor</code> documentation, I'd recommend spending 10-minutes scrolling through and looking into any that catch your eye. Click on them and then write them out in code yourself to see what happens.</p> In\u00a0[50]: Copied! <pre># Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n</pre> # Create a tensor import torch x = torch.arange(1., 8.) x, x.shape Out[50]: <pre>(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))</pre> <p>Now let's add an extra dimension with <code>torch.reshape()</code>.</p> In\u00a0[51]: Copied! <pre># Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n</pre> # Add an extra dimension x_reshaped = x.reshape(1, 7) x_reshaped, x_reshaped.shape Out[51]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>We can also change the view with <code>torch.view()</code>.</p> In\u00a0[52]: Copied! <pre># Change view (keeps same data as original but changes view)\n# See more: https://stackoverflow.com/a/54507446/7900723\nz = x.view(1, 7)\nz, z.shape\n</pre> # Change view (keeps same data as original but changes view) # See more: https://stackoverflow.com/a/54507446/7900723 z = x.view(1, 7) z, z.shape Out[52]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>Remember though, changing the view of a tensor with <code>torch.view()</code> really only creates a new view of the same tensor.</p> <p>So changing the view changes the original tensor too.</p> In\u00a0[53]: Copied! <pre># Changing z changes x\nz[:, 0] = 5\nz, x\n</pre> # Changing z changes x z[:, 0] = 5 z, x Out[53]: <pre>(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))</pre> <p>If we wanted to stack our new tensor on top of itself five times, we could do so with <code>torch.stack()</code>.</p> In\u00a0[54]: Copied! <pre># Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n</pre> # Stack tensors on top of each other x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens x_stacked Out[54]: <pre>tensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])</pre> <p>How about removing all single dimensions from a tensor?</p> <p>To do so you can use <code>torch.squeeze()</code> (I remember this as squeezing the tensor to only have dimensions over 1).</p> In\u00a0[55]: Copied! <pre>print(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n</pre> print(f\"Previous tensor: {x_reshaped}\") print(f\"Previous shape: {x_reshaped.shape}\")  # Remove extra dimension from x_reshaped x_squeezed = x_reshaped.squeeze() print(f\"\\nNew tensor: {x_squeezed}\") print(f\"New shape: {x_squeezed.shape}\") <pre>Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n</pre> <p>And to do the reverse of <code>torch.squeeze()</code> you can use <code>torch.unsqueeze()</code> to add a dimension value of 1 at a specific index.</p> In\u00a0[56]: Copied! <pre>print(f\"Previous tensor: {x_squeezed}\")\nprint(f\"Previous shape: {x_squeezed.shape}\")\n\n## Add an extra dimension with unsqueeze\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"\\nNew tensor: {x_unsqueezed}\")\nprint(f\"New shape: {x_unsqueezed.shape}\")\n</pre> print(f\"Previous tensor: {x_squeezed}\") print(f\"Previous shape: {x_squeezed.shape}\")  ## Add an extra dimension with unsqueeze x_unsqueezed = x_squeezed.unsqueeze(dim=0) print(f\"\\nNew tensor: {x_unsqueezed}\") print(f\"New shape: {x_unsqueezed.shape}\") <pre>Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nPrevious shape: torch.Size([7])\n\nNew tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nNew shape: torch.Size([1, 7])\n</pre> <p>You can also rearrange the order of axes values with <code>torch.permute(input, dims)</code>, where the <code>input</code> gets turned into a view with new <code>dims</code>.</p> In\u00a0[57]: Copied! <pre># Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n</pre> # Create tensor with specific shape x_original = torch.rand(size=(224, 224, 3))  # Permute the original tensor to rearrange the axis order x_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0  print(f\"Previous shape: {x_original.shape}\") print(f\"New shape: {x_permuted.shape}\") <pre>Previous shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n</pre> <p>Note: Because permuting returns a view (shares the same data as the original), the values in the permuted tensor will be the same as the original tensor and if you change the values in the view, it will change the values of the original.</p> In\u00a0[58]: Copied! <pre># Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n</pre> # Create a tensor  import torch x = torch.arange(1, 10).reshape(1, 3, 3) x, x.shape Out[58]: <pre>(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n torch.Size([1, 3, 3]))</pre> <p>Indexing values goes outer dimension -&gt; inner dimension (check out the square brackets).</p> In\u00a0[59]: Copied! <pre># Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n</pre> # Let's index bracket by bracket print(f\"First square bracket:\\n{x[0]}\")  print(f\"Second square bracket: {x[0][0]}\")  print(f\"Third square bracket: {x[0][0][0]}\") <pre>First square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n</pre> <p>You can also use <code>:</code> to specify \"all values in this dimension\" and then use a comma (<code>,</code>) to add another dimension.</p> In\u00a0[60]: Copied! <pre># Get all values of 0th dimension and the 0 index of 1st dimension\nx[:, 0]\n</pre> # Get all values of 0th dimension and the 0 index of 1st dimension x[:, 0] Out[60]: <pre>tensor([[1, 2, 3]])</pre> In\u00a0[61]: Copied! <pre># Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n</pre> # Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension x[:, :, 1] Out[61]: <pre>tensor([[2, 5, 8]])</pre> In\u00a0[62]: Copied! <pre># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\nx[:, 1, 1]\n</pre> # Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension x[:, 1, 1] Out[62]: <pre>tensor([5])</pre> In\u00a0[63]: Copied! <pre># Get index 0 of 0th and 1st dimension and all values of 2nd dimension \nx[0, 0, :] # same as x[0][0]\n</pre> # Get index 0 of 0th and 1st dimension and all values of 2nd dimension  x[0, 0, :] # same as x[0][0] Out[63]: <pre>tensor([1, 2, 3])</pre> <p>Indexing can be quite confusing to begin with, especially with larger tensors (I still have to try indexing multiple times to get it right). But with a bit of practice and following the data explorer's motto (*visualize, visualize, visualize*), you'll start to get the hang of it.</p> In\u00a0[64]: Copied! <pre># NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n</pre> # NumPy array to tensor import torch import numpy as np array = np.arange(1.0, 8.0) tensor = torch.from_numpy(array) array, tensor Out[64]: <pre>(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</pre> <p>Note: By default, NumPy arrays are created with the datatype <code>float64</code> and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above).</p> <p>However, many PyTorch calculations default to using <code>float32</code>.</p> <p>So if you want to convert your NumPy array (float64) -&gt; PyTorch tensor (float64) -&gt; PyTorch tensor (float32), you can use <code>tensor = torch.from_numpy(array).type(torch.float32)</code>.</p> <p>Because we reassigned <code>tensor</code> above, if you change the tensor, the array stays the same.</p> In\u00a0[65]: Copied! <pre># Change the array, keep the tensor\narray = array + 1\narray, tensor\n</pre> # Change the array, keep the tensor array = array + 1 array, tensor Out[65]: <pre>(array([2., 3., 4., 5., 6., 7., 8.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</pre> <p>And if you want to go from PyTorch tensor to NumPy array, you can call <code>tensor.numpy()</code>.</p> In\u00a0[66]: Copied! <pre># Tensor to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n</pre> # Tensor to NumPy array tensor = torch.ones(7) # create a tensor of ones with dtype=float32 numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed tensor, numpy_tensor Out[66]: <pre>(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</pre> <p>And the same rule applies as above, if you change the original <code>tensor</code>, the new <code>numpy_tensor</code> stays the same.</p> In\u00a0[67]: Copied! <pre># Change the tensor, keep the array the same\ntensor = tensor + 1\ntensor, numpy_tensor\n</pre> # Change the tensor, keep the array the same tensor = tensor + 1 tensor, numpy_tensor Out[67]: <pre>(tensor([2., 2., 2., 2., 2., 2., 2.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</pre> In\u00a0[68]: Copied! <pre>import torch\n\n# Create two random tensors\nrandom_tensor_A = torch.rand(3, 4)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"Tensor A:\\n{random_tensor_A}\\n\")\nprint(f\"Tensor B:\\n{random_tensor_B}\\n\")\nprint(f\"Does Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n</pre> import torch  # Create two random tensors random_tensor_A = torch.rand(3, 4) random_tensor_B = torch.rand(3, 4)  print(f\"Tensor A:\\n{random_tensor_A}\\n\") print(f\"Tensor B:\\n{random_tensor_B}\\n\") print(f\"Does Tensor A equal Tensor B? (anywhere)\") random_tensor_A == random_tensor_B <pre>Tensor A:\ntensor([[0.8016, 0.3649, 0.6286, 0.9663],\n        [0.7687, 0.4566, 0.5745, 0.9200],\n        [0.3230, 0.8613, 0.0919, 0.3102]])\n\nTensor B:\ntensor([[0.9536, 0.6002, 0.0351, 0.6826],\n        [0.3743, 0.5220, 0.1336, 0.9666],\n        [0.9754, 0.8474, 0.8988, 0.1105]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n</pre> Out[68]: <pre>tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])</pre> <p>Just as you might've expected, the tensors come out with different values.</p> <p>But what if you wanted to created two random tensors with the same values.</p> <p>As in, the tensors would still contain random values but they would be of the same flavour.</p> <p>That's where <code>torch.manual_seed(seed)</code> comes in, where <code>seed</code> is an integer (like <code>42</code> but it could be anything) that flavours the randomness.</p> <p>Let's try it out by creating some more flavoured random tensors.</p> In\u00a0[69]: Copied! <pre>import torch\nimport random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n</pre> import torch import random  # # Set the random seed RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below torch.manual_seed(seed=RANDOM_SEED)  random_tensor_C = torch.rand(3, 4)  # Have to reset the seed every time a new rand() is called  # Without this, tensor_D would be different to tensor_C  torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens random_tensor_D = torch.rand(3, 4)  print(f\"Tensor C:\\n{random_tensor_C}\\n\") print(f\"Tensor D:\\n{random_tensor_D}\\n\") print(f\"Does Tensor C equal Tensor D? (anywhere)\") random_tensor_C == random_tensor_D <pre>Tensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n</pre> Out[69]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>Nice!</p> <p>It looks like setting the seed worked.</p> <p>Resource: What we've just covered only scratches the surface of reproducibility in PyTorch. For more, on reproducbility in general and random seeds, I'd checkout:</p> <ul> <li>The PyTorch reproducibility documentation (a good exericse would be to read through this for 10-minutes and even if you don't understand it now, being aware of it is important).</li> <li>The Wikipedia random seed page (this'll give a good overview of random seeds and pseudorandomness in general).</li> </ul> In\u00a0[70]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Sat Jan 21 08:34:23 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA TITAN RTX    On   | 00000000:01:00.0 Off |                  N/A |\n| 40%   30C    P8     7W / 280W |    177MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1061      G   /usr/lib/xorg/Xorg                 53MiB |\n|    0   N/A  N/A   2671131      G   /usr/lib/xorg/Xorg                 97MiB |\n|    0   N/A  N/A   2671256      G   /usr/bin/gnome-shell                9MiB |\n+-----------------------------------------------------------------------------+\n</pre> <p>If you don't have a Nvidia GPU accessible, the above will output something like:</p> <pre><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n</code></pre> <p>In that case, go back up and follow the install steps.</p> <p>If you do have a GPU, the line above will output something like:</p> <pre><code>Wed Jan 19 22:09:08 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> In\u00a0[71]: Copied! <pre># Check for GPU\nimport torch\ntorch.cuda.is_available()\n</pre> # Check for GPU import torch torch.cuda.is_available() Out[71]: <pre>True</pre> <p>If the above outputs <code>True</code>, PyTorch can see and use the GPU, if it outputs <code>False</code>, it can't see the GPU and in that case, you'll have to go back through the installation steps.</p> <p>Now, let's say you wanted to setup your code so it ran on CPU or the GPU if it was available.</p> <p>That way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.</p> <p>Let's create a <code>device</code> variable to store what kind of device is available.</p> In\u00a0[72]: Copied! <pre># Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Set device type device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[72]: <pre>'cuda'</pre> <p>If the above output <code>\"cuda\"</code> it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output <code>\"cpu\"</code>, our PyTorch code will stick with the CPU.</p> <p>Note: In PyTorch, it's best practice to write device agnostic code. This means code that'll run on CPU (always available) or GPU (if available).</p> <p>If you want to do faster computing you can use a GPU but if you want to do much faster computing, you can use multiple GPUs.</p> <p>You can count the number of GPUs PyTorch has access to using <code>torch.cuda.device_count()</code>.</p> In\u00a0[73]: Copied! <pre># Count number of devices\ntorch.cuda.device_count()\n</pre> # Count number of devices torch.cuda.device_count() Out[73]: <pre>1</pre> <p>Knowing the number of GPUs PyTorch has access to is helpful incase you wanted to run a specific process on one GPU and another process on another (PyTorch also has features to let you run a process across all GPUs).</p> In\u00a0[74]: Copied! <pre># Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n</pre> # Create tensor (default on CPU) tensor = torch.tensor([1, 2, 3])  # Tensor not on GPU print(tensor, tensor.device)  # Move tensor to GPU (if available) tensor_on_gpu = tensor.to(device) tensor_on_gpu <pre>tensor([1, 2, 3]) cpu\n</pre> Out[74]: <pre>tensor([1, 2, 3], device='cuda:0')</pre> <p>If you have a GPU available, the above code will output something like:</p> <pre><code>tensor([1, 2, 3]) cpu\ntensor([1, 2, 3], device='cuda:0')\n</code></pre> <p>Notice the second tensor has <code>device='cuda:0'</code>, this means it's stored on the 0th GPU available (GPUs are 0 indexed, if two GPUs were available, they'd be <code>'cuda:0'</code> and <code>'cuda:1'</code> respectively, up to <code>'cuda:n'</code>).</p> In\u00a0[75]: Copied! <pre># If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n</pre> # If tensor is on GPU, can't transform it to NumPy (this will error) tensor_on_gpu.numpy() <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 157 in &lt;cell line: 2&gt;()\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'&gt;1&lt;/a&gt; # If tensor is on GPU, can't transform it to NumPy (this will error)\n----&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'&gt;2&lt;/a&gt; tensor_on_gpu.numpy()\n\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</pre> <p>Instead, to get a tensor back to CPU and usable with NumPy we can use <code>Tensor.cpu()</code>.</p> <p>This copies the tensor to CPU memory so it's usable with CPUs.</p> In\u00a0[76]: Copied! <pre># Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n</pre> # Instead, copy the tensor back to cpu tensor_back_on_cpu = tensor_on_gpu.cpu().numpy() tensor_back_on_cpu Out[76]: <pre>array([1, 2, 3])</pre> <p>The above returns a copy of the GPU tensor in CPU memory so the original tensor is still on GPU.</p> In\u00a0[77]: Copied! <pre>tensor_on_gpu\n</pre> tensor_on_gpu Out[77]: <pre>tensor([1, 2, 3], device='cuda:0')</pre>"},{"location":"00_pytorch_fundamentals/#00-pytorch-fundamentals","title":"00. PyTorch Fundamentals\u00b6","text":""},{"location":"00_pytorch_fundamentals/#what-is-pytorch","title":"What is PyTorch?\u00b6","text":"<p>PyTorch is an open source machine learning and deep learning framework.</p>"},{"location":"00_pytorch_fundamentals/#what-can-pytorch-be-used-for","title":"What can PyTorch be used for?\u00b6","text":"<p>PyTorch allows you to manipulate and process data and write machine learning algorithms using Python code.</p>"},{"location":"00_pytorch_fundamentals/#who-uses-pytorch","title":"Who uses PyTorch?\u00b6","text":"<p>Many of the worlds largest technology companies such as Meta (Facebook), Tesla and Microsoft as well as artificial intelligence research companies such as OpenAI use PyTorch to power research and bring machine learning to their products.</p> <p></p> <p>For example, Andrej Karpathy (head of AI at Tesla) has given several talks (PyTorch DevCon 2019, Tesla AI Day 2021) about how Tesla use PyTorch to power their self-driving computer vision models.</p> <p>PyTorch is also used in other industries such as agriculture to power computer vision on tractors.</p>"},{"location":"00_pytorch_fundamentals/#why-use-pytorch","title":"Why use PyTorch?\u00b6","text":"<p>Machine learning researchers love using PyTorch. And as of February 2022, PyTorch is the most used deep learning framework on Papers With Code, a website for tracking machine learning research papers and the code repositories attached with them.</p> <p>PyTorch also helps take care of many things such as GPU acceleration (making your code run faster) behind the scenes.</p> <p>So you can focus on manipulating data and writing algorithms and PyTorch will make sure it runs fast.</p> <p>And if companies such as Tesla and Meta (Facebook) use it to build models they deploy to power hundreds of applications, drive thousands of cars and deliver content to billions of people, it's clearly capable on the development front too.</p>"},{"location":"00_pytorch_fundamentals/#what-were-going-to-cover-in-this-module","title":"What we're going to cover in this module\u00b6","text":"<p>This course is broken down into different sections (notebooks).</p> <p>Each notebook covers important ideas and concepts within PyTorch.</p> <p>Subsequent notebooks build upon knowledge from the previous one (numbering starts at 00, 01, 02 and goes to whatever it ends up going to).</p> <p>This notebook deals with the basic building block of machine learning and deep learning, the tensor.</p> <p>Specifically, we're going to cover:</p> Topic Contents Introduction to tensors Tensors are the basic building block of all of machine learning and deep learning. Creating tensors Tensors can represent almost any kind of data (images, words, tables of numbers). Getting information from tensors If you can put information into a tensor, you'll want to get it out too. Manipulating tensors Machine learning algorithms (like neural networks) involve manipulating tensors in many different ways such as adding, multiplying, combining. Dealing with tensor shapes One of the most common issues in machine learning is dealing with shape mismatches (trying to mixed wrong shaped tensors with other tensors). Indexing on tensors If you've indexed on a Python list or NumPy array, it's very similar with tensors, except they can have far more dimensions. Mixing PyTorch tensors and NumPy PyTorch plays with tensors (<code>torch.Tensor</code>), NumPy likes arrays (<code>np.ndarray</code>) sometimes you'll want to mix and match these. Reproducibility Machine learning is very experimental and since it uses a lot of randomness to work, sometimes you'll want that randomness to not be so random. Running tensors on GPU GPUs (Graphics Processing Units) make your code faster, PyTorch makes it easy to run your code on GPUs."},{"location":"00_pytorch_fundamentals/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"00_pytorch_fundamentals/#importing-pytorch","title":"Importing PyTorch\u00b6","text":"<p>Note: Before running any of the code in this notebook, you should have gone through the PyTorch setup steps.</p> <p>However, if you're running on Google Colab, everything should work (Google Colab comes with PyTorch and other libraries installed).</p> <p>Let's start by importing PyTorch and checking the version we're using.</p>"},{"location":"00_pytorch_fundamentals/#introduction-to-tensors","title":"Introduction to tensors\u00b6","text":"<p>Now we've got PyTorch imported, it's time to learn about tensors.</p> <p>Tensors are the fundamental building block of machine learning.</p> <p>Their job is to represent data in a numerical way.</p> <p>For example, you could represent an image as a tensor with shape <code>[3, 224, 224]</code> which would mean <code>[colour_channels, height, width]</code>, as in the image has <code>3</code> colour channels (red, green, blue), a height of <code>224</code> pixels and a width of <code>224</code> pixels.</p> <p></p> <p>In tensor-speak (the language used to describe tensors), the tensor would have three dimensions, one for <code>colour_channels</code>, <code>height</code> and <code>width</code>.</p> <p>But we're getting ahead of ourselves.</p> <p>Let's learn more about tensors by coding them.</p>"},{"location":"00_pytorch_fundamentals/#creating-tensors","title":"Creating tensors\u00b6","text":"<p>PyTorch loves tensors. So much so there's a whole documentation page dedicated to the <code>torch.Tensor</code> class.</p> <p>Your first piece of homework is to read through the documentation on <code>torch.Tensor</code> for 10-minutes. But you can get to that later.</p> <p>Let's code.</p> <p>The first thing we're going to create is a scalar.</p> <p>A scalar is a single number and in tensor-speak it's a zero dimension tensor.</p> <p>Note: That's a trend for this course. We'll focus on writing specific code. But often I'll set exercises which involve reading and getting familiar with the PyTorch documentation. Because after all, once you're finished this course, you'll no doubt want to learn more. And the documentation is somewhere you'll be finding yourself quite often.</p>"},{"location":"00_pytorch_fundamentals/#random-tensors","title":"Random tensors\u00b6","text":"<p>We've established tensors represent some form of data.</p> <p>And machine learning models such as neural networks manipulate and seek patterns within tensors.</p> <p>But when building machine learning models with PyTorch, it's rare you'll create tensors by hand (like what we've being doing).</p> <p>Instead, a machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.</p> <p>In essence:</p> <p><code>Start with random numbers -&gt; look at data -&gt; update random numbers -&gt; look at data -&gt; update random numbers...</code></p> <p>As a data scientist, you can define how the machine learning model starts (initialization), looks at data (representation) and updates (optimization) its random numbers.</p> <p>We'll get hands on with these steps later on.</p> <p>For now, let's see how to create a tensor of random numbers.</p> <p>We can do so using <code>torch.rand()</code> and passing in the <code>size</code> parameter.</p>"},{"location":"00_pytorch_fundamentals/#zeros-and-ones","title":"Zeros and ones\u00b6","text":"<p>Sometimes you'll just want to fill tensors with zeros or ones.</p> <p>This happens a lot with masking (like masking some of the values in one tensor with zeros to let a model know not to learn them).</p> <p>Let's create a tensor full of zeros with <code>torch.zeros()</code></p> <p>Again, the <code>size</code> parameter comes into play.</p>"},{"location":"00_pytorch_fundamentals/#creating-a-range-and-tensors-like","title":"Creating a range and tensors like\u00b6","text":"<p>Sometimes you might want a range of numbers, such as 1 to 10 or 0 to 100.</p> <p>You can use <code>torch.arange(start, end, step)</code> to do so.</p> <p>Where:</p> <ul> <li><code>start</code> = start of range (e.g. 0)</li> <li><code>end</code> = end of range (e.g. 10)</li> <li><code>step</code> = how many steps in between each value (e.g. 1)</li> </ul> <p>Note: In Python, you can use <code>range()</code> to create a range. However in PyTorch, <code>torch.range()</code> is deprecated and may show an error in the future.</p>"},{"location":"00_pytorch_fundamentals/#tensor-datatypes","title":"Tensor datatypes\u00b6","text":"<p>There are many different tensor datatypes available in PyTorch.</p> <p>Some are specific for CPU and some are better for GPU.</p> <p>Getting to know which is which can take some time.</p> <p>Generally if you see <code>torch.cuda</code> anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).</p> <p>The most common type (and generally the default) is <code>torch.float32</code> or <code>torch.float</code>.</p> <p>This is referred to as \"32-bit floating point\".</p> <p>But there's also 16-bit floating point (<code>torch.float16</code> or <code>torch.half</code>) and 64-bit floating point (<code>torch.float64</code> or <code>torch.double</code>).</p> <p>And to confuse things even more there's also 8-bit, 16-bit, 32-bit and 64-bit integers.</p> <p>Plus more!</p> <p>Note: An integer is a flat round number like <code>7</code> whereas a float has a decimal <code>7.0</code>.</p> <p>The reason for all of these is to do with precision in computing.</p> <p>Precision is the amount of detail used to describe a number.</p> <p>The higher the precision value (8, 16, 32), the more detail and hence data used to express a number.</p> <p>This matters in deep learning and numerical computing because you're making so many operations, the more detail you have to calculate on, the more compute you have to use.</p> <p>So lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).</p> <p>Resources:</p> <ul> <li>See the PyTorch documentation for a list of all available tensor datatypes.</li> <li>Read the Wikipedia page for an overview of what precision in computing is.</li> </ul> <p>Let's see how to create some tensors with specific datatypes. We can do so using the <code>dtype</code> parameter.</p>"},{"location":"00_pytorch_fundamentals/#getting-information-from-tensors","title":"Getting information from tensors\u00b6","text":"<p>Once you've created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.</p> <p>We've seen these before but three of the most common attributes you'll want to find out about tensors are:</p> <ul> <li><code>shape</code> - what shape is the tensor? (some operations require specific shape rules)</li> <li><code>dtype</code> - what datatype are the elements within the tensor stored in?</li> <li><code>device</code> - what device is the tensor stored on? (usually GPU or CPU)</li> </ul> <p>Let's create a random tensor and find out details about it.</p>"},{"location":"00_pytorch_fundamentals/#manipulating-tensors-tensor-operations","title":"Manipulating tensors (tensor operations)\u00b6","text":"<p>In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.</p> <p>A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.</p> <p>These operations are often a wonderful dance between:</p> <ul> <li>Addition</li> <li>Substraction</li> <li>Multiplication (element-wise)</li> <li>Division</li> <li>Matrix multiplication</li> </ul> <p>And that's it. Sure there are a few more here and there but these are the basic building blocks of neural networks.</p> <p>Stacking these building blocks in the right way, you can create the most sophisticated of neural networks (just like lego!).</p>"},{"location":"00_pytorch_fundamentals/#basic-operations","title":"Basic operations\u00b6","text":"<p>Let's start with a few of the fundamental operations, addition (<code>+</code>), subtraction (<code>-</code>), mutliplication (<code>*</code>).</p> <p>They work just as you think they would.</p>"},{"location":"00_pytorch_fundamentals/#matrix-multiplication-is-all-you-need","title":"Matrix multiplication (is all you need)\u00b6","text":"<p>One of the most common operations in machine learning and deep learning algorithms (like neural networks) is matrix multiplication.</p> <p>PyTorch implements matrix multiplication functionality in the <code>torch.matmul()</code> method.</p> <p>The main two rules for matrix multiplication to remember are:</p> <ol> <li>The inner dimensions must match:</li> </ol> <ul> <li><code>(3, 2) @ (3, 2)</code> won't work</li> <li><code>(2, 3) @ (3, 2)</code> will work</li> <li><code>(3, 2) @ (2, 3)</code> will work</li> </ul> <ol> <li>The resulting matrix has the shape of the outer dimensions:</li> </ol> <ul> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code></li> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>Note: \"<code>@</code>\" in Python is the symbol for matrix multiplication.</p> <p>Resource: You can see all of the rules for matrix multiplication using <code>torch.matmul()</code> in the PyTorch documentation.</p> <p>Let's create a tensor and perform element-wise multiplication and matrix multiplication on it.</p>"},{"location":"00_pytorch_fundamentals/#one-of-the-most-common-errors-in-deep-learning-shape-errors","title":"One of the most common errors in deep learning (shape errors)\u00b6","text":"<p>Because much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors you'll run into in deep learning is shape mismatches.</p>"},{"location":"00_pytorch_fundamentals/#finding-the-min-max-mean-sum-etc-aggregation","title":"Finding the min, max, mean, sum, etc (aggregation)\u00b6","text":"<p>Now we've seen a few ways to manipulate tensors, let's run through a few ways to aggregate them (go from more values to less values).</p> <p>First we'll create a tensor and then find the max, min, mean and sum of it.</p>"},{"location":"00_pytorch_fundamentals/#positional-minmax","title":"Positional min/max\u00b6","text":"<p>You can also find the index of a tensor where the max or minimum occurs with <code>torch.argmax()</code> and <code>torch.argmin()</code> respectively.</p> <p>This is helpful incase you just want the position where the highest (or lowest) value is and not the actual value itself (we'll see this in a later section when using the softmax activation function).</p>"},{"location":"00_pytorch_fundamentals/#change-tensor-datatype","title":"Change tensor datatype\u00b6","text":"<p>As mentioned, a common issue with deep learning operations is having your tensors in different datatypes.</p> <p>If one tensor is in <code>torch.float64</code> and another is in <code>torch.float32</code>, you might run into some errors.</p> <p>But there's a fix.</p> <p>You can change the datatypes of tensors using <code>torch.Tensor.type(dtype=None)</code> where the <code>dtype</code> parameter is the datatype you'd like to use.</p> <p>First we'll create a tensor and check it's datatype (the default is <code>torch.float32</code>).</p>"},{"location":"00_pytorch_fundamentals/#reshaping-stacking-squeezing-and-unsqueezing","title":"Reshaping, stacking, squeezing and unsqueezing\u00b6","text":"<p>Often times you'll want to reshape or change the dimensions of your tensors without actually changing the values inside them.</p> <p>To do so, some popular methods are:</p> Method One-line description <code>torch.reshape(input, shape)</code> Reshapes <code>input</code> to <code>shape</code> (if compatible), can also use <code>torch.Tensor.reshape()</code>. <code>torch.Tensor.view(shape)</code> Returns a view of the original tensor in a different <code>shape</code> but shares the same data as the original tensor. <code>torch.stack(tensors, dim=0)</code> Concatenates a sequence of <code>tensors</code> along a new dimension (<code>dim</code>), all <code>tensors</code> must be same size. <code>torch.squeeze(input)</code> Squeezes <code>input</code> to remove all the dimenions with value <code>1</code>. <code>torch.unsqueeze(input, dim)</code> Returns <code>input</code> with a dimension value of <code>1</code> added at <code>dim</code>. <code>torch.permute(input, dims)</code> Returns a view of the original <code>input</code> with its dimensions permuted (rearranged) to <code>dims</code>. <p>Why do any of these?</p> <p>Because deep learning models (neural networks) are all about manipulating tensors in some way. And because of the rules of matrix multiplication, if you've got shape mismatches, you'll run into errors. These methods help you make the right elements of your tensors are mixing with the right elements of other tensors.</p> <p>Let's try them out.</p> <p>First, we'll create a tensor.</p>"},{"location":"00_pytorch_fundamentals/#indexing-selecting-data-from-tensors","title":"Indexing (selecting data from tensors)\u00b6","text":"<p>Sometimes you'll want to select specific data from tensors (for example, only the first column or second row).</p> <p>To do so, you can use indexing.</p> <p>If you've ever done indexing on Python lists or NumPy arrays, indexing in PyTorch with tensors is very similar.</p>"},{"location":"00_pytorch_fundamentals/#pytorch-tensors-numpy","title":"PyTorch tensors &amp; NumPy\u00b6","text":"<p>Since NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.</p> <p>The two main methods you'll want to use for NumPy to PyTorch (and back again) are:</p> <ul> <li><code>torch.from_numpy(ndarray)</code> - NumPy array -&gt; PyTorch tensor.</li> <li><code>torch.Tensor.numpy()</code> - PyTorch tensor -&gt; NumPy array.</li> </ul> <p>Let's try them out.</p>"},{"location":"00_pytorch_fundamentals/#reproducibility-trying-to-take-the-random-out-of-random","title":"Reproducibility (trying to take the random out of random)\u00b6","text":"<p>As you learn more about neural networks and machine learning, you'll start to discover how much randomness plays a part.</p> <p>Well, pseudorandomness that is. Because after all, as they're designed, a computer is fundamentally deterministic (each step is predictable) so the randomness they create are simulated randomness (though there is debate on this too, but since I'm not a computer scientist, I'll let you find out more yourself).</p> <p>How does this relate to neural networks and deep learning then?</p> <p>We've discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we haven't discussed yet) to better describe patterns in data.</p> <p>In short:</p> <p><code>start with random numbers -&gt; tensor operations -&gt; try to make better (again and again and again)</code></p> <p>Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.</p> <p>Why?</p> <p>So you can perform repeatable experiments.</p> <p>For example, you create an algorithm capable of achieving X performance.</p> <p>And then your friend tries it out to verify you're not crazy.</p> <p>How could they do such a thing?</p> <p>That's where reproducibility comes in.</p> <p>In other words, can you get the same (or very similar) results on your computer running the same code as I get on mine?</p> <p>Let's see a brief example of reproducibility in PyTorch.</p> <p>We'll start by creating two random tensors, since they're random, you'd expect them to be different right?</p>"},{"location":"00_pytorch_fundamentals/#running-tensors-on-gpus-and-making-faster-computations","title":"Running tensors on GPUs (and making faster computations)\u00b6","text":"<p>Deep learning algorithms require a lot of numerical operations.</p> <p>And by default these operations are often done on a CPU (computer processing unit).</p> <p>However, there's another common piece of hardware called a GPU (graphics processing unit), which is often much faster at performing the specific types of operations neural networks need (matrix multiplications) than CPUs.</p> <p>Your computer might have one.</p> <p>If so, you should look to use it whenever you can to train neural networks because chances are it'll speed up the training time dramatically.</p> <p>There are a few ways to first get access to a GPU and secondly get PyTorch to use the GPU.</p> <p>Note: When I reference \"GPU\" throughout this course, I'm referencing a Nvidia GPU with CUDA enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing &amp; not just graphics) unless otherwise specified.</p>"},{"location":"00_pytorch_fundamentals/#1-getting-a-gpu","title":"1. Getting a GPU\u00b6","text":"<p>You may already know what's going on when I say GPU. But if not, there are a few ways to get access to one.</p> Method Difficulty to setup Pros Cons How to setup Google Colab Easy Free to use, almost zero setup required, can share work with others as easy as a link Doesn't save your data outputs, limited compute, subject to timeouts Follow the Google Colab Guide Use your own Medium Run everything locally on your own machine GPUs aren't free, require upfront cost Follow the PyTorch installation guidelines Cloud computing (AWS, GCP, Azure) Medium-Hard Small upfront cost, access to almost infinite compute Can get expensive if running continually, takes some time to setup right Follow the PyTorch installation guidelines <p>There are more options for using GPUs but the above three will suffice for now.</p> <p>Personally, I use a combination of Google Colab and my own personal computer for small scale experiments (and creating this course) and go to cloud resources when I need more compute power.</p> <p>Resource: If you're looking to purchase a GPU of your own but not sure what to get, Tim Dettmers has an excellent guide.</p> <p>To check if you've got access to a Nvidia GPU, you can run <code>!nvidia-smi</code> where the <code>!</code> (also called bang) means \"run this on the command line\".</p>"},{"location":"00_pytorch_fundamentals/#2-getting-pytorch-to-run-on-the-gpu","title":"2. Getting PyTorch to run on the GPU\u00b6","text":"<p>Once you've got a GPU ready to access, the next step is getting PyTorch to use for storing data (tensors) and computing on data (performing operations on tensors).</p> <p>To do so, you can use the <code>torch.cuda</code> package.</p> <p>Rather than talk about it, let's try it out.</p> <p>You can test if PyTorch has access to a GPU using <code>torch.cuda.is_available()</code>.</p>"},{"location":"00_pytorch_fundamentals/#3-putting-tensors-and-models-on-the-gpu","title":"3. Putting tensors (and models) on the GPU\u00b6","text":"<p>You can put tensors (and models, we'll see this later) on a specific device by calling <code>to(device)</code> on them. Where <code>device</code> is the target device you'd like the tensor (or model) to go to.</p> <p>Why do this?</p> <p>GPUs offer far faster numerical computing than CPUs do and if a GPU isn't available, because of our device agnostic code (see above), it'll run on the CPU.</p> <p>Note: Putting a tensor on GPU using <code>to(device)</code> (e.g. <code>some_tensor.to(device)</code>) returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:</p> <p><code>some_tensor = some_tensor.to(device)</code></p> <p>Let's try creating a tensor and putting it on the GPU (if it's available).</p>"},{"location":"00_pytorch_fundamentals/#4-moving-tensors-back-to-the-cpu","title":"4. Moving tensors back to the CPU\u00b6","text":"<p>What if we wanted to move the tensor back to CPU?</p> <p>For example, you'll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).</p> <p>Let's try using the <code>torch.Tensor.numpy()</code> method on our <code>tensor_on_gpu</code>.</p>"},{"location":"00_pytorch_fundamentals/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 00.</li> <li>Example solutions notebook for 00 (try the exercises before looking at this).</li> </ul> <ol> <li>Documentation reading - A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get some things for now, the focus is not yet full understanding, it's awareness). See the documentation on <code>torch.Tensor</code> and for <code>torch.cuda</code>.</li> <li>Create a random tensor with shape <code>(7, 7)</code>.</li> <li>Perform a matrix multiplication on the tensor from 2 with another random tensor with shape <code>(1, 7)</code> (hint: you may have to transpose the second tensor).</li> <li>Set the random seed to <code>0</code> and do exercises 2 &amp; 3 over again.</li> <li>Speaking of random seeds, we saw how to set it with <code>torch.manual_seed()</code> but is there a GPU equivalent? (hint: you'll need to look into the documentation for <code>torch.cuda</code> for this one). If there is, set the GPU random seed to <code>1234</code>.</li> <li>Create two random tensors of shape <code>(2, 3)</code> and send them both to the GPU (you'll need access to a GPU for this). Set <code>torch.manual_seed(1234)</code> when creating the tensors (this doesn't have to be the GPU random seed).</li> <li>Perform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors).</li> <li>Find the maximum and minimum values of the output of 7.</li> <li>Find the maximum and minimum index values of the output of 7.</li> <li>Make a random tensor with shape <code>(1, 1, 1, 10)</code> and then create a new tensor with all the <code>1</code> dimensions removed to be left with a tensor of shape <code>(10)</code>. Set the seed to <code>7</code> when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape.</li> </ol>"},{"location":"00_pytorch_fundamentals/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Spend 1-hour going through the PyTorch basics tutorial (I'd recommend the Quickstart and Tensors sections).</li> <li>To learn more on how a tensor can represent data, see this video: What's a tensor?</li> </ul>"},{"location":"01_pytorch_workflow/","title":"01. PyTorch Workflow Fundamentals","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>what_were_covering = {1: \"data (prepare and load)\",\n    2: \"build model\",\n    3: \"fitting the model to data (training)\",\n    4: \"making predictions and evaluating a model (inference)\",\n    5: \"saving and loading a model\",\n    6: \"putting it all together\"\n}\n</pre> what_were_covering = {1: \"data (prepare and load)\",     2: \"build model\",     3: \"fitting the model to data (training)\",     4: \"making predictions and evaluating a model (inference)\",     5: \"saving and loading a model\",     6: \"putting it all together\" } <p>And now let's import what we'll need for this module.</p> <p>We're going to get <code>torch</code>, <code>torch.nn</code> (<code>nn</code> stands for neural network and this package contains the building blocks for creating neural networks in PyTorch) and <code>matplotlib</code>.</p> In\u00a0[2]: Copied! <pre>import torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[2]: <pre>'1.12.1+cu113'</pre> In\u00a0[3]: Copied! <pre># Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n</pre> # Create *known* parameters weight = 0.7 bias = 0.3  # Create data start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) y = weight * X + bias  X[:10], y[:10] Out[3]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>Beautiful! Now we're going to move towards building a model that can learn the relationship between <code>X</code> (features) and <code>y</code> (labels).</p> In\u00a0[4]: Copied! <pre># Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Create train/test split train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[4]: <pre>(40, 40, 10, 10)</pre> <p>Wonderful, we've got 40 samples for training (<code>X_train</code> &amp; <code>y_train</code>) and 10 samples for testing (<code>X_test</code> &amp; <code>y_test</code>).</p> <p>The model we create is going to try and learn the relationship between <code>X_train</code> &amp; <code>y_train</code> and then we will evaluate what it learns on <code>X_test</code> and <code>y_test</code>.</p> <p>But right now our data is just numbers on a page.</p> <p>Let's create a function to visualize it.</p> In\u00a0[5]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n\"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n  \n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=None):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))    # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")      # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")    if predictions is not None:     # Plot the predictions in red (predictions were made on the test data)     plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")    # Show the legend   plt.legend(prop={\"size\": 14}); In\u00a0[6]: Copied! <pre>plot_predictions();\n</pre> plot_predictions(); <p>Epic!</p> <p>Now instead of just being numbers on a page, our data is a straight line.</p> <p>Note: Now's a good time to introduce you to the data explorer's motto... \"visualize, visualize, visualize!\"</p> <p>Think of this whenever you're working with data and turning it into numbers, if you can visualize something, it can do wonders for understanding.</p> <p>Machines love numbers and we humans like numbers too but we also like to look at things.</p> In\u00a0[7]: Copied! <pre># Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                                dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                   requires_grad=True) # &lt;- can we update this value with gradient descent?)\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                            dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                requires_grad=True) # &lt;- can we update this value with gradient descent?))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n</pre> # Create a Linear Regression model class class LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)     def __init__(self):         super().__init__()          self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)                                                 dtype=torch.float), # &lt;- PyTorch loves float32 by default                                    requires_grad=True) # &lt;- can we update this value with gradient descent?)          self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)                                             dtype=torch.float), # &lt;- PyTorch loves float32 by default                                 requires_grad=True) # &lt;- can we update this value with gradient descent?))      # Forward defines the computation in the model     def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)         return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b) <p>Alright there's a fair bit going on above but let's break it down bit by bit.</p> <p>Resource: We'll be using Python classes to create bits and pieces for building neural networks. If you're unfamiliar with Python class notation, I'd recommend reading Real Python's Object Orientating programming in Python 3 guide a few times.</p> In\u00a0[8]: Copied! <pre># Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n</pre> # Set manual seed since nn.Parameter are randomly initialzied torch.manual_seed(42)  # Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s)) model_0 = LinearRegressionModel()  # Check the nn.Parameter(s) within the nn.Module subclass we created list(model_0.parameters()) Out[8]: <pre>[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]</pre> <p>We can also get the state (what the model contains) of the model using <code>.state_dict()</code>.</p> In\u00a0[9]: Copied! <pre># List named parameters \nmodel_0.state_dict()\n</pre> # List named parameters  model_0.state_dict() Out[9]: <pre>OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])</pre> <p>Notice how the values for <code>weights</code> and <code>bias</code> from <code>model_0.state_dict()</code> come out as random float tensors?</p> <p>This is because we initialized them above using <code>torch.randn()</code>.</p> <p>Essentially we want to start from random parameters and get the model to update them towards parameters that fit our data best (the hardcoded <code>weight</code> and <code>bias</code> values we set when creating our straight line data).</p> <p>Exercise: Try changing the <code>torch.manual_seed()</code> value two cells above, see what happens to the weights and bias values.</p> <p>Because our model starts with random values, right now it'll have poor predictive power.</p> In\u00a0[10]: Copied! <pre># Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n</pre> # Make predictions with model with torch.inference_mode():      y_preds = model_0(X_test)  # Note: in older PyTorch code you might also see torch.no_grad() # with torch.no_grad(): #   y_preds = model_0(X_test) <p>Hmm?</p> <p>You probably noticed we used <code>torch.inference_mode()</code> as a context manager (that's what the <code>with torch.inference_mode():</code> is) to make the predictions.</p> <p>As the name suggests, <code>torch.inference_mode()</code> is used when using a model for inference (making predictions).</p> <p><code>torch.inference_mode()</code> turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make forward-passes (data going through the <code>forward()</code> method) faster.</p> <p>Note: In older PyTorch code, you may also see <code>torch.no_grad()</code> being used for inference. While <code>torch.inference_mode()</code> and <code>torch.no_grad()</code> do similar things,</p> <p><code>torch.inference_mode()</code> is newer, potentially faster and preferred. See this Tweet from PyTorch for more.</p> <p>We've made some predictions, let's see what they look like.</p> In\u00a0[11]: Copied! <pre># Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n</pre> # Check the predictions print(f\"Number of testing samples: {len(X_test)}\")  print(f\"Number of predictions made: {len(y_preds)}\") print(f\"Predicted values:\\n{y_preds}\") <pre>Number of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n</pre> <p>Notice how there's one prediction value per testing sample.</p> <p>This is because of the kind of data we're using. For our straight line, one <code>X</code> value maps to one <code>y</code> value.</p> <p>However, machine learning models are very flexible. You could have 100 <code>X</code> values mapping to one, two, three or 10 <code>y</code> values. It all depends on what you're working on.</p> <p>Our predictions are still numbers on a page, let's visualize them with our <code>plot_predictions()</code> function we created above.</p> In\u00a0[12]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) In\u00a0[13]: Copied! <pre>y_test - y_preds\n</pre> y_test - y_preds Out[13]: <pre>tensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])</pre> <p>Woah! Those predictions look pretty bad...</p> <p>This make sense though when you remember our model is just using random parameter values to make predictions.</p> <p>It hasn't even looked at the blue dots to try to predict the green dots.</p> <p>Time to change that.</p> In\u00a0[14]: Copied! <pre># Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n</pre> # Create the loss function loss_fn = nn.L1Loss() # MAE loss is same as L1Loss  # Create the optimizer optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize                             lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time)) In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n</pre> torch.manual_seed(42)  # Set the number of epochs (how many times the model will pass over the training data) epochs = 100  # Create empty loss lists to track values train_loss_values = [] test_loss_values = [] epoch_count = []  for epoch in range(epochs):     ### Training      # Put model in training mode (this is the default state of a model)     model_0.train()      # 1. Forward pass on train data using the forward() method inside      y_pred = model_0(X_train)     # print(y_pred)      # 2. Calculate the loss (how different are our models predictions to the ground truth)     loss = loss_fn(y_pred, y_train)      # 3. Zero grad of the optimizer     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Progress the optimizer     optimizer.step()      ### Testing      # Put the model in evaluation mode     model_0.eval()      with torch.inference_mode():       # 1. Forward pass on test data       test_pred = model_0(X_test)        # 2. Caculate loss on test data       test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type        # Print out what's happening       if epoch % 10 == 0:             epoch_count.append(epoch)             train_loss_values.append(loss.detach().numpy())             test_loss_values.append(test_loss.detach().numpy())             print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \") <pre>Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \nEpoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 \n</pre> <p>Oh would you look at that! Looks like our loss is going down with every epoch, let's plot it to find out.</p> In\u00a0[16]: Copied! <pre># Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Plot the loss curves plt.plot(epoch_count, train_loss_values, label=\"Train loss\") plt.plot(epoch_count, test_loss_values, label=\"Test loss\") plt.title(\"Training and test loss curves\") plt.ylabel(\"Loss\") plt.xlabel(\"Epochs\") plt.legend(); <p>Nice! The loss curves show the loss going down over time. Remember, loss is the measure of how wrong your model is, so the lower the better.</p> <p>But why did the loss go down?</p> <p>Well, thanks to our loss function and optimizer, the model's internal parameters (<code>weights</code> and <code>bias</code>) were updated to better reflect the underlying patterns in the data.</p> <p>Let's inspect our model's <code>.state_dict()</code> to see see how close our model gets to the original values we set for weights and bias.</p> In\u00a0[17]: Copied! <pre># Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters print(\"The model learned the following values for weights and bias:\") print(model_0.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.5784])), ('bias', tensor([0.3513]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>Wow! How cool is that?</p> <p>Our model got very close to calculate the exact original values for <code>weight</code> and <code>bias</code> (and it would probably get even closer if we trained it for longer).</p> <p>Exercise: Try changing the <code>epochs</code> value above to 200, what happens to the loss curves and the weights and bias parameter values of the model?</p> <p>It'd likely never guess them perfectly (especially when using more complicated datasets) but that's okay, often you can do very cool things with a close approximation.</p> <p>This is the whole idea of machine learning and deep learning, there are some ideal values that describe our data and rather than figuring them out by hand, we can train a model to figure them out programmatically.</p> In\u00a0[18]: Copied! <pre># 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n</pre> # 1. Set the model in evaluation mode model_0.eval()  # 2. Setup the inference mode context manager with torch.inference_mode():   # 3. Make sure the calculations are done with the model and data on the same device   # in our case, we haven't setup device-agnostic code yet so our data and model are   # on the CPU by default.   # model_0.to(device)   # X_test = X_test.to(device)   y_preds = model_0(X_test) y_preds Out[18]: <pre>tensor([[0.8141],\n        [0.8256],\n        [0.8372],\n        [0.8488],\n        [0.8603],\n        [0.8719],\n        [0.8835],\n        [0.8950],\n        [0.9066],\n        [0.9182]])</pre> <p>Nice! We've made some predictions with our trained model, now how do they look?</p> In\u00a0[19]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) <p>Woohoo! Those red dots are looking far closer than they were before!</p> <p>Let's get onto saving an reloading a model in PyTorch.</p> In\u00a0[20]: Copied! <pre>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # 1. Create models directory  MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Create model save path  MODEL_NAME = \"01_pytorch_workflow_model_0.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Save the model state dict  print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <pre>Saving model to: models/01_pytorch_workflow_model_0.pth\n</pre> In\u00a0[21]: Copied! <pre># Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n</pre> # Check the saved file path !ls -l models/01_pytorch_workflow_model_0.pth <pre>-rw-rw-r-- 1 daniel daniel 1063 Nov 10 16:07 models/01_pytorch_workflow_model_0.pth\n</pre> In\u00a0[22]: Copied! <pre># Instantiate a new instance of our model (this will be instantiated with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</pre> # Instantiate a new instance of our model (this will be instantiated with random weights) loaded_model_0 = LinearRegressionModel()  # Load the state_dict of our saved model (this will update the new instance of our model with trained weights) loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) Out[22]: <pre>&lt;All keys matched successfully&gt;</pre> <p>Excellent! It looks like things matched up.</p> <p>Now to test our loaded model, let's perform inference with it (make predictions) on the test data.</p> <p>Remember the rules for performing inference with PyTorch models?</p> <p>If not, here's a refresher:</p> PyTorch inference rules <ol> <li> Set the model in evaluation mode (<code>model.eval()</code>). </li> <li> Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>). </li> <li> All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li> </ol> In\u00a0[23]: Copied! <pre># 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n</pre> # 1. Put the loaded model into evaluation mode loaded_model_0.eval()  # 2. Use the inference mode context manager to make predictions with torch.inference_mode():     loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model <p>Now we've made some predictions with the loaded model, let's see if they're the same as the previous predictions.</p> In\u00a0[24]: Copied! <pre># Compare previous model predictions with loaded model predictions (these should be the same)\ny_preds == loaded_model_preds\n</pre> # Compare previous model predictions with loaded model predictions (these should be the same) y_preds == loaded_model_preds Out[24]: <pre>tensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])</pre> <p>Nice!</p> <p>It looks like the loaded model predictions are the same as the previous model predictions (predictions made prior to saving). This indicates our model is saving and loading as expected.</p> <p>Note: There are more methods to save and load PyTorch models but I'll leave these for extra-curriculum and further reading. See the PyTorch guide for saving and loading models for more.</p> In\u00a0[25]: Copied! <pre># Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> # Import PyTorch and matplotlib import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[25]: <pre>'1.12.1+cu113'</pre> <p>Now let's start making our code device agnostic by setting <code>device=\"cuda\"</code> if it's available, otherwise it'll default to <code>device=\"cpu\"</code>.</p> In\u00a0[26]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>If you've got access to a GPU, the above should've printed out:</p> <pre><code>Using device: cuda\n</code></pre> <p>Otherwise, you'll be using a CPU for the following computations. This is fine for our small dataset but it will take longer for larger datasets.</p> In\u00a0[27]: Copied! <pre># Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:10], y[:10]\n</pre> # Create weight and bias weight = 0.7 bias = 0.3  # Create range values start = 0 end = 1 step = 0.02  # Create X and y (features and labels) X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers) y = weight * X + bias  X[:10], y[:10] Out[27]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>Wonderful!</p> <p>Now we've got some data, let's split it into training and test sets.</p> <p>We'll use an 80/20 split with 80% training data and 20% testing data.</p> In\u00a0[28]: Copied! <pre># Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Split data train_split = int(0.8 * len(X)) X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[28]: <pre>(40, 40, 10, 10)</pre> <p>Excellent, let's visualize them to make sure they look okay.</p> In\u00a0[29]: Copied! <pre># Note: If you've reset your runtime, this function won't work, \n# you'll have to rerun the cell above where it's instantiated.\nplot_predictions(X_train, y_train, X_test, y_test)\n</pre> # Note: If you've reset your runtime, this function won't work,  # you'll have to rerun the cell above where it's instantiated. plot_predictions(X_train, y_train, X_test, y_test) In\u00a0[30]: Copied! <pre># Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n</pre> # Subclass nn.Module to make our model class LinearRegressionModelV2(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  # Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens) torch.manual_seed(42) model_1 = LinearRegressionModelV2() model_1, model_1.state_dict() Out[30]: <pre>(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))</pre> <p>Notice the outputs of <code>model_1.state_dict()</code>, the <code>nn.Linear()</code> layer created a random <code>weight</code> and <code>bias</code> parameter for us.</p> <p>Now let's put our model on the GPU (if it's available).</p> <p>We can change the device our PyTorch objects are on using <code>.to(device)</code>.</p> <p>First let's check the model's current device.</p> In\u00a0[31]: Copied! <pre># Check model device\nnext(model_1.parameters()).device\n</pre> # Check model device next(model_1.parameters()).device Out[31]: <pre>device(type='cpu')</pre> <p>Wonderful, looks like the model's on the CPU by default.</p> <p>Let's change it to be on the GPU (if it's available).</p> In\u00a0[32]: Copied! <pre># Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n</pre> # Set model to GPU if it's availalble, otherwise it'll default to CPU model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not next(model_1.parameters()).device Out[32]: <pre>device(type='cuda', index=0)</pre> <p>Nice! Because of our device agnostic code, the above cell will work regardless of whether a GPU is available or not.</p> <p>If you do have access to a CUDA-enabled GPU, you should see an output of something like:</p> <pre><code>device(type='cuda', index=0)\n</code></pre> <p>Time to build a training and testing loop.</p> <p>First we'll need a loss function and an optimizer.</p> <p>Let's use the same functions we used earlier, <code>nn.L1Loss()</code> and <code>torch.optim.SGD()</code>.</p> <p>We'll have to pass the new model's parameters (<code>model.parameters()</code>) to the optimizer for it to adjust them during training.</p> <p>The learning rate of <code>0.01</code> worked well before too so let's use that again.</p> In\u00a0[33]: Copied! <pre># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Create loss function loss_fn = nn.L1Loss()  # Create optimizer optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) <p>Beautiful, loss function and optimizer ready, now let's train and evaluate our model using a training and testing loop.</p> <p>The only different thing we'll be doing in this step compared to the previous training loop is putting the data on the target <code>device</code>.</p> <p>We've already put our model on the target <code>device</code> using <code>model_1.to(device)</code>.</p> <p>And we can do the same with the data.</p> <p>That way if the model is on the GPU, the data is on the GPU (and vice versa).</p> <p>Let's step things up a notch this time and set <code>epochs=1000</code>.</p> <p>If you need a reminder of the PyTorch training loop steps, see below.</p> PyTorch training loop steps <ol> <li>Forward pass - The model goes through all of the training data once, performing its             <code>forward()</code> function             calculations (<code>model(x_train)</code>).         </li> <li>Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated             to see how             wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they             can be             recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li> <li>Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model             parameter to             be updated (each parameter             with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\"             (<code>loss.backward()</code>).</li> <li>Step the optimizer (gradient descent) - Update the parameters with <code>requires_grad=True</code>             with respect to the loss             gradients in order to improve them (<code>optimizer.step()</code>).</li> </ol> In\u00a0[34]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Set the number of epochs  epochs = 1000   # Put data on the available device # Without this, error will happen (not all model/data on device) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <pre>Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089\nEpoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\n</pre> <p>Note: Due to the random nature of machine learning, you will likely get slightly different results (different loss and prediction values) depending on whether your model was trained on CPU or GPU. This is true even if you use the same random seed on either device. If the difference is large, you may want to look for errors, however, if it is small (ideally it is), you can ignore it.</p> <p>Nice! That loss looks pretty low.</p> <p>Let's check the parameters our model has learned and compare them to the original parameters we hard-coded.</p> In\u00a0[35]: Copied! <pre># Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html  print(\"The model learned the following values for weights and bias:\") pprint(model_1.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='cuda:0')),\n             ('linear_layer.bias', tensor([0.3025], device='cuda:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>Ho ho! Now that's pretty darn close to a perfect model.</p> <p>Remember though, in practice, it's rare that you'll know the perfect parameters ahead of time.</p> <p>And if you knew the parameters your model had to learn ahead of time, what would be the fun of machine learning?</p> <p>Plus, in many real-world machine learning problems, the number of parameters can well exceed tens of millions.</p> <p>I don't know about you but I'd rather write code for a computer to figure those out rather than doing it by hand.</p> In\u00a0[36]: Copied! <pre># Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n</pre> # Turn model into evaluation mode model_1.eval()  # Make predictions on the test data with torch.inference_mode():     y_preds = model_1(X_test) y_preds Out[36]: <pre>tensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='cuda:0')</pre> <p>If you're making predictions with data on the GPU, you might notice the output of the above has <code>device='cuda:0'</code> towards the end. That means the data is on CUDA device 0 (the first GPU your system has access to due to zero-indexing), if you end up using multiple GPUs in the future, this number may be higher.</p> <p>Now let's plot our model's predictions.</p> <p>Note: Many data science libraries such as pandas, matplotlib and NumPy aren't capable of using data that is stored on GPU. So you might run into some issues when trying to use a function from one of these libraries with tensor data not stored on the CPU. To fix this, you can call <code>.cpu()</code> on your target tensor to return a copy of your target tensor on the CPU.</p> In\u00a0[37]: Copied! <pre># plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n</pre> # plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU  # Put data on the CPU and plot it plot_predictions(predictions=y_preds.cpu()) <p>Woah! Look at those red dots, they line up almost perfectly with the green dots. I guess the extra epochs helped.</p> In\u00a0[38]: Copied! <pre>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # 1. Create models directory  MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Create model save path  MODEL_NAME = \"01_pytorch_workflow_model_1.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Save the model state dict  print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <pre>Saving model to: models/01_pytorch_workflow_model_1.pth\n</pre> <p>And just to make sure everything worked well, let's load it back in.</p> <p>We'll:</p> <ul> <li>Create a new instance of the <code>LinearRegressionModelV2()</code> class</li> <li>Load in the model state dict using <code>torch.nn.Module.load_state_dict()</code></li> <li>Send the new instance of the model to the target device (to ensure our code is device-agnostic)</li> </ul> In\u00a0[39]: Copied! <pre># Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n</pre> # Instantiate a fresh instance of LinearRegressionModelV2 loaded_model_1 = LinearRegressionModelV2()  # Load model state dict  loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))  # Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions) loaded_model_1.to(device)  print(f\"Loaded model:\\n{loaded_model_1}\") print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\") <pre>Loaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\ncuda:0\n</pre> <p>Now we can evaluate the loaded model to see if its predictions line up with the predictions made prior to saving.</p> In\u00a0[40]: Copied! <pre># Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n</pre> # Evaluate loaded model loaded_model_1.eval() with torch.inference_mode():     loaded_model_1_preds = loaded_model_1(X_test) y_preds == loaded_model_1_preds Out[40]: <pre>tensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='cuda:0')</pre> <p>Everything adds up! Nice!</p> <p>Well, we've come a long way. You've now built and trained your first two neural network models in PyTorch!</p> <p>Time to practice your skills.</p>"},{"location":"01_pytorch_workflow/#01-pytorch-workflow-fundamentals","title":"01. PyTorch Workflow Fundamentals\u00b6","text":"<p>The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discoverd patterns to predict the future.</p> <p>There are many ways to do this and many new ways are being discovered all the time.</p> <p>But let's start small.</p> <p>How about we start with a straight line?</p> <p>And we see if we can build a PyTorch model that learns the pattern of the straight line and matches it.</p>"},{"location":"01_pytorch_workflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>In this module we're going to cover a standard PyTorch workflow (it can be chopped and changed as necessary but it covers the main outline of steps).</p> <p>For now, we'll use this workflow to predict a simple straight line but the workflow steps can be repeated and changed depending on the problem you're working on.</p> <p>Specifically, we're going to cover:</p> Topic Contents 1. Getting data ready Data can be almost anything but to get started we're going to create a simple straight line 2. Building a model Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop. 3. Fitting the model to data (training) We've got data and a model, now let's let the model (try to) find patterns in the (training) data. 4. Making predictions and evaluating a model (inference) Our model's found patterns in the data, let's compare its findings to the actual (testing) data. 5. Saving and loading a model You may want to use your model elsewhere, or come back to it later, here we'll cover that. 6. Putting it all together Let's take all of the above and combine it."},{"location":"01_pytorch_workflow/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p> <p>Let's start by putting what we're covering into a dictionary to reference later.</p>"},{"location":"01_pytorch_workflow/#1-data-preparing-and-loading","title":"1. Data (preparing and loading)\u00b6","text":"<p>I want to stress that \"data\" in machine learning can be almost anything you can imagine. A table of numbers (like a big Excel spreadsheet), images of any kind, videos (YouTube has lots of data!), audio files like songs or podcasts, protein structures, text and more.</p> <p></p> <p>Machine learning is a game of two parts:</p> <ol> <li>Turn your data, whatever it is, into numbers (a representation).</li> <li>Pick or build a model to learn the representation as best as possible.</li> </ol> <p>Sometimes one and two can be done at the same time.</p> <p>But what if you don't have data?</p> <p>Well, that's where we're at now.</p> <p>No data.</p> <p>But we can create some.</p> <p>Let's create our data as a straight line.</p> <p>We'll use linear regression to create the data with known parameters (things that can be learned by a model) and then we'll use PyTorch to see if we can build model to estimate these parameters using gradient descent.</p> <p>Don't worry if the terms above don't mean much now, we'll see them in action and I'll put extra resources below where you can learn more.</p>"},{"location":"01_pytorch_workflow/#split-data-into-training-and-test-sets","title":"Split data into training and test sets\u00b6","text":"<p>We've got some data.</p> <p>But before we build a model we need to split it up.</p> <p>One of most important steps in a machine learning project is creating a training and test set (and when required, a validation set).</p> <p>Each split of the dataset serves a specific purpose:</p> Split Purpose Amount of total data How often is it used? Training set The model learns from this data (like the course materials you study during the semester). ~60-80% Always Validation set The model gets tuned on this data (like the practice exam you take before the final exam). ~10-20% Often but not always Testing set The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). ~10-20% Always <p>For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.</p> <p>We can create them by splitting our <code>X</code> and <code>y</code> tensors.</p> <p>Note: When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.</p>"},{"location":"01_pytorch_workflow/#2-build-model","title":"2. Build model\u00b6","text":"<p>Now we've got some data, let's build a model to use the blue dots to predict the green dots.</p> <p>We're going to jump right in.</p> <p>We'll write the code first and then explain everything.</p> <p>Let's replicate a standard linear regression model using pure PyTorch.</p>"},{"location":"01_pytorch_workflow/#pytorch-model-building-essentials","title":"PyTorch model building essentials\u00b6","text":"<p>PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.</p> <p>They are <code>torch.nn</code>, <code>torch.optim</code>, <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code>. For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).</p> PyTorch module What does it do? <code>torch.nn</code> Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). <code>torch.nn.Parameter</code> Stores tensors that can be used with <code>nn.Module</code>. If <code>requires_grad=True</code> gradients (used for updating model parameters via gradient descent)  are calculated automatically, this is often referred to as \"autograd\". <code>torch.nn.Module</code> The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass <code>nn.Module</code>. Requires a <code>forward()</code> method be implemented. <code>torch.optim</code> Contains various optimization algorithms (these tell the model parameters stored in <code>nn.Parameter</code> how to best change to improve gradient descent and in turn reduce the loss). <code>def forward()</code> All <code>nn.Module</code> subclasses require a <code>forward()</code> method, this defines the computation that will take place on the data passed to the particular <code>nn.Module</code> (e.g. the linear regression formula above). <p>If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from <code>torch.nn</code>,</p> <ul> <li><code>nn.Module</code> contains the larger building blocks (layers)</li> <li><code>nn.Parameter</code> contains the smaller parameters like weights and biases (put these together to make <code>nn.Module</code>(s))</li> <li><code>forward()</code> tells the larger blocks how to make calculations on inputs (tensors full of data) within  <code>nn.Module</code>(s)</li> <li><code>torch.optim</code> contains optimization methods on how to improve the parameters within <code>nn.Parameter</code> to better represent input data</li> </ul> <p> Basic building blocks of creating a PyTorch model by subclassing <code>nn.Module</code>. For objects that subclass <code>nn.Module</code>, the <code>forward()</code> method must be defined.</p> <p>Resource: See more of these essential modules and their uses cases in the PyTorch Cheat Sheet.</p>"},{"location":"01_pytorch_workflow/#checking-the-contents-of-a-pytorch-model","title":"Checking the contents of a PyTorch model\u00b6","text":"<p>Now we've got these out of the way, let's create a model instance with the class we've made and check its parameters using <code>.parameters()</code>.</p>"},{"location":"01_pytorch_workflow/#making-predictions-using-torchinference_mode","title":"Making predictions using <code>torch.inference_mode()</code>\u00b6","text":"<p>To check this we can pass it the test data <code>X_test</code> to see how closely it predicts <code>y_test</code>.</p> <p>When we pass data to our model, it'll go through the model's <code>forward()</code> method and produce a result using the computation we've defined.</p> <p>Let's make some predictions.</p>"},{"location":"01_pytorch_workflow/#3-train-model","title":"3. Train model\u00b6","text":"<p>Right now our model is making predictions using random parameters to make calculations, it's basically guessing (randomly).</p> <p>To fix that, we can update its internal parameters (I also refer to parameters as patterns), the <code>weights</code> and <code>bias</code> values we set randomly using <code>nn.Parameter()</code> and <code>torch.randn()</code> to be something that better represents the data.</p> <p>We could hard code this (since we know the default values <code>weight=0.7</code> and <code>bias=0.3</code>) but where's the fun in that?</p> <p>Much of the time you won't know what the ideal parameters are for a model.</p> <p>Instead, it's much more fun to write code to see if the model can try and figure them out itself.</p>"},{"location":"01_pytorch_workflow/#creating-a-loss-function-and-optimizer-in-pytorch","title":"Creating a loss function and optimizer in PyTorch\u00b6","text":"<p>For our model to update its parameters on its own, we'll need to add a few more things to our recipe.</p> <p>And that's a loss function as well as an optimizer.</p> <p>The rolls of these are:</p> Function What does it do? Where does it live in PyTorch? Common values Loss function Measures how wrong your models predictions (e.g. <code>y_preds</code>) are compared to the truth labels (e.g. <code>y_test</code>). Lower the better. PyTorch has plenty of built-in loss functions in <code>torch.nn</code>. Mean absolute error (MAE) for regression problems (<code>torch.nn.L1Loss()</code>). Binary cross entropy for binary classification problems (<code>torch.nn.BCELoss()</code>). Optimizer Tells your model how to update its internal parameters to best lower the loss. You can find various optimization function implementations in <code>torch.optim</code>. Stochastic gradient descent (<code>torch.optim.SGD()</code>). Adam optimizer (<code>torch.optim.Adam()</code>). <p>Let's create a loss function and an optimizer we can use to help improve our model.</p> <p>Depending on what kind of problem you're working on will depend on what loss function and what optimizer you use.</p> <p>However, there are some common values, that are known to work well such as the SGD (stochastic gradient descent) or Adam optimizer. And the MAE (mean absolute error) loss function for regression problems (predicting a number) or binary cross entropy loss function for classification problems (predicting one thing or another).</p> <p>For our problem, since we're predicting a number, let's use MAE (which is under <code>torch.nn.L1Loss()</code>) in PyTorch as our loss function.</p> <p> Mean absolute error (MAE, in PyTorch: <code>torch.nn.L1Loss</code>) measures the absolute difference between two points (predictions and labels) and then takes the mean across all examples.</p> <p>And we'll use SGD, <code>torch.optim.SGD(params, lr)</code> where:</p> <ul> <li><code>params</code> is the target model parameters you'd like to optimize (e.g. the <code>weights</code> and <code>bias</code> values we randomly set before).</li> <li><code>lr</code> is the learning rate you'd like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparameter (because it's set by a machine learning engineer). Common starting values for the learning rate are <code>0.01</code>, <code>0.001</code>, <code>0.0001</code>, however, these can also be adjusted over time (this is called learning rate scheduling).</li> </ul> <p>Woah, that's a lot, let's see it in code.</p>"},{"location":"01_pytorch_workflow/#creating-an-optimization-loop-in-pytorch","title":"Creating an optimization loop in PyTorch\u00b6","text":"<p>Woohoo! Now we've got a loss function and an optimizer, it's now time to create a training loop (and testing loop).</p> <p>The training loop involves the model going through the training data and learning the relationships between the <code>features</code> and <code>labels</code>.</p> <p>The testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never see's the testing data during training).</p> <p>Each of these is called a \"loop\" because we want our model to look (loop through) at each sample in each dataset.</p> <p>To create these we're going to write a Python <code>for</code> loop in the theme of the unofficial PyTorch optimization loop song (there's a video version too).</p> <p> The unoffical PyTorch optimization loops song, a fun way to remember the steps in a PyTorch training (and testing) loop.</p> <p>There will be a fair bit of code but nothing we can't handle.</p>"},{"location":"01_pytorch_workflow/#pytorch-training-loop","title":"PyTorch training loop\u00b6","text":"<p>For the training loop, we'll build the following steps:</p> Number Step name What does it do? Code example 1 Forward pass The model goes through all of the training data once, performing its <code>forward()</code> function calculations. <code>model(x_train)</code> 2 Calculate the loss The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. <code>loss = loss_fn(y_pred, y_train)</code> 3 Zero gradients The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. <code>optimizer.zero_grad()</code> 4 Perform backpropagation on the loss Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\". <code>loss.backward()</code> 5 Update the optimizer (gradient descent) Update the parameters with <code>requires_grad=True</code> with respect to the loss gradients in order to improve them. <code>optimizer.step()</code> <p></p> <p>Note: The above is just one example of how the steps could be ordered or described. With experience you'll find making PyTorch training loops can be quite flexible.</p> <p>And on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb:</p> <ul> <li>Calculate the loss (<code>loss = ...</code>) before performing backpropagation on it (<code>loss.backward()</code>).</li> <li>Zero gradients (<code>optimizer.zero_grad()</code>) before stepping them (<code>optimizer.step()</code>).</li> <li>Step the optimizer (<code>optimizer.step()</code>) after performing backpropagation on the loss (<code>loss.backward()</code>).</li> </ul> <p>For resources to help understand what's happening behind the scenes with backpropagation and gradient descent, see the extra-curriculum section.</p>"},{"location":"01_pytorch_workflow/#pytorch-testing-loop","title":"PyTorch testing loop\u00b6","text":"<p>As for the testing loop (evaluating our model), the typical steps include:</p> Number Step name What does it do? Code example 1 Forward pass The model goes through all of the training data once, performing its <code>forward()</code> function calculations. <code>model(x_test)</code> 2 Calculate the loss The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. <code>loss = loss_fn(y_pred, y_test)</code> 3 Calulate evaluation metrics (optional) Alongisde the loss value you may want to calculate other evaluation metrics such as accuracy on the test set. Custom functions <p>Notice the testing loop doesn't contain performing backpropagation (<code>loss.backward()</code>) or stepping the optimizer (<code>optimizer.step()</code>), this is because no parameters in the model are being changed during testing, they've already been calculated. For testing, we're only interested in the output of the forward pass through the model.</p> <p></p> <p>Let's put all of the above together and train our model for 100 epochs (forward passes through the data) and we'll evaluate it every 10 epochs.</p>"},{"location":"01_pytorch_workflow/#4-making-predictions-with-a-trained-pytorch-model-inference","title":"4. Making predictions with a trained PyTorch model (inference)\u00b6","text":"<p>Once you've trained a model, you'll likely want to make predictions with it.</p> <p>We've already seen a glimpse of this in the training and testing code above, the steps to do it outside of the training/testing loop are similar.</p> <p>There are three things to remember when making predictions (also called performing inference) with a PyTorch model:</p> <ol> <li>Set the model in evaluation mode (<code>model.eval()</code>).</li> <li>Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>).</li> <li>All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li> </ol> <p>The first two items make sure all helpful calculations and settings PyTorch uses behind the scenes during training but aren't necessary for inference are turned off (this results in faster computation). And the third ensures that you won't run into cross-device errors.</p>"},{"location":"01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model","title":"5. Saving and loading a PyTorch model\u00b6","text":"<p>If you've trained a PyTorch model, chances are you'll want to save it and export it somewhere.</p> <p>As in, you might train it on Google Colab or your local machine with a GPU but you'd like to now export it to some sort of application where others can use it.</p> <p>Or maybe you'd like to save your progress on a model and come back and load it back later.</p> <p>For saving and loading models in PyTorch, there are three main methods you should be aware of (all of below have been taken from the PyTorch saving and loading models guide):</p> PyTorch method What does it do? <code>torch.save</code> Saves a serialized object to disk using Python's <code>pickle</code> utility. Models, tensors and various other Python objects like dictionaries can be saved using <code>torch.save</code>. <code>torch.load</code> Uses <code>pickle</code>'s unpickling features to deserialize and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc). <code>torch.nn.Module.load_state_dict</code> Loads a model's parameter dictionary (<code>model.state_dict()</code>) using a saved <code>state_dict()</code> object. <p>Note: As stated in Python's <code>pickle</code> documentation, the <code>pickle</code> module is not secure. That means you should only ever unpickle (load) data you trust. That goes for loading PyTorch models as well. Only ever use saved PyTorch models from sources you trust.</p>"},{"location":"01_pytorch_workflow/#saving-a-pytorch-models-state_dict","title":"Saving a PyTorch model's <code>state_dict()</code>\u00b6","text":"<p>The recommended way for saving and loading a model for inference (making predictions) is by saving and loading a model's <code>state_dict()</code>.</p> <p>Let's see how we can do that in a few steps:</p> <ol> <li>We'll create a directory for saving models to called <code>models</code> using Python's <code>pathlib</code> module.</li> <li>We'll create a file path to save the model to.</li> <li>We'll call <code>torch.save(obj, f)</code> where <code>obj</code> is the target model's <code>state_dict()</code> and <code>f</code> is the filename of where to save the model.</li> </ol> <p>Note: It's common convention for PyTorch saved models or objects to end with <code>.pt</code> or <code>.pth</code>, like <code>saved_model_01.pth</code>.</p>"},{"location":"01_pytorch_workflow/#loading-a-saved-pytorch-models-state_dict","title":"Loading a saved PyTorch model's <code>state_dict()</code>\u00b6","text":"<p>Since we've now got a saved model <code>state_dict()</code> at <code>models/01_pytorch_workflow_model_0.pth</code> we can now load it in using <code>torch.nn.Module.load_state_dict(torch.load(f))</code> where <code>f</code> is the filepath of our saved model <code>state_dict()</code>.</p> <p>Why call <code>torch.load()</code> inside <code>torch.nn.Module.load_state_dict()</code>?</p> <p>Because we only saved the model's <code>state_dict()</code> which is a dictionary of learned parameters and not the entire model, we first have to load the <code>state_dict()</code> with <code>torch.load()</code> and then pass that <code>state_dict()</code> to a new instance of our model (which is a subclass of <code>nn.Module</code>).</p> <p>Why not save the entire model?</p> <p>Saving the entire model rather than just the <code>state_dict()</code> is more intuitive, however, to quote the PyTorch documentation (italics mine):</p> <p>The disadvantage of this approach (saving the whole model) is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved...</p> <p>Because of this, your code can break in various ways when used in other projects or after refactors.</p> <p>So instead, we're using the flexible method of saving and loading just the <code>state_dict()</code>, which again is basically a dictionary of model parameters.</p> <p>Let's test it out by created another instance of <code>LinearRegressionModel()</code>, which is a subclass of <code>torch.nn.Module</code> and will hence have the in-built method <code>load_state_dit()</code>.</p>"},{"location":"01_pytorch_workflow/#6-putting-it-all-together","title":"6. Putting it all together\u00b6","text":"<p>We've covered a fair bit of ground so far.</p> <p>But once you've had some practice, you'll be performing the above steps like dancing down the street.</p> <p>Speaking of practice, let's put everything we've done so far together.</p> <p>Except this time we'll make our code device agnostic (so if there's a GPU available, it'll use it and if not, it will default to the CPU).</p> <p>There'll be far less commentary in this section than above since what we're going to go through has already been covered.</p> <p>We'll start by importing the standard libraries we need.</p> <p>Note: If you're using Google Colab, to setup a GPU, go to Runtime -&gt; Change runtime type -&gt; Hardware acceleration -&gt; GPU. If you do this, it will reset the Colab runtime and you will lose saved variables.</p>"},{"location":"01_pytorch_workflow/#61-data","title":"6.1 Data\u00b6","text":"<p>Let's create some data just like before.</p> <p>First, we'll hard-code some <code>weight</code> and <code>bias</code> values.</p> <p>Then we'll make a range of numbers between 0 and 1, these will be our <code>X</code> values.</p> <p>Finally, we'll use the <code>X</code> values, as well as the <code>weight</code> and <code>bias</code> values to create <code>y</code> using the linear regression formula (<code>y = weight * X + bias</code>).</p>"},{"location":"01_pytorch_workflow/#62-building-a-pytorch-linear-model","title":"6.2 Building a PyTorch linear model\u00b6","text":"<p>We've got some data, now it's time to make a model.</p> <p>We'll create the same style of model as before except this time, instead of defining the weight and bias parameters of our model manually using <code>nn.Parameter()</code>, we'll use <code>nn.Linear(in_features, out_features)</code> to do it for us.</p> <p>Where <code>in_features</code> is the number of dimensions your input data has and <code>out_features</code> is the number of dimensions you'd like it to be output to.</p> <p>In our case, both of these are <code>1</code> since our data has <code>1</code> input feature (<code>X</code>) per label (<code>y</code>).</p> <p> Creating a linear regression model using <code>nn.Parameter</code> versus using <code>nn.Linear</code>. There are plenty more examples of where the <code>torch.nn</code> module has pre-built computations, including many popular and useful neural network layers.</p>"},{"location":"01_pytorch_workflow/#63-training","title":"6.3 Training\u00b6","text":""},{"location":"01_pytorch_workflow/#64-making-predictions","title":"6.4 Making predictions\u00b6","text":"<p>Now we've got a trained model, let's turn on it's evaluation mode and make some predictions.</p>"},{"location":"01_pytorch_workflow/#65-saving-and-loading-a-model","title":"6.5 Saving and loading a model\u00b6","text":"<p>We're happy with our models predictions, so let's save it to file so it can be used later.</p>"},{"location":"01_pytorch_workflow/#exercises","title":"Exercises\u00b6","text":"<p>All exercises have been inspired from code throughout the notebook.</p> <p>There is one exercise per major section.</p> <p>You should be able to complete them by referencing their specific section.</p> <p>Note: For all exercises, your code should be device agnostic (meaning it could run on CPU or GPU if it's available).</p> <ol> <li>Create a straight line dataset using the linear regression formula (<code>weight * X + bias</code>).</li> </ol> <ul> <li>Set <code>weight=0.3</code> and <code>bias=0.9</code> there should be at least 100 datapoints total.</li> <li>Split the data into 80% training, 20% testing.</li> <li>Plot the training and testing data so it becomes visual.</li> </ul> <ol> <li>Build a PyTorch model by subclassing <code>nn.Module</code>.</li> </ol> <ul> <li>Inside should be a randomly initialized <code>nn.Parameter()</code> with <code>requires_grad=True</code>, one for <code>weights</code> and one for <code>bias</code>.</li> <li>Implement the <code>forward()</code> method to compute the linear regression function you used to create the dataset in 1.</li> <li>Once you've constructed the model, make an instance of it and check its <code>state_dict()</code>.</li> <li>Note: If you'd like to use <code>nn.Linear()</code> instead of <code>nn.Parameter()</code> you can.</li> </ul> <ol> <li>Create a loss function and optimizer using <code>nn.L1Loss()</code> and <code>torch.optim.SGD(params, lr)</code> respectively.</li> </ol> <ul> <li>Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.</li> <li>Write a training loop to perform the appropriate training steps for 300 epochs.</li> <li>The training loop should test the model on the test dataset every 20 epochs.</li> </ul> <ol> <li>Make predictions with the trained model on the test data.</li> </ol> <ul> <li>Visualize these predictions against the original training and testing data (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).</li> </ul> <ol> <li>Save your trained model's <code>state_dict()</code> to file.</li> </ol> <ul> <li>Create a new instance of your model class you made in 2. and load in the <code>state_dict()</code> you just saved to it.</li> <li>Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.</li> </ul> <p>Resource: See the exercises notebooks templates and solutions on the course GitHub.</p>"},{"location":"01_pytorch_workflow/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Listen to The Unofficial PyTorch Optimization Loop Song (to help remember the steps in a PyTorch training/testing loop).</li> <li>Read What is <code>torch.nn</code>, really? by Jeremy Howard for a deeper understanding of how one of the most important modules in PyTorch works.</li> <li>Spend 10-minutes scrolling through and checking out the PyTorch documentation cheatsheet for all of the different PyTorch modules you might come across.</li> <li>Spend 10-minutes reading the loading and saving documentation on the PyTorch website to become more familiar with the different saving and loading options in PyTorch.</li> <li>Spend 1-2 hours read/watching the following for an overview of the internals of gradient descent and backpropagation, the two main algorithms that have been working in the background to help our model learn.</li> <li>Wikipedia page for gradient descent</li> <li>Gradient Descent Algorithm \u2014 a deep dive by Robert Kwiatkowski</li> <li>Gradient descent, how neural networks learn video by 3Blue1Brown</li> <li>What is backpropagation really doing? video by 3Blue1Brown</li> <li>Backpropagation Wikipedia Page</li> </ul>"},{"location":"02_pytorch_classification/","title":"02. PyTorch Neural Network Classification","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>from sklearn.datasets import make_circles\n\n\n# Make 1000 samples \nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03, # a little bit of noise to the dots\n                    random_state=42) # keep random state so we get the same values\n</pre> from sklearn.datasets import make_circles   # Make 1000 samples  n_samples = 1000  # Create circles X, y = make_circles(n_samples,                     noise=0.03, # a little bit of noise to the dots                     random_state=42) # keep random state so we get the same values <p>Alright, now let's view the first 5 <code>X</code> and <code>y</code> values.</p> In\u00a0[2]: Copied! <pre>print(f\"First 5 X features:\\n{X[:5]}\")\nprint(f\"\\nFirst 5 y labels:\\n{y[:5]}\")\n</pre> print(f\"First 5 X features:\\n{X[:5]}\") print(f\"\\nFirst 5 y labels:\\n{y[:5]}\") <pre>First 5 X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst 5 y labels:\n[1 1 1 1 0]\n</pre> <p>Looks like there's two <code>X</code> values per one <code>y</code> value.</p> <p>Let's keep following the data explorer's motto of visualize, visualize, visualize and put them into a pandas DataFrame.</p> In\u00a0[3]: Copied! <pre># Make DataFrame of circle data\nimport pandas as pd\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n    \"X2\": X[:, 1],\n    \"label\": y\n})\ncircles.head(10)\n</pre> # Make DataFrame of circle data import pandas as pd circles = pd.DataFrame({\"X1\": X[:, 0],     \"X2\": X[:, 1],     \"label\": y }) circles.head(10) Out[3]: X1 X2 label 0 0.754246 0.231481 1 1 -0.756159 0.153259 1 2 -0.815392 0.173282 1 3 -0.393731 0.692883 1 4 0.442208 -0.896723 0 5 -0.479646 0.676435 1 6 -0.013648 0.803349 1 7 0.771513 0.147760 1 8 -0.169322 -0.793456 1 9 -0.121486 1.021509 0 <p>It looks like each pair of <code>X</code> features (<code>X1</code> and <code>X2</code>) has a label (<code>y</code>) value of either 0 or 1.</p> <p>This tells us that our problem is binary classification since there's only two options (0 or 1).</p> <p>How many values of each class is there?</p> In\u00a0[4]: Copied! <pre># Check different labels\ncircles.label.value_counts()\n</pre> # Check different labels circles.label.value_counts() Out[4]: <pre>1    500\n0    500\nName: label, dtype: int64</pre> <p>500 each, nice and balanced.</p> <p>Let's plot them.</p> In\u00a0[5]: Copied! <pre># Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0], \n            y=X[:, 1], \n            c=y, \n            cmap=plt.cm.RdYlBu);\n</pre> # Visualize with a plot import matplotlib.pyplot as plt plt.scatter(x=X[:, 0],              y=X[:, 1],              c=y,              cmap=plt.cm.RdYlBu); <p>Alrighty, looks like we've got a problem to solve.</p> <p>Let's find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).</p> <p>Note: This dataset is often what's considered a toy problem (a problem that's used to try and test things out on) in machine learning.</p> <p>But it represents the major key of classification, you have some kind of data represented as numerical values and you'd like to build a model that's able to classify it, in our case, separate it into red or blue dots.</p> In\u00a0[6]: Copied! <pre># Check the shapes of our features and labels\nX.shape, y.shape\n</pre> # Check the shapes of our features and labels X.shape, y.shape Out[6]: <pre>((1000, 2), (1000,))</pre> <p>Looks like we've got a match on the first dimension of each.</p> <p>There's 1000 <code>X</code> and 1000 <code>y</code>.</p> <p>But what's the second dimension on <code>X</code>?</p> <p>It often helps to view the values and shapes of a single sample (features and labels).</p> <p>Doing so will help you understand what input and output shapes you'd be expecting from your model.</p> In\u00a0[7]: Copied! <pre># View the first example of features and labels\nX_sample = X[0]\ny_sample = y[0]\nprint(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\nprint(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")\n</pre> # View the first example of features and labels X_sample = X[0] y_sample = y[0] print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\") print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\") <pre>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for one sample of X: (2,) and the same for y: ()\n</pre> <p>This tells us the second dimension for <code>X</code> means it has two features (vector) where as <code>y</code> has a single feature (scalar).</p> <p>We have two inputs for one output.</p> In\u00a0[8]: Copied! <pre># Turn data into tensors\n# Otherwise this causes issues with computations later on\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# View the first five samples\nX[:5], y[:5]\n</pre> # Turn data into tensors # Otherwise this causes issues with computations later on import torch X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # View the first five samples X[:5], y[:5] Out[8]: <pre>(tensor([[ 0.7542,  0.2315],\n         [-0.7562,  0.1533],\n         [-0.8154,  0.1733],\n         [-0.3937,  0.6929],\n         [ 0.4422, -0.8967]]),\n tensor([1., 1., 1., 1., 0.]))</pre> <p>Now our data is in tensor format, let's split it into training and test sets.</p> <p>To do so, let's use the helpful function <code>train_test_split()</code> from Scikit-Learn.</p> <p>We'll use <code>test_size=0.2</code> (80% training, 20% testing) and because the split happens randomly across the data, let's use <code>random_state=42</code> so the split is reproducible.</p> In\u00a0[9]: Copied! <pre># Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, # 20% test, 80% train\n                                                    random_state=42) # make the random split reproducible\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Split data into train and test sets from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2, # 20% test, 80% train                                                     random_state=42) # make the random split reproducible  len(X_train), len(X_test), len(y_train), len(y_test) Out[9]: <pre>(800, 200, 800, 200)</pre> <p>Nice! Looks like we've now got 800 training samples and 200 testing samples.</p> In\u00a0[10]: Copied! <pre># Standard PyTorch imports\nimport torch\nfrom torch import nn\n\n# Make device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Standard PyTorch imports import torch from torch import nn  # Make device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[10]: <pre>'cuda'</pre> <p>Excellent, now <code>device</code> is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it's available.</p> <p>How about we create a model?</p> <p>We'll want a model capable of handling our <code>X</code> data as inputs and producing something in the shape of our <code>y</code> data as ouputs.</p> <p>In other words, given <code>X</code> (features) we want our model to predict <code>y</code> (label).</p> <p>This setup where you have features and labels is referred to as supervised learning. Because your data is telling your model what the outputs should be given a certain input.</p> <p>To create such a model it'll need to handle the input and output shapes of <code>X</code> and <code>y</code>.</p> <p>Remember how I said input and output shapes are important? Here we'll see why.</p> <p>Let's create a model class that:</p> <ol> <li>Subclasses <code>nn.Module</code> (almost all PyTorch models are subclasses of <code>nn.Module</code>).</li> <li>Creates 2 <code>nn.Linear</code> layers in the constructor capable of handling the input and output shapes of <code>X</code> and <code>y</code>.</li> <li>Defines a <code>forward()</code> method containing the forward pass computation of the model.</li> <li>Instantiates the model class and sends it to the target <code>device</code>.</li> </ol> In\u00a0[11]: Copied! <pre># 1. Construct a model class that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n    \n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n</pre> # 1. Construct a model class that subclasses nn.Module class CircleModelV0(nn.Module):     def __init__(self):         super().__init__()         # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes         self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features         self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)          # 3. Define a forward method containing the forward pass computation     def forward(self, x):         # Return the output of layer_2, a single feature, the same shape as y         return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2  # 4. Create an instance of the model and send it to target device model_0 = CircleModelV0().to(device) model_0 Out[11]: <pre>CircleModelV0(\n  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>What's going on here?</p> <p>We've seen a few of these steps before.</p> <p>The only major change is what's happening between <code>self.layer_1</code> and <code>self.layer_2</code>.</p> <p><code>self.layer_1</code> takes 2 input features <code>in_features=2</code> and produces 5 output features <code>out_features=5</code>.</p> <p>This is known as having 5 hidden units or neurons.</p> <p>This layer turns the input data from having 2 features to 5 features.</p> <p>Why do this?</p> <p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, potentially leading to better outputs.</p> <p>I say potentially because sometimes it doesn't work.</p> <p>The number of hidden units you can use in neural network layers is a hyperparameter (a value you can set yourself) and there's no set in stone value you have to use.</p> <p>Generally more is better but there's also such a thing as too much. The amount you choose will depend on your model type and dataset you're working with.</p> <p>Since our dataset is small and simple, we'll keep it small.</p> <p>The only rule with hidden units is that the next layer, in our case, <code>self.layer_2</code> has to take the same <code>in_features</code> as the previous layer <code>out_features</code>.</p> <p>That's why <code>self.layer_2</code> has <code>in_features=5</code>, it takes the <code>out_features=5</code> from <code>self.layer_1</code> and performs a linear computation on them, turning them into <code>out_features=1</code> (the same shape as <code>y</code>).</p> <p> A visual example of what a similar classificiation neural network to the one we've just built looks like. Try create one of your own on the TensorFlow Playground website.</p> <p>You can also do the same as above using <code>nn.Sequential</code>.</p> <p><code>nn.Sequential</code> performs a forward pass computation of the input data through the layers in the order they appear.</p> In\u00a0[12]: Copied! <pre># Replicate CircleModelV0 with nn.Sequential\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),\n    nn.Linear(in_features=5, out_features=1)\n).to(device)\n\nmodel_0\n</pre> # Replicate CircleModelV0 with nn.Sequential model_0 = nn.Sequential(     nn.Linear(in_features=2, out_features=5),     nn.Linear(in_features=5, out_features=1) ).to(device)  model_0 Out[12]: <pre>Sequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>Woah, that looks much simpler than subclassing <code>nn.Module</code>, why not just always use <code>nn.Sequential</code>?</p> <p><code>nn.Sequential</code> is fantastic for straight-forward computations, however, as the namespace says, it always runs in sequential order.</p> <p>So if you'd something else to happen (rather than just straight-forward sequential computation) you'll want to define your own custom <code>nn.Module</code> subclass.</p> <p>Now we've got a model, let's see what happens when we pass some data through it.</p> In\u00a0[13]: Copied! <pre># Make predictions with the model\nuntrained_preds = model_0(X_test.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\nprint(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n</pre> # Make predictions with the model untrained_preds = model_0(X_test.to(device)) print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\") print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\") print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\") print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\") <pre>Length of predictions: 200, Shape: torch.Size([200, 1])\nLength of test samples: 200, Shape: torch.Size([200])\n\nFirst 10 predictions:\ntensor([[-0.4279],\n        [-0.3417],\n        [-0.5975],\n        [-0.3801],\n        [-0.5078],\n        [-0.4559],\n        [-0.2842],\n        [-0.3107],\n        [-0.6010],\n        [-0.3350]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nFirst 10 test labels:\ntensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n</pre> <p>Hmm, it seems there's the same amount of predictions as there is test labels but the predictions don't look like they're in the same form or shape as the test labels.</p> <p>We've got a couple steps we can do to fix this, we'll see these later on.</p> In\u00a0[14]: Copied! <pre># Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n</pre> # Create a loss function # loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in  # Create an optimizer optimizer = torch.optim.SGD(params=model_0.parameters(),                              lr=0.1) <p>Now let's also create an evaluation metric.</p> <p>An evaluation metric can be used to offer another perspective on how your model is going.</p> <p>If a loss function measures how wrong your model is, I like to think of evaluation metrics as measuring how right it is.</p> <p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p> <p>After all, when evaluating your models it's good to look at things from multiple points of view.</p> <p>There are several evaluation metrics that can be used for classification problems but let's start out with accuracy.</p> <p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p> <p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p> <p>Let's write a function to do so.</p> In\u00a0[15]: Copied! <pre># Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n</pre> # Calculate accuracy (a classification metric) def accuracy_fn(y_true, y_pred):     correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal     acc = (correct / len(y_pred)) * 100      return acc <p>Excellent! We can now use this function whilst training our model to measure it's performance alongside the loss.</p> In\u00a0[16]: Copied! <pre># View the frist 5 outputs of the forward pass on the test data\ny_logits = model_0(X_test.to(device))[:5]\ny_logits\n</pre> # View the frist 5 outputs of the forward pass on the test data y_logits = model_0(X_test.to(device))[:5] y_logits Out[16]: <pre>tensor([[-0.4279],\n        [-0.3417],\n        [-0.5975],\n        [-0.3801],\n        [-0.5078]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</pre> <p>Since our model hasn't been trained, these outputs are basically random.</p> <p>But what are they?</p> <p>They're the output of our <code>forward()</code> method.</p> <p>Which implements two layers of <code>nn.Linear()</code> which internally calls the following equation:</p> $$ \\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias} $$<p>The raw outputs (unmodified) of this equation ($\\mathbf{y}$) and in turn, the raw outputs of our model are often referred to as logits.</p> <p>That's what our model is outputing above when it takes in the input data ($x$ in the equation or <code>X_test</code> in the code), logits.</p> <p>However, these numbers are hard to interpret.</p> <p>We'd like some numbers that are comparable to our truth labels.</p> <p>To get our model's raw outputs (logits) into such a form, we can use the sigmoid activation function.</p> <p>Let's try it out.</p> In\u00a0[17]: Copied! <pre># Use sigmoid on model logits\ny_pred_probs = torch.sigmoid(y_logits)\ny_pred_probs\n</pre> # Use sigmoid on model logits y_pred_probs = torch.sigmoid(y_logits) y_pred_probs Out[17]: <pre>tensor([[0.3946],\n        [0.4154],\n        [0.3549],\n        [0.4061],\n        [0.3757]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</pre> <p>Okay, it seems like the outputs now have some kind of consistency (even though they're still random).</p> <p>They're now in the form of prediction probabilities (I usually refer to these as <code>y_pred_probs</code>), in other words, the values are now how much the model thinks the data point belongs to one class or another.</p> <p>In our case, since we're dealing with binary classification, our ideal outputs are 0 or 1.</p> <p>So these values can be viewed as a decision boundary.</p> <p>The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.</p> <p>More specificially:</p> <ul> <li>If <code>y_pred_probs</code> &gt;= 0.5, <code>y=1</code> (class 1)</li> <li>If <code>y_pred_probs</code> &lt; 0.5, <code>y=0</code> (class 0)</li> </ul> <p>To turn our prediction probabilities in prediction labels, we can round the outputs of the sigmoid activation function.</p> In\u00a0[18]: Copied! <pre># Find the predicted labels (round the prediction probabilities)\ny_preds = torch.round(y_pred_probs)\n\n# In full\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# Check for equality\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# Get rid of extra dimension\ny_preds.squeeze()\n</pre> # Find the predicted labels (round the prediction probabilities) y_preds = torch.round(y_pred_probs)  # In full y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))  # Check for equality print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))  # Get rid of extra dimension y_preds.squeeze() <pre>tensor([True, True, True, True, True], device='cuda:0')\n</pre> Out[18]: <pre>tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)</pre> <p>Excellent! Now it looks like our model's predictions are in the same form as our truth labels (<code>y_test</code>).</p> In\u00a0[19]: Copied! <pre>y_test[:5]\n</pre> y_test[:5] Out[19]: <pre>tensor([1., 0., 1., 0., 1.])</pre> <p>This means we'll be able to compare our models predictions to the test labels to see how well it's going.</p> <p>To recap, we converted our model's raw outputs (logits) to predicition probabilities using a sigmoid activation function.</p> <p>And then converted the prediction probabilities to prediction labels by rounding them.</p> <p>Note: The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we'll be looking at using the softmax activation function (this will come later on).</p> <p>And the use of the sigmoid activation function is not required when passing our model's raw outputs to the <code>nn.BCEWithLogitsLoss</code> (the \"logits\" in logits loss is because it works on the model's raw logits output), this is because it has a sigmoid function built-in.</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 100\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train()\n\n    # 1. Forward pass (model outputs raw logits)\n    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls\n  \n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n    #                y_train) \n    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  # Set the number of epochs epochs = 100  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  # Build training and evaluation loop for epoch in range(epochs):     ### Training     model_0.train()      # 1. Forward pass (model outputs raw logits)     y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device      y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls        # 2. Calculate loss/accuracy     # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()     #                y_train)      loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits                    y_train)      acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)       # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_0.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_0(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.72090, Accuracy: 50.00% | Test loss: 0.72196, Test acc: 50.00%\nEpoch: 10 | Loss: 0.70291, Accuracy: 50.00% | Test loss: 0.70542, Test acc: 50.00%\nEpoch: 20 | Loss: 0.69659, Accuracy: 50.00% | Test loss: 0.69942, Test acc: 50.00%\nEpoch: 30 | Loss: 0.69432, Accuracy: 43.25% | Test loss: 0.69714, Test acc: 41.00%\nEpoch: 40 | Loss: 0.69349, Accuracy: 47.00% | Test loss: 0.69623, Test acc: 46.50%\nEpoch: 50 | Loss: 0.69319, Accuracy: 49.00% | Test loss: 0.69583, Test acc: 46.00%\nEpoch: 60 | Loss: 0.69308, Accuracy: 50.12% | Test loss: 0.69563, Test acc: 46.50%\nEpoch: 70 | Loss: 0.69303, Accuracy: 50.38% | Test loss: 0.69551, Test acc: 46.00%\nEpoch: 80 | Loss: 0.69302, Accuracy: 51.00% | Test loss: 0.69543, Test acc: 46.00%\nEpoch: 90 | Loss: 0.69301, Accuracy: 51.00% | Test loss: 0.69537, Test acc: 46.00%\n</pre> <p>Hmm, what do you notice about the performance of our model?</p> <p>It looks like it went through the training and testing steps fine but the results don't seem to have moved too much.</p> <p>The accuracy barely moves above 50% on each data split.</p> <p>And because we're working with a balanced binary classification problem, it means our model is performing as good as random guessing (with 500 samples of class 0 and class 1 a model predicting class 1 every single time would achieve 50% accuracy).</p> In\u00a0[21]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content)  from helper_functions import plot_predictions, plot_decision_boundary <pre>helper_functions.py already exists, skipping download\n</pre> <pre>/home/daniel/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/daniel/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warn(f\"Failed to load image Python extension: {e}\")\n</pre> In\u00a0[22]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_0, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_0, X_test, y_test) <p>Oh wow, it seems like we've found the cause of model's performance issue.</p> <p>It's currently trying to split the red and blue dots using a straight line...</p> <p>That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.</p> <p>In machine learning terms, our model is underfitting, meaning it's not learning predictive patterns from the data.</p> <p>How could we improve this?</p> In\u00a0[23]: Copied! <pre>class CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        \n    def forward(self, x): # note: always make sure forward is spelt correctly!\n        # Creating a model like this is the same as below, though below\n        # generally benefits from speedups where possible.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n</pre> class CircleModelV1(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer         self.layer_3 = nn.Linear(in_features=10, out_features=1)              def forward(self, x): # note: always make sure forward is spelt correctly!         # Creating a model like this is the same as below, though below         # generally benefits from speedups where possible.         # z = self.layer_1(x)         # z = self.layer_2(z)         # z = self.layer_3(z)         # return z         return self.layer_3(self.layer_2(self.layer_1(x)))  model_1 = CircleModelV1().to(device) model_1 Out[23]: <pre>CircleModelV1(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>Now we've got a model, we'll recreate a loss function and optimizer instance, using the same settings as before.</p> In\u00a0[24]: Copied! <pre># loss_fn = nn.BCELoss() # Requires sigmoid on input\nloss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\noptimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n</pre> # loss_fn = nn.BCELoss() # Requires sigmoid on input loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1) <p>Beautiful, model, optimizer and loss function ready, let's make a training loop.</p> <p>This time we'll train for longer (<code>epochs=1000</code> vs <code>epochs=100</code>) and see if it improves our model.</p> In\u00a0[25]: Copied! <pre>torch.manual_seed(42)\n\nepochs = 1000 # Train for longer\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    # 1. Forward pass\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels\n\n    # 2. Calculate loss/accuracy\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  epochs = 1000 # Train for longer  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     ### Training     # 1. Forward pass     y_logits = model_1(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels      # 2. Calculate loss/accuracy     loss = loss_fn(y_logits, y_train)     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_1.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_1(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%\nEpoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%\nEpoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%\nEpoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%\nEpoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%\nEpoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%\nEpoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\n</pre> <p>What? Our model trained for longer and with an extra layer but it still looks like it didn't learn any patterns better than random guessing.</p> <p>Let's visualize.</p> In\u00a0[26]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_1, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_1, X_test, y_test) <p>Hmmm.</p> <p>Our model is still drawing a straight line between the red and blue dots.</p> <p>If our model is drawing a straight line, could it model linear data? Like we did in notebook 01?</p> In\u00a0[27]: Copied! <pre># Create some data (same as notebook 01)\nweight = 0.7\nbias = 0.3\nstart = 0\nend = 1\nstep = 0.01\n\n# Create data\nX_regression = torch.arange(start, end, step).unsqueeze(dim=1)\ny_regression = weight * X_regression + bias # linear regression formula\n\n# Check the data\nprint(len(X_regression))\nX_regression[:5], y_regression[:5]\n</pre> # Create some data (same as notebook 01) weight = 0.7 bias = 0.3 start = 0 end = 1 step = 0.01  # Create data X_regression = torch.arange(start, end, step).unsqueeze(dim=1) y_regression = weight * X_regression + bias # linear regression formula  # Check the data print(len(X_regression)) X_regression[:5], y_regression[:5] <pre>100\n</pre> Out[27]: <pre>(tensor([[0.0000],\n         [0.0100],\n         [0.0200],\n         [0.0300],\n         [0.0400]]),\n tensor([[0.3000],\n         [0.3070],\n         [0.3140],\n         [0.3210],\n         [0.3280]]))</pre> <p>Wonderful, now let's split our data into training and test sets.</p> In\u00a0[28]: Copied! <pre># Create train and test splits\ntrain_split = int(0.8 * len(X_regression)) # 80% of data used for training set\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# Check the lengths of each split\nprint(len(X_train_regression), \n    len(y_train_regression), \n    len(X_test_regression), \n    len(y_test_regression))\n</pre> # Create train and test splits train_split = int(0.8 * len(X_regression)) # 80% of data used for training set X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split] X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]  # Check the lengths of each split print(len(X_train_regression),      len(y_train_regression),      len(X_test_regression),      len(y_test_regression)) <pre>80 80 20 20\n</pre> <p>Beautiful, let's see how the data looks.</p> <p>To do so, we'll use the <code>plot_predictions()</code> function we created in notebook 01.</p> <p>It's contained within the <code>helper_functions.py</code> script on the Learn PyTorch for Deep Learning repo which we downloaded above.</p> In\u00a0[29]: Copied! <pre>plot_predictions(train_data=X_train_regression,\n    train_labels=y_train_regression,\n    test_data=X_test_regression,\n    test_labels=y_test_regression\n);\n</pre> plot_predictions(train_data=X_train_regression,     train_labels=y_train_regression,     test_data=X_test_regression,     test_labels=y_test_regression ); In\u00a0[30]: Copied! <pre># Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n</pre> # Same architecture as model_1 (but using nn.Sequential) model_2 = nn.Sequential(     nn.Linear(in_features=1, out_features=10),     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=1) ).to(device)  model_2 Out[30]: <pre>Sequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>We'll setup the loss function to be <code>nn.L1Loss()</code> (the same as mean absolute error) and the optimizer to be <code>torch.optim.SGD()</code>.</p> In\u00a0[31]: Copied! <pre># Loss and optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n</pre> # Loss and optimizer loss_fn = nn.L1Loss() optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1) <p>Now let's train the model using the regular training loop steps for <code>epochs=1000</code> (just like <code>model_1</code>).</p> <p>Note: We've been writing similar training loop code over and over again. I've made it that way on purpose though, to keep practicing. However, do you have ideas how we could functionize this? That would save a fair bit of coding in the future. Potentially there could be a function for training and a function for testing.</p> In\u00a0[32]: Copied! <pre># Train the model\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put data to target device\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\nfor epoch in range(epochs):\n    ### Training \n    # 1. Forward pass\n    y_pred = model_2(X_train_regression)\n    \n    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_2.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_pred = model_2(X_test_regression)\n      # 2. Calculate the loss \n      test_loss = loss_fn(test_pred, y_test_regression)\n\n    # Print out what's happening\n    if epoch % 100 == 0: \n        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")\n</pre> # Train the model torch.manual_seed(42)  # Set the number of epochs epochs = 1000  # Put data to target device X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device) X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)  for epoch in range(epochs):     ### Training      # 1. Forward pass     y_pred = model_2(X_train_regression)          # 2. Calculate loss (no accuracy since it's a regression problem, not classification)     loss = loss_fn(y_pred, y_train_regression)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_2.eval()     with torch.inference_mode():       # 1. Forward pass       test_pred = model_2(X_test_regression)       # 2. Calculate the loss        test_loss = loss_fn(test_pred, y_test_regression)      # Print out what's happening     if epoch % 100 == 0:          print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\") <pre>Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143\nEpoch: 100 | Train loss: 0.09309, Test loss: 0.02901\nEpoch: 200 | Train loss: 0.07376, Test loss: 0.02850\nEpoch: 300 | Train loss: 0.06745, Test loss: 0.00615\nEpoch: 400 | Train loss: 0.06107, Test loss: 0.02004\nEpoch: 500 | Train loss: 0.05698, Test loss: 0.01061\nEpoch: 600 | Train loss: 0.04857, Test loss: 0.01326\nEpoch: 700 | Train loss: 0.06109, Test loss: 0.02127\nEpoch: 800 | Train loss: 0.05599, Test loss: 0.01426\nEpoch: 900 | Train loss: 0.05571, Test loss: 0.00603\n</pre> <p>Okay, unlike <code>model_1</code> on the classification data, it looks like <code>model_2</code>'s loss is actually going down.</p> <p>Let's plot its predictions to see if that's so.</p> <p>And remember, since our model and data are using the target <code>device</code>, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can't handle data on the GPU.</p> <p>To handle that, we'll send all of our data to the CPU using <code>.cpu()</code> when we pass it to <code>plot_predictions()</code>.</p> In\u00a0[33]: Copied! <pre># Turn on evaluation mode\nmodel_2.eval()\n\n# Make predictions (inference)\nwith torch.inference_mode():\n    y_preds = model_2(X_test_regression)\n\n# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n# (try removing .cpu() from one of the below and see what happens)\nplot_predictions(train_data=X_train_regression.cpu(),\n                 train_labels=y_train_regression.cpu(),\n                 test_data=X_test_regression.cpu(),\n                 test_labels=y_test_regression.cpu(),\n                 predictions=y_preds.cpu());\n</pre> # Turn on evaluation mode model_2.eval()  # Make predictions (inference) with torch.inference_mode():     y_preds = model_2(X_test_regression)  # Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU) # (try removing .cpu() from one of the below and see what happens) plot_predictions(train_data=X_train_regression.cpu(),                  train_labels=y_train_regression.cpu(),                  test_data=X_test_regression.cpu(),                  test_labels=y_test_regression.cpu(),                  predictions=y_preds.cpu()); <p>Alright, it looks like our model is able to do far better than random guessing on straight lines.</p> <p>This is a good thing.</p> <p>It means our model at least has some capacity to learn.</p> <p>Note: A helpful troubleshooting step when building deep learning models is to start as small as possible to see if the model works before scaling it up.</p> <p>This could mean starting with a simple neural network (not many layers, not many hidden neurons) and a small dataset (like the one we've made) and then overfitting (making the model perform too well) on that small example before increasing the amount data or the model size/design to reduce overfitting.</p> <p>So what could it be?</p> <p>Let's find out.</p> In\u00a0[34]: Copied! <pre># Make and plot data\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\nn_samples = 1000\n\nX, y = make_circles(n_samples=1000,\n    noise=0.03,\n    random_state=42,\n)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);\n</pre> # Make and plot data import matplotlib.pyplot as plt from sklearn.datasets import make_circles  n_samples = 1000  X, y = make_circles(n_samples=1000,     noise=0.03,     random_state=42, )  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu); <p>Nice! Now let's split it into training and test sets using 80% of the data for training and 20% for testing.</p> In\u00a0[35]: Copied! <pre># Convert to tensors and split into train and test sets\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n# Turn data into tensors\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42\n)\n\nX_train[:5], y_train[:5]\n</pre> # Convert to tensors and split into train and test sets import torch from sklearn.model_selection import train_test_split  # Turn data into tensors X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2,                                                     random_state=42 )  X_train[:5], y_train[:5] Out[35]: <pre>(tensor([[ 0.6579, -0.4651],\n         [ 0.6319, -0.7347],\n         [-1.0086, -0.1240],\n         [-0.9666, -0.2256],\n         [-0.1666,  0.7994]]),\n tensor([1., 0., 0., 0., 1.]))</pre> In\u00a0[36]: Copied! <pre># Build model with non-linear activation function\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- add in ReLU activation function\n        # Can also put sigmoid in the model \n        # This would mean you don't need to use it on the predictions\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # Intersperse the ReLU activation function between layers\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n</pre> # Build model with non-linear activation function from torch import nn class CircleModelV2(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10)         self.layer_3 = nn.Linear(in_features=10, out_features=1)         self.relu = nn.ReLU() # &lt;- add in ReLU activation function         # Can also put sigmoid in the model          # This would mean you don't need to use it on the predictions         # self.sigmoid = nn.Sigmoid()      def forward(self, x):       # Intersperse the ReLU activation function between layers        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))  model_3 = CircleModelV2().to(device) print(model_3) <pre>CircleModelV2(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)\n</pre> <p> A visual example of what a similar classificiation neural network to the one we've just built (using ReLU activation) looks like. Try create one of your own on the TensorFlow Playground website.</p> <p>Question: Where should I put the non-linear activation functions when constructing a neural network?</p> <p>A rule of thumb is to put them in between hidden layers and just after the output layer, however, there is no set in stone option. As you learn more about neural networks and deep learning you'll find a bunch of different ways of putting things together. In the meantine, best to experiment, experiment, experiment.</p> <p>Now we've got a model ready to go, let's create a binary classification loss function as well as an optimizer.</p> In\u00a0[37]: Copied! <pre># Setup loss and optimizer \nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n</pre> # Setup loss and optimizer  loss_fn = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1) <p>Wonderful!</p> In\u00a0[38]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\nepochs = 1000\n\n# Put all data on target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    # 1. Forward pass\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n    \n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n    \n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_3.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_3(X_test).squeeze()\n      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n      # 2. Calcuate loss and accuracy\n      test_loss = loss_fn(test_logits, y_test)\n      test_acc = accuracy_fn(y_true=y_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42) epochs = 1000  # Put all data on target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     # 1. Forward pass     y_logits = model_3(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels          # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)          # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_3.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_3(X_test).squeeze()       test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels       # 2. Calcuate loss and accuracy       test_loss = loss_fn(test_logits, y_test)       test_acc = accuracy_fn(y_true=y_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\n</pre> <p>Ho ho! That's looking far better!</p> In\u00a0[39]: Copied! <pre># Make predictions\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # want preds in same format as truth labels\n</pre> # Make predictions model_3.eval() with torch.inference_mode():     y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze() y_preds[:10], y[:10] # want preds in same format as truth labels Out[39]: <pre>(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),\n tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</pre> In\u00a0[40]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity <p>Nice! Not perfect but still far better than before.</p> <p>Potentially you could try a few tricks to improve the test accuracy of the model? (hint: head back to section 5 for tips on improving the model)</p> In\u00a0[41]: Copied! <pre># Create a toy tensor (similar to the data going into our model(s))\nA = torch.arange(-10, 10, 1, dtype=torch.float32)\nA\n</pre> # Create a toy tensor (similar to the data going into our model(s)) A = torch.arange(-10, 10, 1, dtype=torch.float32) A Out[41]: <pre>tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</pre> <p>Wonderful, now let's plot it.</p> In\u00a0[42]: Copied! <pre># Visualize the toy tensor\nplt.plot(A);\n</pre> # Visualize the toy tensor plt.plot(A); <p>A straight line, nice.</p> <p>Now let's see how the ReLU activation function influences it.</p> <p>And instead of using PyTorch's ReLU (<code>torch.nn.ReLU</code>), we'll recreate it ourselves.</p> <p>The ReLU function turns all negatives to 0 and leaves the positive values as they are.</p> In\u00a0[43]: Copied! <pre># Create ReLU function by hand \ndef relu(x):\n  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n\n# Pass toy tensor through ReLU function\nrelu(A)\n</pre> # Create ReLU function by hand  def relu(x):   return torch.maximum(torch.tensor(0), x) # inputs must be tensors  # Pass toy tensor through ReLU function relu(A) Out[43]: <pre>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n        8., 9.])</pre> <p>It looks like our ReLU function worked, all of the negative values are zeros.</p> <p>Let's plot them.</p> In\u00a0[44]: Copied! <pre># Plot ReLU activated toy tensor\nplt.plot(relu(A));\n</pre> # Plot ReLU activated toy tensor plt.plot(relu(A)); <p>Nice! That looks exactly like the shape of the ReLU function on the Wikipedia page for ReLU.</p> <p>How about we try the sigmoid function we've been using?</p> <p>The sigmoid function formula goes like so:</p> $$ out_i = \\frac{1}{1+e^{-input_i}} $$<p>Or using $x$ as input:</p> $$ S(x) = \\frac{1}{1+e^{-x_i}} $$<p>Where $S$ stands for sigmoid, $e$ stands for exponential (<code>torch.exp()</code>) and $i$ stands for a particular element in a tensor.</p> <p>Let's build a function to replicate the sigmoid function with PyTorch.</p> In\u00a0[45]: Copied! <pre># Create a custom sigmoid function\ndef sigmoid(x):\n  return 1 / (1 + torch.exp(-x))\n\n# Test custom sigmoid on toy tensor\nsigmoid(A)\n</pre> # Create a custom sigmoid function def sigmoid(x):   return 1 / (1 + torch.exp(-x))  # Test custom sigmoid on toy tensor sigmoid(A) Out[45]: <pre>tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n        9.9966e-01, 9.9988e-01])</pre> <p>Woah, those values look a lot like prediction probabilities we've seen earlier, let's see what they look like visualized.</p> In\u00a0[46]: Copied! <pre># Plot sigmoid activated toy tensor\nplt.plot(sigmoid(A));\n</pre> # Plot sigmoid activated toy tensor plt.plot(sigmoid(A)); <p>Looking good! We've gone from a straight line to a curved line.</p> <p>Now there's plenty more non-linear activation functions that exist in PyTorch that we haven't tried.</p> <p>But these two are two of the most common.</p> <p>And the point remains, what patterns could you draw using an unlimited amount of linear (straight) and non-linear (not straight) lines?</p> <p>Almost anything right?</p> <p>That's exactly what our model is doing when we combine linear and non-linear functions.</p> <p>Instead of telling our model what to do, we give it tools to figure out how to best discover patterns in the data.</p> <p>And those tools are linear and non-linear functions.</p> In\u00a0[47]: Copied! <pre># Import dependencies\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n# Set the hyperparameters for data creation\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. Create multi-class data\nX_blob, y_blob = make_blobs(n_samples=1000,\n    n_features=NUM_FEATURES, # X features\n    centers=NUM_CLASSES, # y labels \n    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n    random_state=RANDOM_SEED\n)\n\n# 2. Turn data into tensors\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\nprint(X_blob[:5], y_blob[:5])\n\n# 3. Split into train and test sets\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n    y_blob,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\n\n# 4. Plot data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);\n</pre> # Import dependencies import torch import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split  # Set the hyperparameters for data creation NUM_CLASSES = 4 NUM_FEATURES = 2 RANDOM_SEED = 42  # 1. Create multi-class data X_blob, y_blob = make_blobs(n_samples=1000,     n_features=NUM_FEATURES, # X features     centers=NUM_CLASSES, # y labels      cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)     random_state=RANDOM_SEED )  # 2. Turn data into tensors X_blob = torch.from_numpy(X_blob).type(torch.float) y_blob = torch.from_numpy(y_blob).type(torch.LongTensor) print(X_blob[:5], y_blob[:5])  # 3. Split into train and test sets X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,     y_blob,     test_size=0.2,     random_state=RANDOM_SEED )  # 4. Plot data plt.figure(figsize=(10, 7)) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu); <pre>tensor([[-8.4134,  6.9352],\n        [-5.7665, -6.4312],\n        [-6.0421, -6.7661],\n        [ 3.9508,  0.6984],\n        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])\n</pre> <p>Nice! Looks like we've got some multi-class data ready to go.</p> <p>Let's build a model to separate the coloured blobs.</p> <p>Question: Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?</p> In\u00a0[48]: Copied! <pre># Create device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Create device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[48]: <pre>'cuda'</pre> In\u00a0[49]: Copied! <pre>from torch import nn\n\n# Build model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n\"\"\"Initializes all required hyperparameters for a multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model.\n            out_features (int): Number of output features of the model\n              (how many classes there are).\n            hidden_units (int): Number of hidden units between layers, default 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n        )\n    \n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n</pre> from torch import nn  # Build model class BlobModel(nn.Module):     def __init__(self, input_features, output_features, hidden_units=8):         \"\"\"Initializes all required hyperparameters for a multi-class classification model.          Args:             input_features (int): Number of input features to the model.             out_features (int): Number of output features of the model               (how many classes there are).             hidden_units (int): Number of hidden units between layers, default 8.         \"\"\"         super().__init__()         self.linear_layer_stack = nn.Sequential(             nn.Linear(in_features=input_features, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?         )          def forward(self, x):         return self.linear_layer_stack(x)  # Create an instance of BlobModel and send it to the target device model_4 = BlobModel(input_features=NUM_FEATURES,                      output_features=NUM_CLASSES,                      hidden_units=8).to(device) model_4 Out[49]: <pre>BlobModel(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=2, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=8, bias=True)\n    (2): Linear(in_features=8, out_features=4, bias=True)\n  )\n)</pre> <p>Excellent! Our multi-class model is ready to go, let's create a loss function and optimizer for it.</p> In\u00a0[50]: Copied! <pre># Create loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n</pre> # Create loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model_4.parameters(),                              lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance In\u00a0[51]: Copied! <pre># Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\nmodel_4(X_blob_train.to(device))[:5]\n</pre> # Perform a single forward pass on the data (we'll need to put it to the target device for it to work) model_4(X_blob_train.to(device))[:5] Out[51]: <pre>tensor([[-1.2711, -0.6494, -1.4740, -0.7044],\n        [ 0.2210, -1.5439,  0.0420,  1.1531],\n        [ 2.8698,  0.9143,  3.3169,  1.4027],\n        [ 1.9576,  0.3125,  2.2244,  1.1324],\n        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)</pre> <p>What's coming out here?</p> <p>It looks like we get one value per feature of each sample.</p> <p>Let's check the shape to confirm.</p> In\u00a0[52]: Copied! <pre># How many elements in a single prediction sample?\nmodel_4(X_blob_train.to(device))[0].shape, NUM_CLASSES\n</pre> # How many elements in a single prediction sample? model_4(X_blob_train.to(device))[0].shape, NUM_CLASSES  Out[52]: <pre>(torch.Size([4]), 4)</pre> <p>Wonderful, our model is predicting one value for each class that we have.</p> <p>Do you remember what the raw outputs of our model are called?</p> <p>Hint: it rhymes with \"frog splits\" (no animals were harmed in the creation of these materials).</p> <p>If you guessed logits, you'd be correct.</p> <p>So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?</p> <p>As in, how do we go from <code>logits -&gt; prediction probabilities -&gt; prediction labels</code> just like we did with the binary classification problem?</p> <p>That's where the softmax activation function comes into play.</p> <p>The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.</p> <p>If this doesn't make sense, let's see in code.</p> In\u00a0[53]: Copied! <pre># Make prediction logits with model\ny_logits = model_4(X_blob_test.to(device))\n\n# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1) \nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n</pre> # Make prediction logits with model y_logits = model_4(X_blob_test.to(device))  # Perform softmax calculation on logits across dimension 1 to get prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  print(y_logits[:5]) print(y_pred_probs[:5]) <pre>tensor([[-1.2549, -0.8112, -1.4795, -0.5696],\n        [ 1.7168, -1.2270,  1.7367,  2.1010],\n        [ 2.2400,  0.7714,  2.6020,  1.0107],\n        [-0.7993, -0.3723, -0.9138, -0.5388],\n        [-0.4332, -1.6117, -0.6891,  0.6852]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\ntensor([[0.1872, 0.2918, 0.1495, 0.3715],\n        [0.2824, 0.0149, 0.2881, 0.4147],\n        [0.3380, 0.0778, 0.4854, 0.0989],\n        [0.2118, 0.3246, 0.1889, 0.2748],\n        [0.1945, 0.0598, 0.1506, 0.5951]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n</pre> <p>Hmm, what's happened here?</p> <p>It may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasn't been trained and is predicting using random patterns) but there's a very specific thing different about each sample.</p> <p>After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).</p> <p>Let's check.</p> In\u00a0[54]: Copied! <pre># Sum the first sample output of the softmax activation function \ntorch.sum(y_pred_probs[0])\n</pre> # Sum the first sample output of the softmax activation function  torch.sum(y_pred_probs[0]) Out[54]: <pre>tensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)</pre> <p>These prediction probablities are essentially saying how much the model thinks the target <code>X</code> sample (the input) maps to each class.</p> <p>Since there's one value for each class in <code>y_pred_probs</code>, the index of the highest value is the class the model thinks the specific data sample most belongs to.</p> <p>We can check which index has the highest value using <code>torch.argmax()</code>.</p> In\u00a0[55]: Copied! <pre># Which class does the model think is *most* likely at the index 0 sample?\nprint(y_pred_probs[0])\nprint(torch.argmax(y_pred_probs[0]))\n</pre> # Which class does the model think is *most* likely at the index 0 sample? print(y_pred_probs[0]) print(torch.argmax(y_pred_probs[0])) <pre>tensor([0.1872, 0.2918, 0.1495, 0.3715], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\ntensor(3, device='cuda:0')\n</pre> <p>You can see the output of <code>torch.argmax()</code> returns 3, so for the features (<code>X</code>) of the sample at index 0, the model is predicting that the most likely class value (<code>y</code>) is 3.</p> <p>Of course, right now this is just random guessing so it's got a 25% chance of being right (since there's four classes). But we can improve those chances by training the model.</p> <p>Note: To summarize the above, a model's raw output is referred to as logits.</p> <p>For a multi-class classification problem, to turn the logits into prediction probabilities, you use the softmax activation function (<code>torch.softmax</code>).</p> <p>The index of the value with the highest prediction probability is the class number the model thinks is most likely given the input features for that sample (although this is a prediction, it doesn't mean it will be correct).</p> In\u00a0[56]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\n\n# Set number of epochs\nepochs = 100\n\n# Put data to target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_4.train()\n\n    # 1. Forward pass\n    y_logits = model_4(X_blob_train) # model outputs raw logits \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels\n    # print(y_logits)\n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. Calculate test loss and accuracy\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42)  # Set number of epochs epochs = 100  # Put data to target device X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device) X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)  for epoch in range(epochs):     ### Training     model_4.train()      # 1. Forward pass     y_logits = model_4(X_blob_train) # model outputs raw logits      y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels     # print(y_logits)     # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_blob_train)      acc = accuracy_fn(y_true=y_blob_train,                       y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_4.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_4(X_blob_test)       test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)       # 2. Calculate test loss and accuracy       test_loss = loss_fn(test_logits, y_blob_test)       test_acc = accuracy_fn(y_true=y_blob_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%\nEpoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%\nEpoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%\nEpoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%\nEpoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%\nEpoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%\nEpoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%\nEpoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%\nEpoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%\nEpoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%\n</pre> In\u00a0[57]: Copied! <pre># Make predictions\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\n# View the first 10 predictions\ny_logits[:10]\n</pre> # Make predictions model_4.eval() with torch.inference_mode():     y_logits = model_4(X_blob_test)  # View the first 10 predictions y_logits[:10] Out[57]: <pre>tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],\n        [  5.0142, -12.0371,   3.3860,  10.6699],\n        [ -5.5885, -13.3448,  20.9894,  12.7711],\n        [  1.8400,   7.5599,  -8.6016,  -6.9942],\n        [  8.0726,   3.2906, -14.5998,  -3.6186],\n        [  5.5844, -14.9521,   5.0168,  13.2890],\n        [ -5.9739, -10.1913,  18.8655,   9.9179],\n        [  7.0755,  -0.7601,  -9.5531,   0.1736],\n        [ -5.5918, -18.5990,  25.5309,  17.5799],\n        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')</pre> <p>Alright, looks like our model's predictions are still in logit form.</p> <p>Though to evaluate them, they'll have to be in the same form as our labels (<code>y_blob_test</code>) which are in integer form.</p> <p>Let's convert our model's prediction logits to prediction probabilities (using <code>torch.softmax()</code>) then to prediction labels (by taking the <code>argmax()</code> of each sample).</p> <p>Note: It's possible to skip the <code>torch.softmax()</code> function and go straight from <code>predicted logits -&gt; predicted labels</code> by calling <code>torch.argmax()</code> directly on the logits.</p> <p>For example, <code>y_preds = torch.argmax(y_logits, dim=1)</code>, this saves a computation step (no <code>torch.softmax()</code>) but results in no prediction probabilities being available to use.</p> In\u00a0[58]: Copied! <pre># Turn predicted logits in prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1)\n\n# Turn prediction probabilities into prediction labels\ny_preds = y_pred_probs.argmax(dim=1)\n\n# Compare first 10 model preds and test labels\nprint(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\nprint(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n</pre> # Turn predicted logits in prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  # Turn prediction probabilities into prediction labels y_preds = y_pred_probs.argmax(dim=1)  # Compare first 10 model preds and test labels print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\") print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\") <pre>Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nLabels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nTest accuracy: 99.5%\n</pre> <p>Nice! Our model predictions are now in the same form as our test labels.</p> <p>Let's visualize them with <code>plot_decision_boundary()</code>, remember because our data is on the GPU, we'll have to move it to the CPU for use with matplotlib (<code>plot_decision_boundary()</code> does this automatically for us).</p> In\u00a0[59]: Copied! <pre>plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)\n</pre> plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_4, X_blob_train, y_blob_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_4, X_blob_test, y_blob_test) In\u00a0[60]: Copied! <pre>try:\n    from torchmetrics import Accuracy\nexcept:\n    !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)\n    from torchmetrics import Accuracy\n\n# Setup metric and make sure it's on the target device\ntorchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n\n# Calculate accuracy\ntorchmetrics_accuracy(y_preds, y_blob_test)\n</pre> try:     from torchmetrics import Accuracy except:     !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)     from torchmetrics import Accuracy  # Setup metric and make sure it's on the target device torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)  # Calculate accuracy torchmetrics_accuracy(y_preds, y_blob_test) Out[60]: <pre>tensor(0.9950, device='cuda:0')</pre>"},{"location":"02_pytorch_classification/#02-pytorch-neural-network-classification","title":"02. PyTorch Neural Network Classification\u00b6","text":""},{"location":"02_pytorch_classification/#what-is-a-classification-problem","title":"What is a classification problem?\u00b6","text":"<p>A classification problem involves predicting whether something is one thing or another.</p> <p>For example, you might want to:</p> Problem type What is it? Example Binary classification Target can be one of two options, e.g. yes or no Predict whether or not someone has heart disease based on their health parameters. Multi-class classification Target can be one of more than two options Decide whether a photo of is of food, a person or a dog. Multi-label classification Target can be assigned more than one option Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science &amp; philosohpy). <p>Classification, along with regression (predicting a number, covered in notebook 01) is one of the most common types of machine learning problems.</p> <p>In this notebook, we're going to work through a couple of different classification problems with PyTorch.</p> <p>In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>"},{"location":"02_pytorch_classification/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>In this notebook we're going to reiterate over the PyTorch workflow we coverd in 01. PyTorch Workflow.</p> <p>Except instead of trying to predict a straight line (predicting a number, also called a regression problem), we'll be working on a classification problem.</p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Architecture of a classification neural network Neural networks can come in almost any shape or size, but they typically follow a similar floor plan. 1. Getting binary classification data ready Data can be almost anything but to get started we're going to create a simple binary classification dataset. 2. Building a PyTorch classification model Here we'll create a model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop specific to classification. 3. Fitting the model to data (training) We've got data and a model, now let's let the model (try to) find patterns in the (training) data. 4. Making predictions and evaluating a model (inference) Our model's found patterns in the data, let's compare its findings to the actual (testing) data. 5. Improving a model (from a model perspective) We've trained an evaluated a model but it's not working, let's try a few things to improve it. 6. Non-linearity So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines? 7. Replicating non-linear functions We used non-linear functions to help model non-linear data, but what do these look like? 8. Putting it all together with multi-class classification Let's put everything we've done so far for binary classification together with a multi-class classification problem."},{"location":"02_pytorch_classification/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>And if you run into trouble, you can ask a question on the Discussions page there too.</p> <p>There's also the PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"02_pytorch_classification/#0-architecture-of-a-classification-neural-network","title":"0. Architecture of a classification neural network\u00b6","text":"<p>Before we get into writing code, let's look at the general architecture of a classification neural network.</p> Hyperparameter Binary Classification Multiclass classification Input layer shape (<code>in_features</code>) Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) Same as binary classification Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Same as binary classification Neurons per hidden layer Problem specific, generally 10 to 512 Same as binary classification Output layer shape (<code>out_features</code>) 1 (one class or the other) 1 per class (e.g. 3 for food, person or dog photo) Hidden layer activation Usually ReLU (rectified linear unit) but can be many others Same as binary classification Output activation Sigmoid (<code>torch.sigmoid</code> in PyTorch) Softmax (<code>torch.softmax</code> in PyTorch) Loss function Binary crossentropy (<code>torch.nn.BCELoss</code> in PyTorch) Cross entropy (<code>torch.nn.CrossEntropyLoss</code> in PyTorch) Optimizer SGD (stochastic gradient descent), Adam (see <code>torch.optim</code> for more options) Same as binary classification <p>Of course, this ingredient list of classification neural network components will vary depending on the problem you're working on.</p> <p>But it's more than enough to get started.</p> <p>We're going to gets hands-on with this setup throughout this notebook.</p>"},{"location":"02_pytorch_classification/#1-make-classification-data-and-get-it-ready","title":"1. Make classification data and get it ready\u00b6","text":"<p>Let's begin by making some data.</p> <p>We'll use the <code>make_circles()</code> method from Scikit-Learn to generate two circles with different coloured dots.</p>"},{"location":"02_pytorch_classification/#11-input-and-output-shapes","title":"1.1 Input and output shapes\u00b6","text":"<p>One of the most common errors in deep learning is shape errors.</p> <p>Mismatching the shapes of tensors and tensor operations with result in errors in your models.</p> <p>We're going to see plenty of these throughout the course.</p> <p>And there's no surefire way to making sure they won't happen, they will.</p> <p>What you can do instead is continaully familiarize yourself with the shape of the data you're working with.</p> <p>I like referring to it as input and output shapes.</p> <p>Ask yourself:</p> <p>\"What shapes are my inputs and what shapes are my outputs?\"</p> <p>Let's find out.</p>"},{"location":"02_pytorch_classification/#12-turn-data-into-tensors-and-create-train-and-test-splits","title":"1.2 Turn data into tensors and create train and test splits\u00b6","text":"<p>We've investigated the input and output shapes of our data, now let's prepare it for being used with PyTorch and for modelling.</p> <p>Specifically, we'll need to:</p> <ol> <li>Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors).</li> <li>Split our data into training and test sets (we'll train a model on the training set to learn the patterns between <code>X</code> and <code>y</code> and then evaluate those learned patterns on the test dataset).</li> </ol>"},{"location":"02_pytorch_classification/#2-building-a-model","title":"2. Building a model\u00b6","text":"<p>We've got some data ready, now it's time to build a model.</p> <p>We'll break it down into a few parts.</p> <ol> <li>Setting up device agnostic code (so our model can run on CPU or GPU if it's available).</li> <li>Constructing a model by subclassing <code>nn.Module</code>.</li> <li>Defining a loss function and optimizer.</li> <li>Creating a training loop (this'll be in the next section).</li> </ol> <p>The good news is we've been through all of the above steps before in notebook 01.</p> <p>Except now we'll be adjusting them so they work with a classification dataset.</p> <p>Let's start by importing PyTorch and <code>torch.nn</code> as well as setting up device agnostic code.</p>"},{"location":"02_pytorch_classification/#21-setup-loss-function-and-optimizer","title":"2.1 Setup loss function and optimizer\u00b6","text":"<p>We've setup a loss (also called a criterion or cost function) and optimizer before in notebook 01.</p> <p>But different problem types require different loss functions.</p> <p>For example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.</p> <p>And for a binary classification problem (like ours), you'll often use binary cross entropy as the loss function.</p> <p>However, the same optimizer function can often be used across different problem spaces.</p> <p>For example, the stochastic gradient descent optimizer (SGD, <code>torch.optim.SGD()</code>) can be used for a range of problems, so can too the Adam optimizer (<code>torch.optim.Adam()</code>).</p> Loss function/Optimizer Problem type PyTorch Code Stochastic Gradient Descent (SGD) optimizer Classification, regression, many others. <code>torch.optim.SGD()</code> Adam Optimizer Classification, regression, many others. <code>torch.optim.Adam()</code> Binary cross entropy loss Binary classification <code>torch.nn.BCELossWithLogits</code> or <code>torch.nn.BCELoss</code> Cross entropy loss Mutli-class classification <code>torch.nn.CrossEntropyLoss</code> Mean absolute error (MAE) or L1 Loss Regression <code>torch.nn.L1Loss</code> Mean squared error (MSE) or L2 Loss Regression <code>torch.nn.MSELoss</code> <p>Table of various loss functions and optimizers, there are more but these some common ones you'll see.</p> <p>Since we're working with a binary classification problem, let's use a binary cross entropy loss function.</p> <p>Note: Recall a loss function is what measures how wrong your model predictions are, the higher the loss, the worse your model.</p> <p>Also, PyTorch documentation often refers to loss functions as \"loss criterion\" or \"criterion\", these are all different ways of describing the same thing.</p> <p>PyTorch has two binary cross entropy implementations:</p> <ol> <li><code>torch.nn.BCELoss()</code> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).</li> <li><code>torch.nn.BCEWithLogitsLoss()</code> - This is the same as above except it has a sigmoid layer (<code>nn.Sigmoid</code>) built-in (we'll see what this means soon).</li> </ol> <p>Which one should you use?</p> <p>The documentation for <code>torch.nn.BCEWithLogitsLoss()</code> states that it's more numerically stable than using <code>torch.nn.BCELoss()</code> after a <code>nn.Sigmoid</code> layer.</p> <p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code>nn.Sigmoid</code> and <code>torch.nn.BCELoss()</code> but that is beyond the scope of this notebook.</p> <p>Knowing this, let's create a loss function and an optimizer.</p> <p>For the optimizer we'll use <code>torch.optim.SGD()</code> to optimize the model parameters with learning rate 0.1.</p> <p>Note: There's a discussion on the PyTorch forums about the use of <code>nn.BCELoss</code> vs. <code>nn.BCEWithLogitsLoss</code>. It can be confusing at first but as with many things, it becomes easier with practice.</p>"},{"location":"02_pytorch_classification/#3-train-model","title":"3. Train model\u00b6","text":"<p>Okay, now we've got a loss function and optimizer ready to go, let's train a model.</p> <p>Do you remember the steps in a PyTorch training loop?</p> <p>If not, here's a reminder.</p> <p>Steps in training:</p> PyTorch training loop steps <ol> <li>Forward pass - The model goes through all of the training data once, performing its             <code>forward()</code> function             calculations (<code>model(x_train)</code>).         </li> <li>Calculate the loss - The model's outputs (predictions) are compared to the ground truth and evaluated             to see how             wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li> <li>Zero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they             can be             recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li> <li>Perform backpropagation on the loss - Computes the gradient of the loss with respect for every model             parameter to             be updated (each parameter             with <code>requires_grad=True</code>). This is known as backpropagation, hence \"backwards\"             (<code>loss.backward()</code>).</li> <li>Step the optimizer (gradient descent) - Update the parameters with <code>requires_grad=True</code>             with respect to the loss             gradients in order to improve them (<code>optimizer.step()</code>).</li> </ol>"},{"location":"02_pytorch_classification/#31-going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels","title":"3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)\u00b6","text":"<p>Before we the training loop steps, let's see what comes out of our model during the forward pass (the forward pass is defined by the <code>forward()</code> method).</p> <p>To do so, let's pass the model some data.</p>"},{"location":"02_pytorch_classification/#32-building-a-training-and-testing-loop","title":"3.2 Building a training and testing loop\u00b6","text":"<p>Alright, we've discussed how to take our raw model outputs and convert them to prediction labels, now let's build a training loop.</p> <p>Let's start by training for 100 epochs and outputing the model's progress every 10 epochs.</p>"},{"location":"02_pytorch_classification/#4-make-predictions-and-evaluate-the-model","title":"4. Make predictions and evaluate the model\u00b6","text":"<p>From the metrics it looks like our model is random guessing.</p> <p>How could we investigate this further?</p> <p>I've got an idea.</p> <p>The data explorer's motto!</p> <p>\"Visualize, visualize, visualize!\"</p> <p>Let's make a plot of our model's predictions, the data it's trying to predict on and the decision boundary it's creating for whether something is class 0 or class 1.</p> <p>To do so, we'll write some code to download and import the <code>helper_functions.py</code> script from the Learn PyTorch for Deep Learning repo.</p> <p>It contains a helpful function called <code>plot_decision_boundary()</code> which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.</p> <p>We'll also import <code>plot_predictions()</code> which we wrote in notebook 01 to use later.</p>"},{"location":"02_pytorch_classification/#5-improving-a-model-from-a-model-perspective","title":"5. Improving a model (from a model perspective)\u00b6","text":"<p>Let's try to fix our model's underfitting problem.</p> <p>Focusing specifically on the model (not the data), there are a few ways we could do this.</p> Model improvement technique* What does it do? Add more layers Each layer potentially increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network deeper. Add more hidden units Similar to the above, more hidden units per layer means a potential increase in learning capabilities of the model, more hidden units is often referred to as making your neural network wider. Fitting for longer (more epochs) Your model might learn more if it had more opportunities to look at the data. Changing the activation functions Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint). Change the learning rate Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough. Change the loss function Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem. Use transfer learning Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in notebook 06. <p>Note: because you can adjust all of these by hand, they're referred to as hyperparameters*.</p> <p>And this is also where machine learning's half art half science comes in, there's no real way to know here what the best combination of values is for your project, best to follow the data scientist's motto of \"experiment, experiment, experiment\".</p> <p>Let's see what happens if we add an extra layer to our model, fit for longer (<code>epochs=1000</code> instead of <code>epochs=100</code>) and increase the number of hidden units from <code>5</code> to <code>10</code>.</p> <p>We'll follow the same steps we did above but with a few changed hyperparameters.</p>"},{"location":"02_pytorch_classification/#51-preparing-data-to-see-if-our-model-can-model-a-straight-line","title":"5.1 Preparing data to see if our model can model a straight line\u00b6","text":"<p>Let's create some linear data to see if our model's able to model it and we're not just using a model that can't learn anything.</p>"},{"location":"02_pytorch_classification/#52-adjusting-model_1-to-fit-a-straight-line","title":"5.2 Adjusting <code>model_1</code> to fit a straight line\u00b6","text":"<p>Now we've got some data, let's recreate <code>model_1</code> but with a loss function suited to our regression data.</p>"},{"location":"02_pytorch_classification/#6-the-missing-piece-non-linearity","title":"6. The missing piece: non-linearity\u00b6","text":"<p>We've seen our model can draw straight (linear) lines, thanks to its linear layers.</p> <p>But how about we give it the capacity to draw non-straight (non-linear) lines?</p> <p>How?</p> <p>Let's find out.</p>"},{"location":"02_pytorch_classification/#61-recreating-non-linear-data-red-and-blue-circles","title":"6.1 Recreating non-linear data (red and blue circles)\u00b6","text":"<p>First, let's recreate the data to start off fresh. We'll use the same setup as before.</p>"},{"location":"02_pytorch_classification/#62-building-a-model-with-non-linearity","title":"6.2 Building a model with non-linearity\u00b6","text":"<p>Now here comes the fun part.</p> <p>What kind of pattern do you think you could draw with unlimited straight (linear) and non-straight (non-linear) lines?</p> <p>I bet you could get pretty creative.</p> <p>So far our neural networks have only been using linear (straight) line functions.</p> <p>But the data we've been working with is non-linear (circles).</p> <p>What do you think will happen when we introduce the capability for our model to use non-linear actviation functions?</p> <p>Well let's see.</p> <p>PyTorch has a bunch of ready-made non-linear activation functions that do similiar but different things.</p> <p>One of the most common and best performing is [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) (rectified linear-unit, <code>torch.nn.ReLU()</code>).</p> <p>Rather than talk about it, let's put it in our neural network between the hidden layers in the forward pass and see what happens.</p>"},{"location":"02_pytorch_classification/#63-training-a-model-with-non-linearity","title":"6.3 Training a model with non-linearity\u00b6","text":"<p>You know the drill, model, loss function, optimizer ready to go, let's create a training and testing loop.</p>"},{"location":"02_pytorch_classification/#64-evaluating-a-model-trained-with-non-linear-activation-functions","title":"6.4 Evaluating a model trained with non-linear activation functions\u00b6","text":"<p>Remember how our circle data is non-linear? Well, let's see how our models predictions look now the model's been trained with non-linear activation functions.</p>"},{"location":"02_pytorch_classification/#7-replicating-non-linear-activation-functions","title":"7. Replicating non-linear activation functions\u00b6","text":"<p>We saw before how adding non-linear activation functions to our model can help it to model non-linear data.</p> <p>Note: Much of the data you'll encounter in the wild is non-linear (or a combination of linear and non-linear). Right now we've been working with dots on a 2D plot. But imagine if you had images of plants you'd like to classify, there's a lot of different plant shapes. Or text from Wikipedia you'd like to summarize, there's lots of different ways words can be put together (linear and non-linear patterns).</p> <p>But what does a non-linear activation look like?</p> <p>How about we replicate some and what they do?</p> <p>Let's start by creating a small amount of data.</p>"},{"location":"02_pytorch_classification/#8-putting-things-together-by-building-a-multi-class-pytorch-model","title":"8. Putting things together by building a multi-class PyTorch model\u00b6","text":"<p>We've covered a fair bit.</p> <p>But now let's put it all together using a multi-class classification problem.</p> <p>Recall a binary classification problem deals with classifying something as one of two options (e.g. a photo as a cat photo or a dog photo) where as a multi-class classification problem deals with classifying something from a list of more than two options (e.g. classifying a photo as a cat a dog or a chicken).</p> <p> Example of binary vs. multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular ImageNet-1k dataset is used as a computer vision benchmark and has 1000 classes.</p>"},{"location":"02_pytorch_classification/#81-creating-mutli-class-classification-data","title":"8.1 Creating mutli-class classification data\u00b6","text":"<p>To begin a multi-class classification problem, let's create some multi-class data.</p> <p>To do so, we can leverage Scikit-Learn's <code>make_blobs()</code> method.</p> <p>This method will create however many classes (using the <code>centers</code> parameter) we want.</p> <p>Specifically, let's do the following:</p> <ol> <li>Create some multi-class data with <code>make_blobs()</code>.</li> <li>Turn the data into tensors (the default of <code>make_blobs()</code> is to use NumPy arrays).</li> <li>Split the data into training and test sets using <code>train_test_split()</code>.</li> <li>Visualize the data.</li> </ol>"},{"location":"02_pytorch_classification/#82-building-a-multi-class-classification-model-in-pytorch","title":"8.2 Building a multi-class classification model in PyTorch\u00b6","text":"<p>We've created a few models in PyTorch so far.</p> <p>You might also be starting to get an idea of how flexible neural networks are.</p> <p>How about we build one similar to <code>model_3</code> but this still capable of handling multi-class data?</p> <p>To do so, let's create a subclass of <code>nn.Module</code> that takes in three hyperparameters:</p> <ul> <li><code>input_features</code> - the number of <code>X</code> features coming into the model.</li> <li><code>output_features</code> - the ideal numbers of output features we'd like (this will be equivalent to <code>NUM_CLASSES</code> or the number of classes in your multi-class classification problem).</li> <li><code>hidden_units</code> - the number of hidden neurons we'd like each hidden layer to use.</li> </ul> <p>Since we're putting things together, let's setup some device agnostic code (we don't have to do this again in the same notebook, it's only a reminder).</p> <p>Then we'll create the model class using the hyperparameters above.</p>"},{"location":"02_pytorch_classification/#83-creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model","title":"8.3 Creating a loss function and optimizer for a multi-class PyTorch model\u00b6","text":"<p>Since we're working on a multi-class classification problem, we'll use the <code>nn.CrossEntropyLoss()</code> method as our loss function.</p> <p>And we'll stick with using SGD with a learning rate of 0.1 for optimizing our <code>model_4</code> parameters.</p>"},{"location":"02_pytorch_classification/#84-getting-prediction-probabilities-for-a-multi-class-pytorch-model","title":"8.4 Getting prediction probabilities for a multi-class PyTorch model\u00b6","text":"<p>Alright, we've got a loss function and optimizer ready, and we're ready to train our model but before we do let's do a single forward pass with our model to see if it works.</p>"},{"location":"02_pytorch_classification/#85-creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model","title":"8.5 Creating a training and testing loop for a multi-class PyTorch model\u00b6","text":"<p>Alright, now we've got all of the preparation steps out of the way, let's write a training and testing loop to improve and evaluate our model.</p> <p>We've done many of these steps before so much of this will be practice.</p> <p>The only difference is that we'll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).</p> <p>Let's train the model for <code>epochs=100</code> and evaluate it every 10 epochs.</p>"},{"location":"02_pytorch_classification/#86-making-and-evaluating-predictions-with-a-pytorch-multi-class-model","title":"8.6 Making and evaluating predictions with a PyTorch multi-class model\u00b6","text":"<p>It looks like our trained model is performaning pretty well.</p> <p>But to make sure of this, let's make some predictions and visualize them.</p>"},{"location":"02_pytorch_classification/#9-more-classification-evaluation-metrics","title":"9. More classification evaluation metrics\u00b6","text":"<p>So far we've only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).</p> <p>These are some of the most common methods you'll come across and are a good starting point.</p> <p>However, you may want to evaluate your classification model using more metrics such as the following:</p> Metric name/Evaluation method Defintion Code Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. <code>torchmetrics.Accuracy()</code> or <code>sklearn.metrics.accuracy_score()</code> Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). <code>torchmetrics.Precision()</code> or <code>sklearn.metrics.precision_score()</code> Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. <code>torchmetrics.Recall()</code> or <code>sklearn.metrics.recall_score()</code> F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. <code>torchmetrics.F1Score()</code> or <code>sklearn.metrics.f1_score()</code> Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). <code>torchmetrics.ConfusionMatrix</code> or <code>sklearn.metrics.plot_confusion_matrix()</code> Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. <code>sklearn.metrics.classification_report()</code> <p>Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you're looking for a PyTorch-like version, check out TorchMetrics, especially the TorchMetrics classification section.</p> <p>Let's try the <code>torchmetrics.Accuracy</code> metric out.</p>"},{"location":"02_pytorch_classification/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agonistic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 02</li> <li>Example solutions notebook for 02 (try the exercises before looking at this)</li> </ul> <ol> <li>Make a binary classification dataset with Scikit-Learn's <code>make_moons()</code> function.<ul> <li>For consistency, the dataset should have 1000 samples and a <code>random_state=42</code>.</li> <li>Turn the data into PyTorch tensors. Split the data into training and test sets using <code>train_test_split</code> with 80% training and 20% testing.</li> </ul> </li> <li>Build a model by subclassing <code>nn.Module</code> that incorporates non-linear activation functions and is capable of fitting the data you created in 1.<ul> <li>Feel free to use any combination of PyTorch layers (linear and non-linear) you want.</li> </ul> </li> <li>Setup a binary classification compatible loss function and optimizer to use when training the model.</li> <li>Create a training and testing loop to fit the model you created in 2 to the data you created in 1.<ul> <li>To measure model accuray, you can create your own accuracy function or use the accuracy function in TorchMetrics.</li> <li>Train the model for long enough for it to reach over 96% accuracy.</li> <li>The training loop should output progress every 10 epochs of the model's training and test set loss and accuracy.</li> </ul> </li> <li>Make predictions with your trained model and plot them using the <code>plot_decision_boundary()</code> function created in this notebook.</li> <li>Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.<ul> <li>Feel free to reference the ML cheatsheet website for the formula.</li> </ul> </li> <li>Create a multi-class dataset using the spirals data creation function from CS231n (see below for the code).<ul> <li>Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).</li> <li>Build a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).</li> <li>Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like).</li> <li>Plot the decision boundaries on the spirals dataset from your model predictions, the <code>plot_decision_boundary()</code> function should work for this dataset too.</li> </ul> </li> </ol> <pre># Code for creating a spiral dataset from CS231n\nimport numpy as np\nN = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# lets visualize the data\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()\n</pre>"},{"location":"02_pytorch_classification/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Write down 3 problems where you think machine classification could be useful (these can be anything, get creative as you like, for example, classifying credit card transactions as fraud or not fraud based on the purchase amount and purchase location features).</li> <li>Research the concept of \"momentum\" in gradient-based optimizers (like SGD or Adam), what does it mean?</li> <li>Spend 10-minutes reading the Wikipedia page for different activation functions, how many of these can you line up with PyTorch's activation functions?</li> <li>Research when accuracy might be a poor metric to use (hint: read \"Beyond Accuracy\" by by Will Koehrsen for ideas).</li> <li>Watch: For an idea of what's happening within our neural networks and what they're doing to learn, watch MIT's Introduction to Deep Learning video.</li> </ul>"},{"location":"03_pytorch_computer_vision/","title":"03. PyTorch Computer Vision","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre># Import PyTorch\nimport torch\nfrom torch import nn\n\n# Import torchvision \nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Import matplotlib for visualization\nimport matplotlib.pyplot as plt\n\n# Check versions\n# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\nprint(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")\n</pre> # Import PyTorch import torch from torch import nn  # Import torchvision  import torchvision from torchvision import datasets from torchvision.transforms import ToTensor  # Import matplotlib for visualization import matplotlib.pyplot as plt  # Check versions # Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11 print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\") <pre>PyTorch version: 1.12.1+cu113\ntorchvision version: 0.13.1+cu113\n</pre> In\u00a0[2]: Copied! <pre># Setup training data\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=ToTensor()\n)\n</pre> # Setup training data train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Setup testing data test_data = datasets.FashionMNIST(     root=\"data\",     train=False, # get test data     download=True,     transform=ToTensor() ) <p>Let's check out the first sample of the training data.</p> In\u00a0[3]: Copied! <pre># See first training sample\nimage, label = train_data[0]\nimage, label\n</pre> # See first training sample image, label = train_data[0] image, label Out[3]: <pre>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n           0.0157, 0.0000, 0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0471, 0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n           0.3020, 0.5098, 0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n           0.5529, 0.3451, 0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n           0.4824, 0.7686, 0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n           0.8745, 0.9608, 0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n           0.8627, 0.9529, 0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n           0.8863, 0.7725, 0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n           0.9608, 0.4667, 0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n           0.8510, 0.8196, 0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n           0.8549, 1.0000, 0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n           0.8784, 0.9569, 0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n           0.9137, 0.9333, 0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n           0.8627, 0.9098, 0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n           0.8706, 0.8941, 0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n           0.8745, 0.8784, 0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n           0.8627, 0.8667, 0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n           0.7098, 0.8039, 0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n           0.6549, 0.6941, 0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n           0.7529, 0.8471, 0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n           0.3882, 0.2275, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 9)</pre> In\u00a0[4]: Copied! <pre># What's the shape of the image?\nimage.shape\n</pre> # What's the shape of the image? image.shape Out[4]: <pre>torch.Size([1, 28, 28])</pre> <p>The shape of the image tensor is <code>[1, 28, 28]</code> or more specifically:</p> <pre><code>[color_channels=1, height=28, width=28]\n</code></pre> <p>Having <code>color_channels=1</code> means the image is grayscale.</p> <p> Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.</p> <p>If <code>color_channels=3</code>, the image comes in pixel values for red, green and blue (this is also known a the RGB color model).</p> <p>The order of our current tensor is often referred to as <code>CHW</code> (Color Channels, Height, Width).</p> <p>There's debate on whether images should be represented as <code>CHW</code> (color channels first) or <code>HWC</code> (color channels last).</p> <p>Note: You'll also see <code>NCHW</code> and <code>NHWC</code> formats where <code>N</code> stands for number of images. For example if you have a <code>batch_size=32</code>, your tensor shape may be <code>[32, 1, 28, 28]</code>. We'll cover batch sizes later.</p> <p>PyTorch generally accepts <code>NCHW</code> (channels first) as the default for many operators.</p> <p>However, PyTorch also explains that <code>NHWC</code> (channels last) performs better and is considered best practice.</p> <p>For now, since our dataset and models are relatively small, this won't make too much of a difference.</p> <p>But keep it in mind for when you're working on larger image datasets and using convolutional neural networks (we'll see these later).</p> <p>Let's check out more shapes of our data.</p> In\u00a0[5]: Copied! <pre># How many samples are there? \nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n</pre> # How many samples are there?  len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets) Out[5]: <pre>(60000, 60000, 10000, 10000)</pre> <p>So we've got 60,000 training samples and 10,000 testing samples.</p> <p>What classes are there?</p> <p>We can find these via the <code>.classes</code> attribute.</p> In\u00a0[6]: Copied! <pre># See classes\nclass_names = train_data.classes\nclass_names\n</pre> # See classes class_names = train_data.classes class_names Out[6]: <pre>['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']</pre> <p>Sweet! It looks like we're dealing with 10 different kinds of clothes.</p> <p>Because we're working with 10 different classes, it means our problem is multi-class classification.</p> <p>Let's get visual.</p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\nplt.title(label);\n</pre> import matplotlib.pyplot as plt image, label = train_data[0] print(f\"Image shape: {image.shape}\") plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width) plt.title(label); <pre>Image shape: torch.Size([1, 28, 28])\n</pre> <p>We can turn the image into grayscale using the <code>cmap</code> parameter of <code>plt.imshow()</code>.</p> In\u00a0[8]: Copied! <pre>plt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(class_names[label]);\n</pre> plt.imshow(image.squeeze(), cmap=\"gray\") plt.title(class_names[label]); <p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p> <p>Let's view a few more.</p> In\u00a0[9]: Copied! <pre># Plot more images\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows * cols + 1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n</pre> # Plot more images torch.manual_seed(42) fig = plt.figure(figsize=(9, 9)) rows, cols = 4, 4 for i in range(1, rows * cols + 1):     random_idx = torch.randint(0, len(train_data), size=[1]).item()     img, label = train_data[random_idx]     fig.add_subplot(rows, cols, i)     plt.imshow(img.squeeze(), cmap=\"gray\")     plt.title(class_names[label])     plt.axis(False); <p>Hmmm, this dataset doesn't look too aesthetic.</p> <p>But the principles we're going to learn on how to build a model for it will be similar across a wide range of computer vision problems.</p> <p>In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.</p> <p>Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?</p> <p>You probably could.</p> <p>But I think coding a model in PyTorch would be faster.</p> <p>Question: Do you think the above data can be model with only straight (linear) lines? Or do you think you'd also need non-straight (non-linear) lines?</p> In\u00a0[10]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 32\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True # shuffle data every epoch?\n)\n\ntest_dataloader = DataLoader(test_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False # don't necessarily have to shuffle the testing data\n)\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n</pre> from torch.utils.data import DataLoader  # Setup the batch size hyperparameter BATCH_SIZE = 32  # Turn datasets into iterables (batches) train_dataloader = DataLoader(train_data, # dataset to turn into iterable     batch_size=BATCH_SIZE, # how many samples per batch?      shuffle=True # shuffle data every epoch? )  test_dataloader = DataLoader(test_data,     batch_size=BATCH_SIZE,     shuffle=False # don't necessarily have to shuffle the testing data )  # Let's check out what we've created print(f\"Dataloaders: {train_dataloader, test_dataloader}\")  print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") <pre>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7ff03be2c700&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7ff03be2c2e0&gt;)\nLength of train dataloader: 1875 batches of 32\nLength of test dataloader: 313 batches of 32\n</pre> In\u00a0[11]: Copied! <pre># Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n</pre> # Check out what's inside the training dataloader train_features_batch, train_labels_batch = next(iter(train_dataloader)) train_features_batch.shape, train_labels_batch.shape Out[11]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>And we can see that the data remains unchanged by checking a single sample.</p> In\u00a0[12]: Copied! <pre># Show a sample\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n</pre> # Show a sample torch.manual_seed(42) random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() img, label = train_features_batch[random_idx], train_labels_batch[random_idx] plt.imshow(img.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(\"Off\"); print(f\"Image size: {img.shape}\") print(f\"Label: {label}, label size: {label.shape}\") <pre>Image size: torch.Size([1, 28, 28])\nLabel: 6, label size: torch.Size([])\n</pre> In\u00a0[13]: Copied! <pre># Create a flatten layer\nflatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n\n# Get a single sample\nx = train_features_batch[0]\n\n# Flatten the sample\noutput = flatten_model(x) # perform forward pass\n\n# Print out what happened\nprint(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n\n# Try uncommenting below and see what happens\n#print(x)\n#print(output)\n</pre> # Create a flatten layer flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)  # Get a single sample x = train_features_batch[0]  # Flatten the sample output = flatten_model(x) # perform forward pass  # Print out what happened print(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\") print(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")  # Try uncommenting below and see what happens #print(x) #print(output) <pre>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]\nShape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]\n</pre> <p>The <code>nn.Flatten()</code> layer took our shape from <code>[color_channels, height, width]</code> to <code>[color_channels, height*width]</code>.</p> <p>Why do this?</p> <p>Because we've now turned our pixel data from height and width dimensions into one long feature vector.</p> <p>And <code>nn.Linear()</code> layers like their inputs to be in the form of feature vectors.</p> <p>Let's create our first model using <code>nn.Flatten()</code> as the first layer.</p> In\u00a0[14]: Copied! <pre>from torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # neural networks like their inputs in vector form\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n</pre> from torch import nn class FashionMNISTModelV0(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # neural networks like their inputs in vector form             nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)             nn.Linear(in_features=hidden_units, out_features=output_shape)         )          def forward(self, x):         return self.layer_stack(x) <p>Wonderful!</p> <p>We've got a baseline model class we can use, now let's instantiate a model.</p> <p>We'll need to set the following parameters:</p> <ul> <li><code>input_shape=784</code> - this is how many features you've got going in the model, in our case, it's one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).</li> <li><code>hidden_units=10</code> - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we'll start with <code>10</code>.</li> <li><code>output_shape=len(class_names)</code> - since we're working with a multi-class classification problem, we need an output neuron per class in our dataset.</li> </ul> <p>Let's create an instance of our model and send to the CPU for now (we'll run a small test for running <code>model_0</code> on CPU vs. a similar model on GPU soon).</p> In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Need to setup model with input parameters\nmodel_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n    hidden_units=10, # how many units in the hiden layer\n    output_shape=len(class_names) # one for every class\n)\nmodel_0.to(\"cpu\") # keep model on CPU to begin with\n</pre> torch.manual_seed(42)  # Need to setup model with input parameters model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)     hidden_units=10, # how many units in the hiden layer     output_shape=len(class_names) # one for every class ) model_0.to(\"cpu\") # keep model on CPU to begin with  Out[15]: <pre>FashionMNISTModelV0(\n  (layer_stack): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n  )\n)</pre> In\u00a0[16]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # Note: you need the \"raw\" GitHub URL for this to work\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   # Note: you need the \"raw\" GitHub URL for this to work   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content) <pre>helper_functions.py already exists, skipping download\n</pre> In\u00a0[17]: Copied! <pre># Import accuracy metric\nfrom helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n</pre> # Import accuracy metric from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) In\u00a0[18]: Copied! <pre>from timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n\"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</pre> from timeit import default_timer as timer  def print_train_time(start: float, end: float, device: torch.device = None):     \"\"\"Prints difference between start and end time.      Args:         start (float): Start time of computation (preferred in timeit format).          end (float): End time of computation.         device ([type], optional): Device that compute is running on. Defaults to None.      Returns:         float: time between start and end in seconds (higher is longer).     \"\"\"     total_time = end - start     print(f\"Train time on {device}: {total_time:.3f} seconds\")     return total_time In\u00a0[19]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# Set the seed and start the timer\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Set the number of epochs (we'll keep this small for faster training times)\nepochs = 3\n\n# Create training and testing loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate loss (per batch)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulatively add up the loss per epoch \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Print out how many samples have been seen\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n\n    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n    train_loss /= len(train_dataloader)\n    \n    ### Testing\n    # Setup variables for accumulatively adding up loss and accuracy \n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X)\n           \n            # 2. Calculate loss (accumatively)\n            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n\n            # 3. Calculate accuracy (preds need to be same as y_true)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # Calculations on test metrics need to happen inside torch.inference_mode()\n        # Divide total test loss by length of test dataloader (per batch)\n        test_loss /= len(test_dataloader)\n\n        # Divide total accuracy by length of test dataloader (per batch)\n        test_acc /= len(test_dataloader)\n\n    ## Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n\n# Calculate training time      \ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # Set the seed and start the timer torch.manual_seed(42) train_time_start_on_cpu = timer()  # Set the number of epochs (we'll keep this small for faster training times) epochs = 3  # Create training and testing loop for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n-------\")     ### Training     train_loss = 0     # Add a loop to loop through training batches     for batch, (X, y) in enumerate(train_dataloader):         model_0.train()          # 1. Forward pass         y_pred = model_0(X)          # 2. Calculate loss (per batch)         loss = loss_fn(y_pred, y)         train_loss += loss # accumulatively add up the loss per epoch           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Print out how many samples have been seen         if batch % 400 == 0:             print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")      # Divide total train loss by length of train dataloader (average loss per batch per epoch)     train_loss /= len(train_dataloader)          ### Testing     # Setup variables for accumulatively adding up loss and accuracy      test_loss, test_acc = 0, 0      model_0.eval()     with torch.inference_mode():         for X, y in test_dataloader:             # 1. Forward pass             test_pred = model_0(X)                         # 2. Calculate loss (accumatively)             test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch              # 3. Calculate accuracy (preds need to be same as y_true)             test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))                  # Calculations on test metrics need to happen inside torch.inference_mode()         # Divide total test loss by length of test dataloader (per batch)         test_loss /= len(test_dataloader)          # Divide total accuracy by length of test dataloader (per batch)         test_acc /= len(test_dataloader)      ## Print out what's happening     print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")  # Calculate training time       train_time_end_on_cpu = timer() total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,                                             end=train_time_end_on_cpu,                                            device=str(next(model_0.parameters()).device)) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n\nEpoch: 1\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n\nEpoch: 2\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n\nTrain time on cpu: 15.549 seconds\n</pre> <p>Nice! Looks like our baseline model did fairly well.</p> <p>It didn't take too long to train either, even just on the CPU, I wonder if it'll speed up on the GPU?</p> <p>Let's write some code to evaluate our model.</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n\"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Make predictions with the model\n            y_pred = model(X)\n            \n            # Accumulate the loss and accuracy values per batch\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # Scale loss and acc to find the average loss/acc per batch\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 0 results on test dataset\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n</pre> torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn):     \"\"\"Returns a dictionary containing the results of model predicting on data_loader.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Make predictions with the model             y_pred = model(X)                          # Accumulate the loss and accuracy values per batch             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)                  # Scale loss and acc to find the average loss/acc per batch         loss /= len(data_loader)         acc /= len(data_loader)              return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 0 results on test dataset model_0_results = eval_model(model=model_0, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn ) model_0_results Out[20]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}</pre> <p>Looking good!</p> <p>We can use this dictionary to compare the baseline model results to other models later on.</p> In\u00a0[21]: Copied! <pre># Setup device agnostic code\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code import torch device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[21]: <pre>'cuda'</pre> <p>Beautiful!</p> <p>Let's build another model.</p> In\u00a0[22]: Copied! <pre># Create a model with non-linear and linear layers\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n</pre> # Create a model with non-linear and linear layers class FashionMNISTModelV1(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # flatten inputs into single vector             nn.Linear(in_features=input_shape, out_features=hidden_units),             nn.ReLU(),             nn.Linear(in_features=hidden_units, out_features=output_shape),             nn.ReLU()         )          def forward(self, x: torch.Tensor):         return self.layer_stack(x) <p>That looks good.</p> <p>Now let's instantiate it with the same settings we used before.</p> <p>We'll need <code>input_shape=784</code> (equal to the number of features of our image data), <code>hidden_units=10</code> (starting small and the same as our baseline model) and <code>output_shape=len(class_names)</code> (one output unit per class).</p> <p>Note: Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again.</p> In\u00a0[23]: Copied! <pre>torch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n    hidden_units=10,\n    output_shape=len(class_names) # number of output classes desired\n).to(device) # send model to GPU if it's available\nnext(model_1.parameters()).device # check model device\n</pre> torch.manual_seed(42) model_1 = FashionMNISTModelV1(input_shape=784, # number of input features     hidden_units=10,     output_shape=len(class_names) # number of output classes desired ).to(device) # send model to GPU if it's available next(model_1.parameters()).device # check model device Out[23]: <pre>device(type='cuda', index=0)</pre> In\u00a0[24]: Copied! <pre>from helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n</pre> from helper_functions import accuracy_fn loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_1.parameters(),                              lr=0.1) In\u00a0[25]: Copied! <pre>def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    model.to(device)\n    for batch, (X, y) in enumerate(data_loader):\n        # Send data to GPU\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.to(device)\n    model.eval() # put model in eval mode\n    # Turn on inference context manager\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred = model(X)\n            \n            # 2. Calculate loss and accuracy\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels\n            )\n        \n        # Adjust metrics and print out\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n</pre> def train_step(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,                optimizer: torch.optim.Optimizer,                accuracy_fn,                device: torch.device = device):     train_loss, train_acc = 0, 0     model.to(device)     for batch, (X, y) in enumerate(data_loader):         # Send data to GPU         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate loss         loss = loss_fn(y_pred, y)         train_loss += loss         train_acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels          # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()      # Calculate loss and accuracy per epoch and print out what's happening     train_loss /= len(data_loader)     train_acc /= len(data_loader)     print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")  def test_step(data_loader: torch.utils.data.DataLoader,               model: torch.nn.Module,               loss_fn: torch.nn.Module,               accuracy_fn,               device: torch.device = device):     test_loss, test_acc = 0, 0     model.to(device)     model.eval() # put model in eval mode     # Turn on inference context manager     with torch.inference_mode():          for X, y in data_loader:             # Send data to GPU             X, y = X.to(device), y.to(device)                          # 1. Forward pass             test_pred = model(X)                          # 2. Calculate loss and accuracy             test_loss += loss_fn(test_pred, y)             test_acc += accuracy_fn(y_true=y,                 y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels             )                  # Adjust metrics and print out         test_loss /= len(data_loader)         test_acc /= len(data_loader)         print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\") <p>Woohoo!</p> <p>Now we've got some functions for training and testing our model, let's run them.</p> <p>We'll do so inside another loop for each epoch.</p> <p>That way for each epoch we're going a training and a testing step.</p> <p>Note: You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.</p> <p>Let's also time things to see how long our code takes to run on the GPU.</p> In\u00a0[26]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_on_gpu = timer()  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_1,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn     )     test_step(data_loader=test_dataloader,         model=model_1,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn     )  train_time_end_on_gpu = timer() total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,                                             end=train_time_end_on_gpu,                                             device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 1.09199 | Train accuracy: 61.34%\nTest loss: 0.95636 | Test accuracy: 65.00%\n\nEpoch: 1\n---------\nTrain loss: 0.78101 | Train accuracy: 71.93%\nTest loss: 0.72227 | Test accuracy: 73.91%\n\nEpoch: 2\n---------\nTrain loss: 0.67027 | Train accuracy: 75.94%\nTest loss: 0.68500 | Test accuracy: 75.02%\n\nTrain time on cuda: 17.356 seconds\n</pre> <p>Excellent!</p> <p>Our model trained but the training time took longer?</p> <p>Note: The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.</p> <p>Question: \"I used a a GPU but my model didn't train faster, why might that be?\"</p> <p>Answer: Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.</p> <p>There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.</p> <p>So for smaller models and datasets, the CPU might actually be the optimal place to compute on.</p> <p>But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.</p> <p>However, this is largely dependant on the hardware you're using. With practice, you will get used to where the best place to train your models is.</p> <p>Let's evaluate our trained <code>model_1</code> using our <code>eval_model()</code> function and see how it went.</p> In\u00a0[27]: Copied! <pre>torch.manual_seed(42)\n\n# Note: This will error due to `eval_model()` not using device agnostic code \nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results\n</pre> torch.manual_seed(42)  # Note: This will error due to `eval_model()` not using device agnostic code  model_1_results = eval_model(model=model_1,      data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn)  model_1_results  <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb Cell 62 in &lt;cell line: 4&gt;()\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'&gt;1&lt;/a&gt; torch.manual_seed(42)\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'&gt;3&lt;/a&gt; # Note: This will error due to `eval_model()` not using device agnostic code \n----&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'&gt;4&lt;/a&gt; model_1_results = eval_model(model=model_1, \n      &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'&gt;5&lt;/a&gt;     data_loader=test_dataloader,\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     loss_fn=loss_fn, \n      &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'&gt;7&lt;/a&gt;     accuracy_fn=accuracy_fn) \n      &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'&gt;8&lt;/a&gt; model_1_results\n\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb Cell 62 in eval_model(model, data_loader, loss_fn, accuracy_fn)\n     &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'&gt;19&lt;/a&gt; with torch.inference_mode():\n     &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'&gt;20&lt;/a&gt;     for X, y in data_loader:\n     &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'&gt;21&lt;/a&gt;         # Make predictions with the model\n---&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'&gt;22&lt;/a&gt;         y_pred = model(X)\n     &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'&gt;24&lt;/a&gt;         # Accumulate the loss and accuracy values per batch\n     &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'&gt;25&lt;/a&gt;         loss += loss_fn(y_pred, y)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb Cell 62 in FashionMNISTModelV1.forward(self, x)\n     &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'&gt;13&lt;/a&gt; def forward(self, x: torch.Tensor):\n---&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B192.168.1.25/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/03_pytorch_computer_vision.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'&gt;14&lt;/a&gt;     return self.layer_stack(x)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre> <p>Oh no!</p> <p>It looks like our <code>eval_model()</code> function errors out with:</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code></p> <p>It's because we've setup our data and model to use device-agnostic code but not our evaluation function.</p> <p>How about we fix that by passing a target <code>device</code> parameter to our <code>eval_model()</code> function?</p> <p>Then we'll try calculating the results again.</p> In\u00a0[28]: Copied! <pre># Move values to device\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n\"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 1 results with device-agnostic code \nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n</pre> # Move values to device torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn,                 device: torch.device = device):     \"\"\"Evaluates a given model on a given dataset.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.         device (str, optional): Target device to compute on. Defaults to device.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Send data to the target device             X, y = X.to(device), y.to(device)             y_pred = model(X)             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))                  # Scale loss and acc         loss /= len(data_loader)         acc /= len(data_loader)     return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 1 results with device-agnostic code  model_1_results = eval_model(model=model_1, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn,     device=device ) model_1_results Out[28]: <pre>{'model_name': 'FashionMNISTModelV1',\n 'model_loss': 0.6850008964538574,\n 'model_acc': 75.01996805111821}</pre> In\u00a0[29]: Copied! <pre># Check baseline results\nmodel_0_results\n</pre> # Check baseline results model_0_results Out[29]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}</pre> <p>Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.</p> <p>That's a thing to note in machine learning, sometimes the thing you thought should work doesn't.</p> <p>And then the thing you thought might not work does.</p> <p>It's part science, part art.</p> <p>From the looks of things, it seems like our model is overfitting on the training data.</p> <p>Overfitting means our model is learning the training data well but those patterns aren't generalizing to the testing data.</p> <p>Two of the main to fix overfitting include:</p> <ol> <li>Using a smaller or different model (some models fit certain kinds of data better than others).</li> <li>Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).</li> </ol> <p>There are more, but I'm going to leave that as a challenge for you to explore.</p> <p>Try searching online, \"ways to prevent overfitting in machine learning\" and see what comes up.</p> <p>In the meantime, let's take a look at number 1: using a different model.</p> In\u00a0[30]: Copied! <pre># Create a convolutional neural network \nclass FashionMNISTModelV2(nn.Module):\n\"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n</pre> # Create a convolutional neural network  class FashionMNISTModelV2(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*7*7,                        out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.block_1(x)         # print(x.shape)         x = self.block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x  torch.manual_seed(42) model_2 = FashionMNISTModelV2(input_shape=1,      hidden_units=10,      output_shape=len(class_names)).to(device) model_2 Out[30]: <pre>FashionMNISTModelV2(\n  (block_1): Sequential(\n    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=490, out_features=10, bias=True)\n  )\n)</pre> <p>Nice!</p> <p>Our biggest model yet!</p> <p>What we've done is a common practice in machine learning.</p> <p>Find a model architecture somewhere and replicate it with code.</p> In\u00a0[31]: Copied! <pre>torch.manual_seed(42)\n\n# Create sample batch of random numbers with same size as image batch\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # get a single image for testing\nprint(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"Single image pixel values:\\n{test_image}\")\n</pre> torch.manual_seed(42)  # Create sample batch of random numbers with same size as image batch images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width] test_image = images[0] # get a single image for testing print(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\")  print(f\"Single image pixel values:\\n{test_image}\") <pre>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nSingle image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nSingle image pixel values:\ntensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n         ...,\n         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n\n        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n         ...,\n         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n\n        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n         ...,\n         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n</pre> <p>Let's create an example <code>nn.Conv2d()</code> with various parameters:</p> <ul> <li><code>in_channels</code> (int) - Number of channels in the input image.</li> <li><code>out_channels</code> (int) - Number of channels produced by the convolution.</li> <li><code>kernel_size</code> (int or tuple) - Size of the convolving kernel/filter.</li> <li><code>stride</code> (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.</li> <li><code>padding</code> (int, tuple, str) - Padding added to all four sides of input. Default: 0.</li> </ul> <p></p> <p>Example of what happens when you change the hyperparameters of a <code>nn.Conv2d()</code> layer.</p> In\u00a0[32]: Copied! <pre>torch.manual_seed(42)\n\n# Create a convolutional layer with same dimensions as TinyVGG \n# (try changing any of the parameters and see what happens)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # also try using \"valid\" or \"same\" here \n\n# Pass the data through the convolutional layer\nconv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)\n</pre> torch.manual_seed(42)  # Create a convolutional layer with same dimensions as TinyVGG  # (try changing any of the parameters and see what happens) conv_layer = nn.Conv2d(in_channels=3,                        out_channels=10,                        kernel_size=3,                        stride=1,                        padding=0) # also try using \"valid\" or \"same\" here   # Pass the data through the convolutional layer conv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)  Out[32]: <pre>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n         ...,\n         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n\n        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n         ...,\n         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n\n        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n         ...,\n         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n\n        ...,\n\n        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n         ...,\n         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n\n        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n         ...,\n         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n\n        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n         ...,\n         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n       grad_fn=&lt;SqueezeBackward1&gt;)</pre> <p>If we try to pass a single image in, we get a shape mismatch error:</p> <p><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead</code></p> <p>Note: If you're running PyTorch 1.11.0+, this error won't occur.</p> <p>This is because our <code>nn.Conv2d()</code> layer expects a 4-dimensional tensor as input with size <code>(N, C, H, W)</code> or <code>[batch_size, color_channels, height, width]</code>.</p> <p>Right now our single image <code>test_image</code> only has a shape of <code>[color_channels, height, width]</code> or <code>[3, 64, 64]</code>.</p> <p>We can fix this for a single image using <code>test_image.unsqueeze(dim=0)</code> to add an extra dimension for <code>N</code>.</p> In\u00a0[33]: Copied! <pre># Add extra dimension to test image\ntest_image.unsqueeze(dim=0).shape\n</pre> # Add extra dimension to test image test_image.unsqueeze(dim=0).shape Out[33]: <pre>torch.Size([1, 3, 64, 64])</pre> In\u00a0[34]: Copied! <pre># Pass test image with extra dimension through conv_layer\nconv_layer(test_image.unsqueeze(dim=0)).shape\n</pre> # Pass test image with extra dimension through conv_layer conv_layer(test_image.unsqueeze(dim=0)).shape Out[34]: <pre>torch.Size([1, 10, 62, 62])</pre> <p>Hmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on CNN Explainer), we get different channel sizes as well as different pixel sizes.</p> <p>What if we changed the values of <code>conv_layer</code>?</p> In\u00a0[35]: Copied! <pre>torch.manual_seed(42)\n# Create a new conv_layer with different values (try setting these to whatever you like)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n</pre> torch.manual_seed(42) # Create a new conv_layer with different values (try setting these to whatever you like) conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image                          out_channels=10,                          kernel_size=(5, 5), # kernel is usually a square so a tuple also works                          stride=2,                          padding=0)  # Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input) conv_layer_2(test_image.unsqueeze(dim=0)).shape Out[35]: <pre>torch.Size([1, 10, 30, 30])</pre> <p>Woah, we get another shape change.</p> <p>Now our image is of shape <code>[1, 10, 30, 30]</code> (it will be different if you use different values) or <code>[batch_size=1, color_channels=10, height=30, width=30]</code>.</p> <p>What's going on here?</p> <p>Behind the scenes, our <code>nn.Conv2d()</code> is compressing the information stored in the image.</p> <p>It does this by performing operations on the input (our test image) against its internal parameters.</p> <p>The goal of this is similar to all of the other neural networks we've been building.</p> <p>Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.</p> <p>The only difference is how the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer <code>forward()</code> method.</p> <p>If we check out our <code>conv_layer_2.state_dict()</code> we'll find a similar weight and bias setup as we've seen before.</p> In\u00a0[36]: Copied! <pre># Check out the conv_layer_2 internal parameters\nprint(conv_layer_2.state_dict())\n</pre> # Check out the conv_layer_2 internal parameters print(conv_layer_2.state_dict()) <pre>OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n\n         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n\n         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n\n\n        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n\n         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n\n         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n\n\n        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n\n         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n\n         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n\n\n        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n\n         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n\n         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n\n\n        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n\n         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n\n         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n\n\n        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n\n         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n\n         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n\n\n        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n\n         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n\n         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n\n\n        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n\n         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n\n         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n\n\n        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n\n         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n\n         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n\n\n        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n\n         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n\n         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n        -0.0797,  0.1121]))])\n</pre> <p>Look at that! A bunch of random numbers for a weight and bias tensor.</p> <p>The shapes of these are manipulated by the inputs we passed to <code>nn.Conv2d()</code> when we set it up.</p> <p>Let's check them out.</p> In\u00a0[37]: Copied! <pre># Get shapes of weight and bias tensors within conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n</pre> # Get shapes of weight and bias tensors within conv_layer_2 print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\") print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\") <pre>conv_layer_2 weight shape: \ntorch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n\nconv_layer_2 bias shape: \ntorch.Size([10]) -&gt; [out_channels=10]\n</pre> <p>Question: What should we set the parameters of our <code>nn.Conv2d()</code> layers?</p> <p>That's a good one. But similar to many other things in machine learning, the values of these aren't set in stone (and recall, because these values are ones we can set ourselves, they're referred to as \"hyperparameters\").</p> <p>The best way to find out is to try out different values and see how they effect your model's performance.</p> <p>Or even better, find a working example on a problem similar to yours (like we've done with TinyVGG) and copy it.</p> <p>We're working with a different of layer here to what we've seen before.</p> <p>But the premise remains the same: start with random numbers and update them to better represent the data.</p> In\u00a0[38]: Copied! <pre># Print out original image shape without and with unsqueezed dimension\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Create a sample nn.MaxPoo2d() layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass data through just the conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pass data through the max pool layer\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n</pre> # Print out original image shape without and with unsqueezed dimension print(f\"Test image original shape: {test_image.shape}\") print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")  # Create a sample nn.MaxPoo2d() layer max_pool_layer = nn.MaxPool2d(kernel_size=2)  # Pass data through just the conv_layer test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0)) print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")  # Pass data through the max pool layer test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv) print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\") <pre>Test image original shape: torch.Size([3, 64, 64])\nTest image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\nShape after going through conv_layer(): torch.Size([1, 10, 62, 62])\nShape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n</pre> <p>Notice the change in the shapes of what's happening in and out of a <code>nn.MaxPool2d()</code> layer.</p> <p>The <code>kernel_size</code> of the <code>nn.MaxPool2d()</code> layer will effects the size of the output shape.</p> <p>In our case, the shape halves from a <code>62x62</code> image to <code>31x31</code> image.</p> <p>Let's see this work with a smaller tensor.</p> In\u00a0[39]: Copied! <pre>torch.manual_seed(42)\n# Create a random tensor with a similiar number of dimensions to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n</pre> torch.manual_seed(42) # Create a random tensor with a similiar number of dimensions to our images random_tensor = torch.randn(size=(1, 1, 2, 2)) print(f\"Random tensor:\\n{random_tensor}\") print(f\"Random tensor shape: {random_tensor.shape}\")  # Create a max pool layer max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value   # Pass the random tensor through the max pool layer max_pool_tensor = max_pool_layer(random_tensor) print(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\") print(f\"Max pool tensor shape: {max_pool_tensor.shape}\") <pre>Random tensor:\ntensor([[[[0.3367, 0.1288],\n          [0.2345, 0.2303]]]])\nRandom tensor shape: torch.Size([1, 1, 2, 2])\n\nMax pool tensor:\ntensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor\nMax pool tensor shape: torch.Size([1, 1, 1, 1])\n</pre> <p>Notice the final two dimensions between <code>random_tensor</code> and <code>max_pool_tensor</code>, they go from <code>[2, 2]</code> to <code>[1, 1]</code>.</p> <p>In essence, they get halved.</p> <p>And the change would be different for different values of <code>kernel_size</code> for <code>nn.MaxPool2d()</code>.</p> <p>Also notice the value leftover in <code>max_pool_tensor</code> is the maximum value from <code>random_tensor</code>.</p> <p>What's happening here?</p> <p>This is another important piece of the puzzle of neural networks.</p> <p>Essentially, every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space.</p> <p>In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being smaller in size than the original values.</p> <p>From an artificial intelligence perspective, you could consider the whole goal of a neural network to compress information.</p> <p></p> <p>This means, that from the point of view of a neural network, intelligence is compression.</p> <p>This is the idea of the use of a <code>nn.MaxPool2d()</code> layer: take the maximum value from a portion of a tensor and disregard the rest.</p> <p>In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.</p> <p>It is the same story for a <code>nn.Conv2d()</code> layer.</p> <p>Except instead of just taking the maximum, the <code>nn.Conv2d()</code> performs a convolutional operation on the data (see this in action on the CNN Explainer webpage).</p> <p>Exercise: What do you think the <code>nn.AvgPool2d()</code> layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.</p> <p>Extra-curriculum: Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the <code>torchvision.models</code> library? What do you think you could do with these?</p> In\u00a0[40]: Copied! <pre># Setup loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n</pre> # Setup loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_2.parameters(),                               lr=0.1) In\u00a0[41]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# Train and test model \nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_model_2 = timer()  # Train and test model  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_2,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn,         device=device     )     test_step(data_loader=test_dataloader,         model=model_2,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn,         device=device     )  train_time_end_model_2 = timer() total_train_time_model_2 = print_train_time(start=train_time_start_model_2,                                            end=train_time_end_model_2,                                            device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 0.59738 | Train accuracy: 78.40%\nTest loss: 0.40513 | Test accuracy: 85.51%\n\nEpoch: 1\n---------\nTrain loss: 0.36556 | Train accuracy: 86.87%\nTest loss: 0.35803 | Test accuracy: 86.75%\n\nEpoch: 2\n---------\nTrain loss: 0.32796 | Train accuracy: 88.16%\nTest loss: 0.32870 | Test accuracy: 88.19%\n\nTrain time on cuda: 23.650 seconds\n</pre> <p>Woah! Looks like the convolutional and max pooling layers helped improve performance a little.</p> <p>Let's evaluate <code>model_2</code>'s results with our <code>eval_model()</code> function.</p> In\u00a0[42]: Copied! <pre># Get model_2 results \nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results\n</pre> # Get model_2 results  model_2_results = eval_model(     model=model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn ) model_2_results Out[42]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.3286978006362915,\n 'model_acc': 88.18889776357827}</pre> In\u00a0[43]: Copied! <pre>import pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n</pre> import pandas as pd compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results]) compare_results Out[43]: model_name model_loss model_acc 0 FashionMNISTModelV0 0.476639 83.426518 1 FashionMNISTModelV1 0.685001 75.019968 2 FashionMNISTModelV2 0.328698 88.188898 <p>Nice!</p> <p>We can add the training time values too.</p> In\u00a0[44]: Copied! <pre># Add training times to results comparison\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n</pre> # Add training times to results comparison compare_results[\"training_time\"] = [total_train_time_model_0,                                     total_train_time_model_1,                                     total_train_time_model_2] compare_results Out[44]: model_name model_loss model_acc training_time 0 FashionMNISTModelV0 0.476639 83.426518 15.548849 1 FashionMNISTModelV1 0.685001 75.019968 17.355571 2 FashionMNISTModelV2 0.328698 88.188898 23.650468 <p>It looks like our CNN (<code>FashionMNISTModelV2</code>) model performed the best (lowest loss, highest accuracy) but had the longest training time.</p> <p>And our baseline model (<code>FashionMNISTModelV0</code>) performed better than <code>model_1</code> (<code>FashionMNISTModelV1</code>).</p> In\u00a0[45]: Copied! <pre># Visualize our model results\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");\n</pre> # Visualize our model results compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\") plt.xlabel(\"accuracy (%)\") plt.ylabel(\"model\"); In\u00a0[46]: Copied! <pre>def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n</pre> def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):     pred_probs = []     model.eval()     with torch.inference_mode():         for sample in data:             # Prepare sample             sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device              # Forward pass (model outputs raw logit)             pred_logit = model(sample)              # Get prediction probability (logit -&gt; prediction probability)             pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)              # Get pred_prob off GPU for further calculations             pred_probs.append(pred_prob.cpu())                  # Stack the pred_probs to turn list into a tensor     return torch.stack(pred_probs) In\u00a0[47]: Copied! <pre>import random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# View the first test sample shape and label\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n</pre> import random random.seed(42) test_samples = [] test_labels = [] for sample, label in random.sample(list(test_data), k=9):     test_samples.append(sample)     test_labels.append(label)  # View the first test sample shape and label print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\") <pre>Test sample image shape: torch.Size([1, 28, 28])\nTest sample label: 5 (Sandal)\n</pre> <p>And now we can use our <code>make_predictions()</code> function to predict on <code>test_samples</code>.</p> In\u00a0[48]: Copied! <pre># Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n</pre> # Make predictions on test samples with model 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # View first two prediction probabilities list pred_probs[:2] Out[48]: <pre>tensor([[9.5605e-08, 1.9588e-08, 2.7693e-08, 2.5152e-08, 9.9790e-09, 9.9992e-01,\n         1.1720e-07, 8.6103e-06, 9.0887e-06, 5.7658e-05],\n        [8.2017e-02, 6.7443e-01, 8.9147e-04, 1.2562e-01, 2.5472e-02, 3.7753e-05,\n         9.0767e-02, 2.8317e-04, 3.7734e-04, 9.8759e-05]])</pre> <p>Excellent!</p> <p>And now we can go from prediction probabilities to prediction labels by taking the <code>torch.argmax()</code> of the output of the <code>torch.softmax()</code> activation function.</p> In\u00a0[49]: Copied! <pre># Turn the prediction probabilities into prediction labels by taking the argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n</pre> # Turn the prediction probabilities into prediction labels by taking the argmax() pred_classes = pred_probs.argmax(dim=1) pred_classes Out[49]: <pre>tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])</pre> In\u00a0[50]: Copied! <pre># Are our predictions in the same form as our test labels? \ntest_labels, pred_classes\n</pre> # Are our predictions in the same form as our test labels?  test_labels, pred_classes Out[50]: <pre>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))</pre> <p>Now our predicted classes are in the same format as our test labels, we can compare.</p> <p>Since we're dealing with image data, let's stay true to the data explorer's motto.</p> <p>\"Visualize, visualize, visualize!\"</p> In\u00a0[51]: Copied! <pre># Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n</pre> # Plot predictions plt.figure(figsize=(9, 9)) nrows = 3 ncols = 3 for i, sample in enumerate(test_samples):   # Create a subplot   plt.subplot(nrows, ncols, i+1)    # Plot the target image   plt.imshow(sample.squeeze(), cmap=\"gray\")    # Find the prediction label (in text form, e.g. \"Sandal\")   pred_label = class_names[pred_classes[i]]    # Get the truth label (in text form, e.g. \"T-shirt\")   truth_label = class_names[test_labels[i]]     # Create the title text of the plot   title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"      # Check for equality and change title colour accordingly   if pred_label == truth_label:       plt.title(title_text, fontsize=10, c=\"g\") # green text if correct   else:       plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong   plt.axis(False); <p>Well, well, well, doesn't that look good!</p> <p>Not bad for a couple dozen lines of PyTorch code!</p> In\u00a0[58]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# 1. Make predictions with trained model\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenate list of predictions into a tensor\ny_pred_tensor = torch.cat(y_preds)\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # 1. Make predictions with trained model y_preds = [] model_2.eval() with torch.inference_mode():   for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):     # Send data and targets to target device     X, y = X.to(device), y.to(device)     # Do the forward pass     y_logit = model_2(X)     # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels     y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)     # Put predictions on CPU for evaluation     y_preds.append(y_pred.cpu()) # Concatenate list of predictions into a tensor y_pred_tensor = torch.cat(y_preds) <pre>Making predictions:   0%|          | 0/313 [00:00&lt;?, ?it/s]</pre> <p>Wonderful!</p> <p>Now we've got predictions, let's go through steps 2 &amp; 3: 2. Make a confusion matrix using <code>torchmetrics.ConfusionMatrix</code>. 3. Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code>.</p> <p>First we'll need to make sure we've got <code>torchmetrics</code> and <code>mlxtend</code> installed (these two libraries will help us make and visual a confusion matrix).</p> <p>Note: If you're using Google Colab, the default version of <code>mlxtend</code> installed is 0.14.0 (as of March 2022), however, for the parameters of the <code>plot_confusion_matrix()</code> function we'd like use, we need 0.19.0 or higher.</p> In\u00a0[62]: Copied! <pre># See if torchmetrics exists, if not, install it\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n</pre> # See if torchmetrics exists, if not, install it try:     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\")     assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\" except:     !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\") <pre>mlxtend version: 0.21.0\n</pre> <p>To plot the confusion matrix, we need to make sure we've got and <code>mlxtend</code> version of 0.19.0 or higher.</p> In\u00a0[63]: Copied! <pre># Import mlxtend upgraded version\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n</pre> # Import mlxtend upgraded version import mlxtend  print(mlxtend.__version__) assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher <pre>0.21.0\n</pre> <p><code>torchmetrics</code> and <code>mlxtend</code> installed, let's make a confusion matrix!</p> <p>First we'll create a <code>torchmetrics.ConfusionMatrix</code> instance telling it how many classes we're dealing with by setting <code>num_classes=len(class_names)</code>.</p> <p>Then we'll create a confusion matrix (in tensor format) by passing our instance our model's predictions (<code>preds=y_pred_tensor</code>) and targets (<code>target=test_data.targets</code>).</p> <p>Finally we can plot our confision matrix using the <code>plot_confusion_matrix()</code> function from <code>mlxtend.plotting</code>.</p> In\u00a0[64]: Copied! <pre>from torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Setup confusion matrix instance and compare predictions to targets\nconfmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Plot the confusion matrix\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);\n</pre> from torchmetrics import ConfusionMatrix from mlxtend.plotting import plot_confusion_matrix  # 2. Setup confusion matrix instance and compare predictions to targets confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass') confmat_tensor = confmat(preds=y_pred_tensor,                          target=test_data.targets)  # 3. Plot the confusion matrix fig, ax = plot_confusion_matrix(     conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy      class_names=class_names, # turn the row and column labels into class names     figsize=(10, 7) ); <p>Woah! Doesn't that look good?</p> <p>We can see our model does fairly well since most of the dark squares are down the diagonal from top left to bottom right (and ideal model will have only values in these squares and 0 everywhere else).</p> <p>The model gets most \"confused\" on classes that are similar, for example predicting \"Pullover\" for images that are actually labelled \"Shirt\".</p> <p>And the same for predicting \"Shirt\" for classes that are actually labelled \"T-shirt/top\".</p> <p>This kind of information is often more helpful than a single accuracy metric because it tells use where a model is getting things wrong.</p> <p>It also hints at why the model may be getting certain things wrong.</p> <p>It's understandable the model sometimes predicts \"Shirt\" for images labelled \"T-shirt/top\".</p> <p>We can use this kind of information to further inspect our models and data to see how it could be improved.</p> <p>Exercise: Use the trained <code>model_2</code> to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should've been. After visualing these predictions do you think it's more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?</p> In\u00a0[65]: Copied! <pre>from pathlib import Path\n\n# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # create parent directories if needed\n                 exist_ok=True # if models directory already exists, don't error\n)\n\n# Create model save path\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, # create parent directories if needed                  exist_ok=True # if models directory already exists, don't error )  # Create model save path MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # Save the model state dict print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters            f=MODEL_SAVE_PATH) <pre>Saving model to: models/03_pytorch_computer_vision_model_2.pth\n</pre> <p>Now we've got a saved model <code>state_dict()</code> we can load it back in using a combination of <code>load_state_dict()</code> and <code>torch.load()</code>.</p> <p>Since we're using <code>load_state_dict()</code>, we'll need to create a new instance of <code>FashionMNISTModelV2()</code> with the same input parameters as our saved model <code>state_dict()</code>.</p> In\u00a0[66]: Copied! <pre># Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n# Note: loading model will error if the shapes here aren't the same as the saved version\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # try changing this to 128 and seeing what happens \n                                    output_shape=10) \n\n# Load in the saved state_dict()\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# Send model to GPU\nloaded_model_2 = loaded_model_2.to(device)\n</pre> # Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict()) # Note: loading model will error if the shapes here aren't the same as the saved version loaded_model_2 = FashionMNISTModelV2(input_shape=1,                                      hidden_units=10, # try changing this to 128 and seeing what happens                                      output_shape=10)   # Load in the saved state_dict() loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))  # Send model to GPU loaded_model_2 = loaded_model_2.to(device) <p>And now we've got a loaded model we can evaluate it with <code>eval_model()</code> to make sure its parameters work similarly to <code>model_2</code> prior to saving.</p> In\u00a0[67]: Copied! <pre># Evaluate loaded model\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n</pre> # Evaluate loaded model torch.manual_seed(42)  loaded_model_2_results = eval_model(     model=loaded_model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn )  loaded_model_2_results Out[67]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.3286978006362915,\n 'model_acc': 88.18889776357827}</pre> <p>Do these results look the same as <code>model_2_results</code>?</p> In\u00a0[68]: Copied! <pre>model_2_results\n</pre> model_2_results Out[68]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.3286978006362915,\n 'model_acc': 88.18889776357827}</pre> <p>We can find out if two tensors are close to each other using <code>torch.isclose()</code> and passing in a tolerance level of closeness via the parameters <code>atol</code> (absolute tolerance) and <code>rtol</code> (relative tolerance).</p> <p>If our model's results are close, the output of <code>torch.isclose()</code> should be true.</p> In\u00a0[69]: Copied! <pre># Check to see if results are close to each other (if they are very far away, there may be an error)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # absolute tolerance\n              rtol=0.0001) # relative tolerance\n</pre> # Check to see if results are close to each other (if they are very far away, there may be an error) torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),                torch.tensor(loaded_model_2_results[\"model_loss\"]),               atol=1e-08, # absolute tolerance               rtol=0.0001) # relative tolerance Out[69]: <pre>tensor(True)</pre>"},{"location":"03_pytorch_computer_vision/#03-pytorch-computer-vision","title":"03. PyTorch Computer Vision\u00b6","text":"<p>Computer vision is the art of teaching a computer to see.</p> <p>For example, it could involve building a model to classify whether a photo is of a cat or a dog (binary classification).</p> <p>Or whether a photo is of a cat, dog or chicken (multi-class classification).</p> <p>Or identifying where a car appears in a video frame (object detection).</p> <p>Or figuring out where different objects in an image can be separated (panoptic segmentation).</p> <p> Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.</p>"},{"location":"03_pytorch_computer_vision/#where-does-computer-vision-get-used","title":"Where does computer vision get used?\u00b6","text":"<p>If you use a smartphone, you've already used computer vision.</p> <p>Camera and photo apps use computer vision to enhance and sort images.</p> <p>Modern cars use computer vision to avoid other cars and stay within lane lines.</p> <p>Manufacturers use computer vision to identify defects in various products.</p> <p>Security cameras use computer vision to detect potential intruders.</p> <p>In essence, anything that can described in a visual sense can be a potential computer vision problem.</p>"},{"location":"03_pytorch_computer_vision/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to apply the PyTorch Workflow we've been learning in the past couple of sections to computer vision.</p> <p></p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Computer vision libraries in PyTorch PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out. 1. Load data To practice computer vision, we'll start with some images of different pieces of clothing from FashionMNIST. 2. Prepare data We've got some images, let's load them in with a PyTorch <code>DataLoader</code> so we can use them with our training loop. 3. Model 0: Building a baseline model Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a loss function, optimizer and build a training loop. 4. Making predictions and evaluting model 0 Let's make some predictions with our baseline model and evaluate them. 5. Setup device agnostic code for future models It's best practice to write device-agnostic code, so let's set it up. 6. Model 1: Adding non-linearity Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers. 7. Model 2: Convolutional Neural Network (CNN) Time to get computer vision specific and introduce the powerful convolutional neural network architecture. 8. Comparing our models We've built three different models, let's compare them. 9. Evaluating our best model Let's make some predictons on random images and evaluate our best model. 10. Making a confusion matrix A confusion matrix is a great way to evaluate a classification model, let's see how we can make one. 11. Saving and loading the best performing model Since we might want to use our model for later, let's save it and make sure it loads back in correctly."},{"location":"03_pytorch_computer_vision/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"03_pytorch_computer_vision/#0-computer-vision-libraries-in-pytorch","title":"0. Computer vision libraries in PyTorch\u00b6","text":"<p>Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.</p> PyTorch module What does it do? <code>torchvision</code> Contains datasets, model architectures and image transformations often used for computer vision problems. <code>torchvision.datasets</code> Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains a series of base classes for making custom datasets. <code>torchvision.models</code> This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. <code>torchvision.transforms</code> Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. <code>torch.utils.data.Dataset</code> Base dataset class for PyTorch. <code>torch.utils.data.DataLoader</code> Creates a Python iteralbe over a dataset (created with <code>torch.utils.data.Dataset</code>). <p>Note: The <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.</p> <p>Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies.</p>"},{"location":"03_pytorch_computer_vision/#1-getting-a-dataset","title":"1. Getting a dataset\u00b6","text":"<p>To begin working on a computer vision problem, let's get a computer vision dataset.</p> <p>We're going to start with FashionMNIST.</p> <p>MNIST stands for Modified National Institute of Standards and Technology.</p> <p>The original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.</p> <p>FashionMNIST, made by Zalando Research, is a similar setup.</p> <p>Except it contains grayscale images of 10 different kinds of clothing.</p> <p> <code>torchvision.datasets</code> contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.</p> <p>Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.</p> <p>PyTorch has a bunch of common computer vision datasets stored in <code>torchvision.datasets</code>.</p> <p>Including FashionMNIST in <code>torchvision.datasets.FashionMNIST()</code>.</p> <p>To download it, we provide the following parameters:</p> <ul> <li><code>root: str</code> - which folder do you want to download the data to?</li> <li><code>train: Bool</code> - do you want the training or test split?</li> <li><code>download: Bool</code> - should the data be downloaded?</li> <li><code>transform: torchvision.transforms</code> - what transformations would you like to do on the data?</li> <li><code>target_transform</code> - you can transform the targets (labels) if you like too.</li> </ul> <p>Many other datasets in <code>torchvision</code> have these parameter options.</p>"},{"location":"03_pytorch_computer_vision/#11-input-and-output-shapes-of-a-computer-vision-model","title":"1.1 Input and output shapes of a computer vision model\u00b6","text":"<p>We've got a big tensor of values (the image) leading to a single value for the target (the label).</p> <p>Let's see the image shape.</p>"},{"location":"03_pytorch_computer_vision/#12-visualizing-our-data","title":"1.2 Visualizing our data\u00b6","text":""},{"location":"03_pytorch_computer_vision/#2-prepare-dataloader","title":"2. Prepare DataLoader\u00b6","text":"<p>Now we've got a dataset ready to go.</p> <p>The next step is to prepare it with a <code>torch.utils.data.DataLoader</code> or <code>DataLoader</code> for short.</p> <p>The <code>DataLoader</code> does what you think it might do.</p> <p>It helps load data into a model.</p> <p>For training and for inference.</p> <p>It turns a large <code>Dataset</code> into a Python iterable of smaller chunks.</p> <p>These smaller chunks are called batches or mini-batches and can be set by the <code>batch_size</code> parameter.</p> <p>Why do this?</p> <p>Because it's more computationally efficient.</p> <p>In an ideal world you could do the forward pass and backward pass across all of your data at once.</p> <p>But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.</p> <p>It also gives your model more opportunities to improve.</p> <p>With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).</p> <p>What's a good batch size?</p> <p>32 is a good place to start for a fair amount of problems.</p> <p>But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).</p> <p> Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.</p> <p>Let's create <code>DataLoader</code>'s for our training and test sets.</p>"},{"location":"03_pytorch_computer_vision/#3-model-0-build-a-baseline-model","title":"3. Model 0: Build a baseline model\u00b6","text":"<p>Data loaded and prepared!</p> <p>Time to build a baseline model by subclassing <code>nn.Module</code>.</p> <p>A baseline model is one of the simplest models you can imagine.</p> <p>You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.</p> <p>Our baseline will consist of two <code>nn.Linear()</code> layers.</p> <p>We've done this in a previous section but there's going to one slight difference.</p> <p>Because we're working with image data, we're going to use a different layer to start things off.</p> <p>And that's the <code>nn.Flatten()</code> layer.</p> <p><code>nn.Flatten()</code> compresses the dimensions of a tensor into a single vector.</p> <p>This is easier to understand when you see it.</p>"},{"location":"03_pytorch_computer_vision/#31-setup-loss-optimizer-and-evaluation-metrics","title":"3.1 Setup loss, optimizer and evaluation metrics\u00b6","text":"<p>Since we're working on a classification problem, let's bring in our <code>helper_functions.py</code> script and subsequently the <code>accuracy_fn()</code> we defined in notebook 02.</p> <p>Note: Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the TorchMetrics package.</p>"},{"location":"03_pytorch_computer_vision/#32-creating-a-function-to-time-our-experiments","title":"3.2 Creating a function to time our experiments\u00b6","text":"<p>Loss function and optimizer ready!</p> <p>It's time to start training a model.</p> <p>But how about we do a little experiment while we train.</p> <p>I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.</p> <p>We'll train this model on the CPU but the next one on the GPU and see what happens.</p> <p>Our timing function will import the <code>timeit.default_timer()</code> function from the Python <code>timeit</code> module.</p>"},{"location":"03_pytorch_computer_vision/#33-creating-a-training-loop-and-training-a-model-on-batches-of-data","title":"3.3 Creating a training loop and training a model on batches of data\u00b6","text":"<p>Beautiful!</p> <p>Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.</p> <p>Let's now create a training loop and a testing loop to train and evaluate our model.</p> <p>We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.</p> <p>Our data batches are contained within our <code>DataLoader</code>s, <code>train_dataloader</code> and <code>test_dataloader</code> for the training and test data splits respectively.</p> <p>A batch is <code>BATCH_SIZE</code> samples of <code>X</code> (features) and <code>y</code> (labels), since we're using <code>BATCH_SIZE=32</code>, our batches have 32 samples of images and targets.</p> <p>And since we're computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.</p> <p>This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.</p> <p>Let's step through it:</p> <ol> <li>Loop through epochs.</li> <li>Loop through training batches, perform training steps, calculate the train loss per batch.</li> <li>Loop through testing batches, perform testing steps, calculate the test loss per batch.</li> <li>Print out what's happening.</li> <li>Time it all (for fun).</li> </ol> <p>A fair few steps but...</p> <p>...if in doubt, code it out.</p>"},{"location":"03_pytorch_computer_vision/#4-make-predictions-and-get-model-0-results","title":"4. Make predictions and get Model 0 results\u00b6","text":"<p>Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.</p> <p>Namely, let's create a function that takes in a trained model, a <code>DataLoader</code>, a loss function and an accuracy function.</p> <p>The function will use the model to make predictions on the data in the <code>DataLoader</code> and then we can evaluate those predictions using the loss function and accuracy function.</p>"},{"location":"03_pytorch_computer_vision/#5-setup-device-agnostic-code-for-using-a-gpu-if-there-is-one","title":"5. Setup device agnostic-code (for using a GPU if there is one)\u00b6","text":"<p>We've seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.</p> <p>Note: Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.</p> <p>Now let's setup some device-agnostic code for our models and data to run on GPU if it's available.</p> <p>If you're running this notebook on Google Colab, and you don't a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p>"},{"location":"03_pytorch_computer_vision/#6-model-1-building-a-better-model-with-non-linearity","title":"6. Model 1: Building a better model with non-linearity\u00b6","text":"<p>We learned about the power of non-linearity in notebook 02.</p> <p>Seeing the data we've been working with, do you think it needs non-linear functions?</p> <p>And remember, linear means straight and non-linear means non-straight.</p> <p>Let's find out.</p> <p>We'll do so by recreating a similar model to before, except this time we'll put non-linear functions (<code>nn.ReLU()</code>) in between each linear layer.</p>"},{"location":"03_pytorch_computer_vision/#61-setup-loss-optimizer-and-evaluation-metrics","title":"6.1 Setup loss, optimizer and evaluation metrics\u00b6","text":"<p>As usual, we'll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we'll stick with accuracy for now).</p>"},{"location":"03_pytorch_computer_vision/#62-functionizing-training-and-test-loops","title":"6.2 Functionizing training and test loops\u00b6","text":"<p>So far we've been writing train and test loops over and over.</p> <p>Let's write them again but this time we'll put them in functions so they can be called again and again.</p> <p>And because we're using device-agnostic code now, we'll be sure to call <code>.to(device)</code> on our feature (<code>X</code>) and target (<code>y</code>) tensors.</p> <p>For the training loop we'll create a function called <code>train_step()</code> which takes in a model, a <code>DataLoader</code> a loss function and an optimizer.</p> <p>The testing loop will be similar but it'll be called <code>test_step()</code> and it'll take in a model, a <code>DataLoader</code>, a loss function and an evaluation function.</p> <p>Note: Since these are functions, you can customize them in any way you like. What we're making here can be considered barebones training and testing functions for our specific classification use case.</p>"},{"location":"03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn","title":"7. Model 2: Building a Convolutional Neural Network (CNN)\u00b6","text":"<p>Alright, time to step things up a notch.</p> <p>It's time to create a Convolutional Neural Network (CNN or ConvNet).</p> <p>CNN's are known for their capabilities to find patterns in visual data.</p> <p>And since we're dealing with visual data, let's see if using a CNN model can improve upon our baseline.</p> <p>The CNN model we're going to be using is known as TinyVGG from the CNN Explainer website.</p> <p>It follows the typical structure of a convolutional neural network:</p> <p><code>Input layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer</code></p> <p>Where the contents of <code>[Convolutional layer -&gt; activation layer -&gt; pooling layer]</code> can be upscaled and repeated multiple times, depending on requirements.</p>"},{"location":"03_pytorch_computer_vision/#what-model-should-i-use","title":"What model should I use?\u00b6","text":"<p>Question: Wait, you say CNN's are good for images, are there any other model types I should be aware of?</p> <p>Good question.</p> <p>This table is a good general guide for which model to use (though there are exceptions).</p> Problem type Model to use (generally) Code example Structured data (Excel spreadsheets, row and column data) Gradient boosted models, Random Forests, XGBoost <code>sklearn.ensemble</code>, XGBoost library Unstructured data (images, audio, language) Convolutional Neural Networks, Transformers <code>torchvision.models</code>, HuggingFace Transformers <p>Note: The table above is only for reference, the model you end up using will be highly dependant on the problem you're working on and the constraints you have (amount of data, latency requirements).</p> <p>Enough talking about models, let's now build a CNN that replicates the model on the CNN Explainer website.</p> <p></p> <p>To do so, we'll leverage the <code>nn.Conv2d()</code> and <code>nn.MaxPool2d()</code> layers from <code>torch.nn</code>.</p>"},{"location":"03_pytorch_computer_vision/#71-stepping-through-nnconv2d","title":"7.1 Stepping through <code>nn.Conv2d()</code>\u00b6","text":"<p>We could start using our model above and see what happens but let's first step through the two new layers we've added:</p> <ul> <li><code>nn.Conv2d()</code>, also known as a convolutional layer.</li> <li><code>nn.MaxPool2d()</code>, also known as a max pooling layer.</li> </ul> <p>Question: What does the \"2d\" in <code>nn.Conv2d()</code> stand for?</p> <p>The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there's color channel dimension but each of the color channel dimensions have two dimensions too: height and width.</p> <p>For other dimensional data (such as 1D for text or 3D for 3D objects) there's also <code>nn.Conv1d()</code> and <code>nn.Conv3d()</code>.</p> <p>To test the layers out, let's create some toy data just like the data used on CNN Explainer.</p>"},{"location":"03_pytorch_computer_vision/#72-stepping-through-nnmaxpool2d","title":"7.2 Stepping through <code>nn.MaxPool2d()</code>\u00b6","text":"<p>Now let's check out what happens when we move data through <code>nn.MaxPool2d()</code>.</p>"},{"location":"03_pytorch_computer_vision/#73-setup-a-loss-function-and-optimizer-for-model_2","title":"7.3 Setup a loss function and optimizer for <code>model_2</code>\u00b6","text":"<p>We've stepped through the layers in our first CNN enough.</p> <p>But remember, if something still isn't clear, try starting small.</p> <p>Pick a single layer of a model, pass some data through it and see what happens.</p> <p>Now it's time to move forward and get to training!</p> <p>Let's setup a loss function and an optimizer.</p> <p>We'll use the functions as before, <code>nn.CrossEntropyLoss()</code> as the loss function (since we're working with multi-class classification data).</p> <p>And <code>torch.optim.SGD()</code> as the optimizer to optimize <code>model_2.parameters()</code> with a learning rate of <code>0.1</code>.</p>"},{"location":"03_pytorch_computer_vision/#74-training-and-testing-model_2-using-our-training-and-test-functions","title":"7.4 Training and testing <code>model_2</code> using our training and test functions\u00b6","text":"<p>Loss and optimizer ready!</p> <p>Time to train and test.</p> <p>We'll use our <code>train_step()</code> and <code>test_step()</code> functions we created before.</p> <p>We'll also measure the time to compare it to our other models.</p>"},{"location":"03_pytorch_computer_vision/#8-compare-model-results-and-training-time","title":"8. Compare model results and training time\u00b6","text":"<p>We've trained three different models.</p> <ol> <li><code>model_0</code> - our baseline model with two <code>nn.Linear()</code> layers.</li> <li><code>model_1</code> - the same setup as our baseline model except with <code>nn.ReLU()</code> layers in between the <code>nn.Linear()</code> layers.</li> <li><code>model_2</code> - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.</li> </ol> <p>This is a regular practice in machine learning.</p> <p>Building multiple models and performing multiple training experiments to see which performs best.</p> <p>Let's combine our model results dictionaries into a DataFrame and find out.</p>"},{"location":"03_pytorch_computer_vision/#performance-speed-tradeoff","title":"Performance-speed tradeoff\u00b6","text":"<p>Something to be aware of in machine learning is the performance-speed tradeoff.</p> <p>Generally, you get better performance out of a larger, more complex model (like we did with <code>model_2</code>).</p> <p>However, this performance increase often comes at a sacrifice of training speed and inference speed.</p> <p>Note: The training times you get will be very dependant on the hardware you use.</p> <p>Generally, the more CPU cores you have, the faster your models will train on CPU. And similar for GPUs.</p> <p>Newer hardware (in terms of age) will also often train models faster due to incorporating technology advances.</p> <p>How about we get visual?</p>"},{"location":"03_pytorch_computer_vision/#9-make-and-evaluate-random-predictions-with-best-model","title":"9. Make and evaluate random predictions with best model\u00b6","text":"<p>Alright, we've compared our models to each other, let's further evaluate our best performing model, <code>model_2</code>.</p> <p>To do so, let's create a function <code>make_predictions()</code> where we can pass the model and some data for it to predict on.</p>"},{"location":"03_pytorch_computer_vision/#10-making-a-confusion-matrix-for-further-prediction-evaluation","title":"10. Making a confusion matrix for further prediction evaluation\u00b6","text":"<p>There are many different evaluation metrics we can use for classification problems.</p> <p>One of the most visual is a confusion matrix.</p> <p>A confusion matrix shows you where your classification model got confused between predicitons and true labels.</p> <p>To make a confusion matrix, we'll go through three steps:</p> <ol> <li>Make predictions with our trained model, <code>model_2</code> (a confusion matrix compares predictions to true labels).</li> <li>Make a confusion matrix using <code>torchmetrics.ConfusionMatrix</code>.</li> <li>Plot the confusion matrix using <code>mlxtend.plotting.plot_confusion_matrix()</code>.</li> </ol> <p>Let's start by making predictions with our trained model.</p>"},{"location":"03_pytorch_computer_vision/#11-save-and-load-best-performing-model","title":"11. Save and load best performing model\u00b6","text":"<p>Let's finish this section off by saving and loading in our best performing model.</p> <p>Recall from notebook 01 we can save and load a PyTorch model using a combination of:</p> <ul> <li><code>torch.save</code> - a function to save a whole PyTorch model or a model's <code>state_dict()</code>.</li> <li><code>torch.load</code> - a function to load in a saved PyTorch object.</li> <li><code>torch.nn.Module.load_state_dict()</code> - a function to load a saved <code>state_dict()</code> into an existing model instance.</li> </ul> <p>You can see more of these three in the PyTorch saving and loading models documentation.</p> <p>For now, let's save our <code>model_2</code>'s <code>state_dict()</code> then load it back in and evaluate it to make sure the save and load went correctly.</p>"},{"location":"03_pytorch_computer_vision/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 03</li> <li>Example solutions notebook for 03 (try the exercises before looking at this)</li> </ul> <ol> <li>What are 3 areas in industry where computer vision is currently being used?</li> <li>Search \"what is overfitting in machine learning\" and write down a sentence about what you find.</li> <li>Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. Note: there are lots of these, so don't worry too much about all of them, just pick 3 and start with those.</li> <li>Spend 20-minutes reading and clicking through the CNN Explainer website.<ul> <li>Upload your own example image using the \"upload\" button and see what happens in each layer of a CNN as your image passes through it.</li> </ul> </li> <li>Load the <code>torchvision.datasets.MNIST()</code> train and test datasets.</li> <li>Visualize at least 5 different samples of the MNIST training dataset.</li> <li>Turn the MNIST train and test datasets into dataloaders using <code>torch.utils.data.DataLoader</code>, set the <code>batch_size=32</code>.</li> <li>Recreate <code>model_2</code> used in this notebook (the same model from the CNN Explainer website, also known as TinyVGG) capable of fitting on the MNIST dataset.</li> <li>Train the model you built in exercise 8. on CPU and GPU and see how long it takes on each.</li> <li>Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label.</li> <li>Plot a confusion matrix comparing your model's predictions to the truth labels.</li> <li>Create a random tensor of shape <code>[1, 3, 64, 64]</code> and pass it through a <code>nn.Conv2d()</code> layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the <code>kernel_size</code> parameter goes up and down?</li> <li>Use a model similar to the trained <code>model_2</code> from this notebook to make predictions on the test <code>torchvision.datasets.FashionMNIST</code> dataset.<ul> <li>Then plot some predictions where the model was wrong alongside what the label of the image should've been.</li> <li>After visualing these predictions do you think it's more of a modelling error or a data error?</li> <li>As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?</li> </ul> </li> </ol>"},{"location":"03_pytorch_computer_vision/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Watch: MIT's Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks.</li> <li>Spend 10-minutes clicking thorugh the different options of the PyTorch vision library, what different modules are available?</li> <li>Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the <code>torchvision.models</code> library? What do you think you could do with these?</li> <li>For a large number of pretrained PyTorch computer vision models as well as many different extensions to PyTorch's computer vision functionalities check out the PyTorch Image Models library <code>timm</code> (Torch Image Models) by Ross Wightman.</li> </ul>"},{"location":"04_pytorch_custom_datasets/","title":"04. PyTorch Custom Datasets","text":"<p>View Source Code | View Slides | Watch Video Walkthrough</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torch import nn\n\n# Note: this notebook requires torch &gt;= 1.10.0\ntorch.__version__\n</pre> import torch from torch import nn  # Note: this notebook requires torch &gt;= 1.10.0 torch.__version__ Out[1]: <pre>'1.12.1+cu113'</pre> <p>And now let's follow best practice and setup device-agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p> In\u00a0[2]: Copied! <pre># Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device-agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[2]: <pre>'cuda'</pre> In\u00a0[3]: Copied! <pre>import requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n</pre> import requests import zipfile from pathlib import Path  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path) <pre>data/pizza_steak_sushi directory exists.\n</pre> In\u00a0[4]: Copied! <pre>import os\ndef walk_through_dir(dir_path):\n\"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os def walk_through_dir(dir_path):   \"\"\"   Walks through dir_path returning its contents.   Args:     dir_path (str or pathlib.Path): target directory      Returns:     A print out of:       number of subdiretories in dir_path       number of images (files) in each subdirectory       name of each subdirectory   \"\"\"   for dirpath, dirnames, filenames in os.walk(dir_path):     print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") In\u00a0[5]: Copied! <pre>walk_through_dir(image_path)\n</pre> walk_through_dir(image_path) <pre>There are 2 directories and 1 images in 'data/pizza_steak_sushi'.\nThere are 3 directories and 0 images in 'data/pizza_steak_sushi/test'.\nThere are 0 directories and 19 images in 'data/pizza_steak_sushi/test/steak'.\nThere are 0 directories and 31 images in 'data/pizza_steak_sushi/test/sushi'.\nThere are 0 directories and 25 images in 'data/pizza_steak_sushi/test/pizza'.\nThere are 3 directories and 0 images in 'data/pizza_steak_sushi/train'.\nThere are 0 directories and 75 images in 'data/pizza_steak_sushi/train/steak'.\nThere are 0 directories and 72 images in 'data/pizza_steak_sushi/train/sushi'.\nThere are 0 directories and 78 images in 'data/pizza_steak_sushi/train/pizza'.\n</pre> <p>Excellent!</p> <p>It looks like we've got about 75 images per training class and 25 images per testing class.</p> <p>That should be enough to get started.</p> <p>Remember, these images are subsets of the original Food101 dataset.</p> <p>You can see how they were created in the data creation notebook.</p> <p>While we're at it, let's setup our training and testing paths.</p> In\u00a0[6]: Copied! <pre># Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n</pre> # Setup train and testing paths train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir Out[6]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> In\u00a0[7]: Copied! <pre>import random\nfrom PIL import Image\n\n# Set seed\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n</pre> import random from PIL import Image  # Set seed random.seed(42) # &lt;- try changing this and see what happens  # 1. Get all image paths (* means \"any combination\") image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # 2. Get random image path random_image_path = random.choice(image_path_list)  # 3. Get image class from path name (the image class is the name of the directory where the image is stored) image_class = random_image_path.parent.stem  # 4. Open image img = Image.open(random_image_path)  # 5. Print metadata print(f\"Random image path: {random_image_path}\") print(f\"Image class: {image_class}\") print(f\"Image height: {img.height}\")  print(f\"Image width: {img.width}\") img <pre>Random image path: data/pizza_steak_sushi/test/pizza/2124579.jpg\nImage class: pizza\nImage height: 384\nImage width: 512\n</pre> Out[7]: <p>We can do the same with <code>matplotlib.pyplot.imshow()</code>, except we have to convert the image to a NumPy array first.</p> In\u00a0[8]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Turn the image into an array img_as_array = np.asarray(img)  # Plot the image with matplotlib plt.figure(figsize=(10, 7)) plt.imshow(img_as_array) plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\") plt.axis(False); In\u00a0[9]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n</pre> import torch from torch.utils.data import DataLoader from torchvision import datasets, transforms In\u00a0[10]: Copied! <pre># Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n</pre> # Write transform for image data_transform = transforms.Compose([     # Resize the images to 64x64     transforms.Resize(size=(64, 64)),     # Flip the images randomly on the horizontal     transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance     # Turn the image into a torch.Tensor     transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0  ]) <p>Now we've got a composition of transforms, let's write a function to try them out on various images.</p> In\u00a0[11]: Copied! <pre>def plot_transformed_images(image_paths, transform, n=3, seed=42):\n\"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # Note: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n</pre> def plot_transformed_images(image_paths, transform, n=3, seed=42):     \"\"\"Plots a series of random images from image_paths.      Will open n image paths from image_paths, transform them     with transform and plot them side by side.      Args:         image_paths (list): List of target image paths.          transform (PyTorch Transforms): Transforms to apply to images.         n (int, optional): Number of images to plot. Defaults to 3.         seed (int, optional): Random seed for the random generator. Defaults to 42.     \"\"\"     random.seed(seed)     random_image_paths = random.sample(image_paths, k=n)     for image_path in random_image_paths:         with Image.open(image_path) as f:             fig, ax = plt.subplots(1, 2)             ax[0].imshow(f)              ax[0].set_title(f\"Original \\nSize: {f.size}\")             ax[0].axis(\"off\")              # Transform and plot image             # Note: permute() will change shape of image to suit matplotlib              # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])             transformed_image = transform(f).permute(1, 2, 0)              ax[1].imshow(transformed_image)              ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")             ax[1].axis(\"off\")              fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)  plot_transformed_images(image_path_list,                          transform=data_transform,                          n=3) <p>Nice!</p> <p>We've now got a way to convert our images to tensors using <code>torchvision.transforms</code>.</p> <p>We also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).</p> <p>Generally, the larger the shape of the image, the more information a model can recover.</p> <p>For example, an image of size <code>[256, 256, 3]</code> will have 16x more pixels than an image of size <code>[64, 64, 3]</code> (<code>(256*256*3)/(64*64*3)=16</code>).</p> <p>However, the tradeoff is that more pixels requires more computations.</p> <p>Exercise: Try commenting out one of the transforms in <code>data_transform</code> and running the plotting function <code>plot_transformed_images()</code> again, what happens?</p> In\u00a0[12]: Copied! <pre># Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n</pre> # Use ImageFolder to create dataset(s) from torchvision import datasets train_data = datasets.ImageFolder(root=train_dir, # target folder of images                                   transform=data_transform, # transforms to perform on data (images)                                   target_transform=None) # transforms to perform on labels (if necessary)  test_data = datasets.ImageFolder(root=test_dir,                                   transform=data_transform)  print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\") <pre>Train data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data/pizza_steak_sushi/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data/pizza_steak_sushi/test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\n</pre> <p>Beautiful!</p> <p>It looks like PyTorch has registered our <code>Dataset</code>'s.</p> <p>Let's inspect them by checking out the <code>classes</code> and <code>class_to_idx</code> attributes as well as the lengths of our training and test sets.</p> In\u00a0[13]: Copied! <pre># Get class names as a list\nclass_names = train_data.classes\nclass_names\n</pre> # Get class names as a list class_names = train_data.classes class_names Out[13]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[14]: Copied! <pre># Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n</pre> # Can also get class names as a dict class_dict = train_data.class_to_idx class_dict Out[14]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> In\u00a0[15]: Copied! <pre># Check the lengths\nlen(train_data), len(test_data)\n</pre> # Check the lengths len(train_data), len(test_data) Out[15]: <pre>(225, 75)</pre> <p>Nice! Looks like we'll be able to use these to reference for later.</p> <p>How about our images and labels?</p> <p>How do they look?</p> <p>We can index on our <code>train_data</code> and <code>test_data</code> <code>Dataset</code>'s to find samples and their target labels.</p> In\u00a0[16]: Copied! <pre>img, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n</pre> img, label = train_data[0][0], train_data[0][1] print(f\"Image tensor:\\n{img}\") print(f\"Image shape: {img.shape}\") print(f\"Image datatype: {img.dtype}\") print(f\"Image label: {label}\") print(f\"Label datatype: {type(label)}\") <pre>Image tensor:\ntensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],\n         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],\n         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],\n         ...,\n         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],\n\n        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],\n         [0.0706, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],\n         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],\n         ...,\n         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],\n         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],\n         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],\n\n        [[0.0941, 0.0902, 0.0902,  ..., 0.0196, 0.0196, 0.0196],\n         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],\n         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],\n         ...,\n         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1804],\n         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],\n         [0.1059, 0.1020, 0.1059,  ..., 0.1843, 0.1804, 0.1765]]])\nImage shape: torch.Size([3, 64, 64])\nImage datatype: torch.float32\nImage label: 0\nLabel datatype: &lt;class 'int'&gt;\n</pre> <p>Our images are now in the form of a tensor (with shape <code>[3, 64, 64]</code>) and the labels are in the form of an integer relating to a specific class (as referenced by the <code>class_to_idx</code> attribute).</p> <p>How about we plot a single image tensor using <code>matplotlib</code>?</p> <p>We'll first have to to permute (rearrange the order of its dimensions) so it's compatible.</p> <p>Right now our image dimensions are in the format <code>CHW</code> (color channels, height, width) but <code>matplotlib</code> prefers <code>HWC</code> (height, width, color channels).</p> In\u00a0[17]: Copied! <pre># Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n</pre> # Rearrange the order of dimensions img_permute = img.permute(1, 2, 0)  # Print out different shapes (before and after permute) print(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\") print(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")  # Plot the image plt.figure(figsize=(10, 7)) plt.imshow(img.permute(1, 2, 0)) plt.axis(\"off\") plt.title(class_names[label], fontsize=14); <pre>Original shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nImage permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]\n</pre> <p>Notice the image is now more pixelated (less quality).</p> <p>This is due to it being resized from <code>512x512</code> to <code>64x64</code> pixels.</p> <p>The intuition here is that if you think the image is harder to recognize what's going on, chances are a model will find it harder to understand too.</p> In\u00a0[18]: Copied! <pre># Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n</pre> # Turn train and test Datasets into DataLoaders from torch.utils.data import DataLoader train_dataloader = DataLoader(dataset=train_data,                                batch_size=1, # how many samples per batch?                               num_workers=1, # how many subprocesses to use for data loading? (higher = more)                               shuffle=True) # shuffle the data?  test_dataloader = DataLoader(dataset=test_data,                               batch_size=1,                               num_workers=1,                               shuffle=False) # don't usually need to shuffle testing data  train_dataloader, test_dataloader Out[18]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9dca0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9de50&gt;)</pre> <p>Wonderful!</p> <p>Now our data is iterable.</p> <p>Let's try it out and check the shapes.</p> In\u00a0[19]: Copied! <pre>img, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n</pre> img, label = next(iter(train_dataloader))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>We could now use these <code>DataLoader</code>'s with a training and testing loop to train a model.</p> <p>But before we do, let's look at another option to load images (or almost any other kind of data).</p> In\u00a0[20]: Copied! <pre>import os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n</pre> import os import pathlib import torch  from PIL import Image from torch.utils.data import Dataset from torchvision import transforms from typing import Tuple, Dict, List <p>Remember how our instances of <code>torchvision.datasets.ImageFolder()</code> allowed us to use the <code>classes</code> and <code>class_to_idx</code> attributes?</p> In\u00a0[21]: Copied! <pre># Instance of torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n</pre> # Instance of torchvision.datasets.ImageFolder() train_data.classes, train_data.class_to_idx Out[21]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> In\u00a0[22]: Copied! <pre># Setup path for target directory\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Get the class names from the target directory\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n</pre> # Setup path for target directory target_directory = train_dir print(f\"Target directory: {target_directory}\")  # Get the class names from the target directory class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))]) print(f\"Class names found: {class_names_found}\") <pre>Target directory: data/pizza_steak_sushi/train\nClass names found: ['pizza', 'steak', 'sushi']\n</pre> <p>Excellent!</p> <p>How about we turn it into a full function?</p> In\u00a0[23]: Copied! <pre># Make function to find classes in target directory\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n\"\"\"Finds the class folder names in a target directory.\n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n</pre> # Make function to find classes in target directory def find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:     \"\"\"Finds the class folder names in a target directory.          Assumes target directory is in standard image classification format.      Args:         directory (str): target directory to load classnames from.      Returns:         Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))          Example:         find_classes(\"food_images/train\")         &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})     \"\"\"     # 1. Get the class names by scanning the target directory     classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())          # 2. Raise an error if class names not found     if not classes:         raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")              # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)     class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}     return classes, class_to_idx <p>Looking good!</p> <p>Now let's test out our <code>find_classes()</code> function.</p> In\u00a0[24]: Copied! <pre>find_classes(train_dir)\n</pre> find_classes(train_dir) Out[24]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> <p>Woohoo! Looking good!</p> In\u00a0[25]: Copied! <pre># Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n</pre> # Write a custom dataset class (inherits from torch.utils.data.Dataset) from torch.utils.data import Dataset  # 1. Subclass torch.utils.data.Dataset class ImageFolderCustom(Dataset):          # 2. Initialize with a targ_dir and transform (optional) parameter     def __init__(self, targ_dir: str, transform=None) -&gt; None:                  # 3. Create class attributes         # Get all image paths         self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's         # Setup transforms         self.transform = transform         # Create classes and class_to_idx attributes         self.classes, self.class_to_idx = find_classes(targ_dir)      # 4. Make function to load images     def load_image(self, index: int) -&gt; Image.Image:         \"Opens an image via a path and returns it.\"         image_path = self.paths[index]         return Image.open(image_path)           # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)     def __len__(self) -&gt; int:         \"Returns the total number of samples.\"         return len(self.paths)          # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)     def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:         \"Returns one sample of data, data and label (X, y).\"         img = self.load_image(index)         class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg         class_idx = self.class_to_idx[class_name]          # Transform if necessary         if self.transform:             return self.transform(img), class_idx # return data, label (X, y)         else:             return img, class_idx # return data, label (X, y) <p>Woah! A whole bunch of code to load in our images.</p> <p>This is one of the downsides of creating your own custom <code>Dataset</code>'s.</p> <p>However, now we've written it once, we could move it into a <code>.py</code> file such as <code>data_loader.py</code> along with some other helpful data functions and reuse it later on.</p> <p>Before we test out our new <code>ImageFolderCustom</code> class, let's create some transforms to prepare our images.</p> In\u00a0[26]: Copied! <pre># Augment train data\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Augment train data train_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.RandomHorizontalFlip(p=0.5),     transforms.ToTensor() ])  # Don't augment test data, only reshape test_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>Now comes the moment of truth!</p> <p>Let's turn our training images (contained in <code>train_dir</code>) and our testing images (contained in <code>test_dir</code>) into <code>Dataset</code>'s using our own <code>ImageFolderCustom</code> class.</p> In\u00a0[27]: Copied! <pre>train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n</pre> train_data_custom = ImageFolderCustom(targ_dir=train_dir,                                        transform=train_transforms) test_data_custom = ImageFolderCustom(targ_dir=test_dir,                                       transform=test_transforms) train_data_custom, test_data_custom Out[27]: <pre>(&lt;__main__.ImageFolderCustom at 0x7f5461f70c70&gt;,\n &lt;__main__.ImageFolderCustom at 0x7f5461f70c40&gt;)</pre> <p>Hmm... no errors, did it work?</p> <p>Let's try calling <code>len()</code> on our new <code>Dataset</code>'s and find the <code>classes</code> and <code>class_to_idx</code> attributes.</p> In\u00a0[28]: Copied! <pre>len(train_data_custom), len(test_data_custom)\n</pre> len(train_data_custom), len(test_data_custom) Out[28]: <pre>(225, 75)</pre> In\u00a0[29]: Copied! <pre>train_data_custom.classes\n</pre> train_data_custom.classes Out[29]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[30]: Copied! <pre>train_data_custom.class_to_idx\n</pre> train_data_custom.class_to_idx Out[30]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> <p><code>len(test_data_custom) == len(test_data)</code> and <code>len(test_data_custom) == len(test_data)</code> Yes!!!</p> <p>It looks like it worked.</p> <p>We could check for equality with the <code>Dataset</code>'s made by the <code>torchvision.datasets.ImageFolder()</code> class too.</p> In\u00a0[31]: Copied! <pre># Check for equality amongst our custom Dataset and ImageFolder Dataset\nprint((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n</pre> # Check for equality amongst our custom Dataset and ImageFolder Dataset print((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data))) print(train_data_custom.classes == train_data.classes) print(train_data_custom.class_to_idx == train_data.class_to_idx) <pre>True\nTrue\nTrue\n</pre> <p>Ho ho!</p> <p>Look at us go!</p> <p>Three <code>True</code>'s!</p> <p>You can't get much better than that.</p> <p>How about we take it up a notch and plot some random images to test our <code>__getitem__</code> override?</p> In\u00a0[32]: Copied! <pre># 1. Take in a Dataset as well as a list of class names\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n</pre> # 1. Take in a Dataset as well as a list of class names def display_random_images(dataset: torch.utils.data.dataset.Dataset,                           classes: List[str] = None,                           n: int = 10,                           display_shape: bool = True,                           seed: int = None):          # 2. Adjust display if n too high     if n &gt; 10:         n = 10         display_shape = False         print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")          # 3. Set random seed     if seed:         random.seed(seed)      # 4. Get random sample indexes     random_samples_idx = random.sample(range(len(dataset)), k=n)      # 5. Setup plot     plt.figure(figsize=(16, 8))      # 6. Loop through samples and display random samples      for i, targ_sample in enumerate(random_samples_idx):         targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]          # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]         targ_image_adjust = targ_image.permute(1, 2, 0)          # Plot adjusted samples         plt.subplot(1, n, i+1)         plt.imshow(targ_image_adjust)         plt.axis(\"off\")         if classes:             title = f\"class: {classes[targ_label]}\"             if display_shape:                 title = title + f\"\\nshape: {targ_image_adjust.shape}\"         plt.title(title) <p>What a good looking function!</p> <p>Let's test it out first with the <code>Dataset</code> we created with <code>torchvision.datasets.ImageFolder()</code>.</p> In\u00a0[33]: Copied! <pre># Display random images from ImageFolder created Dataset\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n</pre> # Display random images from ImageFolder created Dataset display_random_images(train_data,                        n=5,                        classes=class_names,                       seed=None) <p>And now with the <code>Dataset</code> we created with our own <code>ImageFolderCustom</code>.</p> In\u00a0[34]: Copied! <pre># Display random images from ImageFolderCustom Dataset\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n</pre> # Display random images from ImageFolderCustom Dataset display_random_images(train_data_custom,                        n=12,                        classes=class_names,                       seed=None) # Try setting the seed for reproducible images <pre>For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\n</pre> <p>Nice!!!</p> <p>Looks like our <code>ImageFolderCustom</code> is working just as we'd like it to.</p> In\u00a0[35]: Copied! <pre># Turn train and test custom Dataset's into DataLoader's\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n</pre> # Turn train and test custom Dataset's into DataLoader's from torch.utils.data import DataLoader train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset                                      batch_size=1, # how many samples per batch?                                      num_workers=0, # how many subprocesses to use for data loading? (higher = more)                                      shuffle=True) # shuffle the data?  test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset                                     batch_size=1,                                      num_workers=0,                                      shuffle=False) # don't usually need to shuffle testing data  train_dataloader_custom, test_dataloader_custom Out[35]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ab8400&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ab8490&gt;)</pre> <p>Do the shapes of the samples look the same?</p> In\u00a0[36]: Copied! <pre># Get image and label from custom DataLoader\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n</pre> # Get image and label from custom DataLoader img_custom, label_custom = next(iter(train_dataloader_custom))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label_custom.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>They sure do!</p> <p>Let's now take a lot at some other forms of data transforms.</p> In\u00a0[37]: Copied! <pre>from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1\n])\n\n# Don't need to perform augmentation on the test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n</pre> from torchvision import transforms  train_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense      transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1 ])  # Don't need to perform augmentation on the test data test_transforms = transforms.Compose([     transforms.Resize((224, 224)),      transforms.ToTensor() ]) <p>Note: You usually don't perform data augmentation on the test set. The idea of data augmentation is to to artificially increase the diversity of the training set to better predict on the testing set.</p> <p>However, you do need to make sure your test set images are transformed to tensors. We size the test images to the same size as our training images too, however, inference can be done on different size images if necessary (though this may alter performance).</p> <p>Beautiful, now we've got a training transform (with data augmentation) and test transform (without data augmentation).</p> <p>Let's test our data augmentation out!</p> In\u00a0[38]: Copied! <pre># Get all image paths\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Plot random images\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n</pre> # Get all image paths image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # Plot random images plot_transformed_images(     image_paths=image_path_list,     transform=train_transforms,     n=3,     seed=None ) <p>Try running the cell above a few times and seeing how the original image changes as it goes through the transform.</p> In\u00a0[39]: Copied! <pre># Create simple transform\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n</pre> # Create simple transform simple_transform = transforms.Compose([      transforms.Resize((64, 64)),     transforms.ToTensor(), ]) <p>Excellent, now we've got a simple transform, let's:</p> <ol> <li>Load the data, turning each of our training and test folders first into a <code>Dataset</code> with <code>torchvision.datasets.ImageFolder()</code></li> <li>Then into a <code>DataLoader</code> using <code>torch.utils.data.DataLoader()</code>.<ul> <li>We'll set the <code>batch_size=32</code> and <code>num_workers</code> to as many CPUs on our machine (this will depend on what machine you're using).</li> </ul> </li> </ol> In\u00a0[40]: Copied! <pre># 1. Load and transform data\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Turn data into DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Setup batch size and number of workers \nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Create DataLoader's\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n</pre> # 1. Load and transform data from torchvision import datasets train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform) test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)  # 2. Turn data into DataLoaders import os from torch.utils.data import DataLoader  # Setup batch size and number of workers  BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count() print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")  # Create DataLoader's train_dataloader_simple = DataLoader(train_data_simple,                                       batch_size=BATCH_SIZE,                                       shuffle=True,                                       num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_simple, test_dataloader_simple <pre>Creating DataLoader's with batch size 32 and 16 workers.\n</pre> Out[40]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ad2f70&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ad23d0&gt;)</pre> <p><code>DataLoader</code>'s created!</p> <p>Let's build a model.</p> In\u00a0[41]: Copied! <pre>class TinyVGG(nn.Module):\n\"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n</pre> class TinyVGG(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:         super().__init__()         self.conv_block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.conv_block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*16*16,                       out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.conv_block_1(x)         # print(x.shape)         x = self.conv_block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x         # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion  torch.manual_seed(42) model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device) model_0 Out[41]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>Note: One of the ways to speed up deep learning models computing on a GPU is to leverage operator fusion.</p> <p>This means in the <code>forward()</code> method in our model above, instead of calling a layer block and reassigning <code>x</code> every time, we call each block in succession (see the final line of the <code>forward()</code> method in the model above for an example).</p> <p>This saves the time spent reassigning <code>x</code> (memory heavy) and focuses on only computing on <code>x</code>.</p> <p>See Making Deep Learning Go Brrrr From First Principles by Horace He for more ways on how to speed up machine learning models.</p> <p>Now that's a nice looking model!</p> <p>How about we test it out with a forward pass on a single image?</p> In\u00a0[42]: Copied! <pre># 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n</pre> # 1. Get a batch of images and labels from the DataLoader img_batch, label_batch = next(iter(train_dataloader_simple))  # 2. Get a single image from the batch and unsqueeze the image so its shape fits the model img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0] print(f\"Single image shape: {img_single.shape}\\n\")  # 3. Perform a forward pass on a single image model_0.eval() with torch.inference_mode():     pred = model_0(img_single.to(device))      # 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label print(f\"Output logits:\\n{pred}\\n\") print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\") print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\") print(f\"Actual label:\\n{label_single}\") <pre>Single image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[0.0578, 0.0634, 0.0352]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3352, 0.3371, 0.3277]], device='cuda:0')\n\nOutput prediction label:\ntensor([1], device='cuda:0')\n\nActual label:\n2\n</pre> <p>Wonderful, it looks like our model is outputting what we'd expect it to output.</p> <p>You can run the cell above a few times and each time have a different image be predicted on.</p> <p>And you'll probably notice the predictions are often wrong.</p> <p>This is to be expected because the model hasn't been trained yet and it's essentially guessing using random weights.</p> In\u00a0[43]: Copied! <pre># Install torchinfo if it's not available, import it if it is\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size\n</pre> # Install torchinfo if it's not available, import it if it is try:      import torchinfo except:     !pip install torchinfo     import torchinfo      from torchinfo import summary summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size  Out[43]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  [1, 3]                    --\n\u251c\u2500Sequential: 1-1                        [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-1                       [1, 10, 64, 64]           280\n\u2502    \u2514\u2500ReLU: 2-2                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500Conv2d: 2-3                       [1, 10, 64, 64]           910\n\u2502    \u2514\u2500ReLU: 2-4                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n\u251c\u2500Sequential: 1-2                        [1, 10, 16, 16]           --\n\u2502    \u2514\u2500Conv2d: 2-6                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-7                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-8                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-9                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n\u251c\u2500Sequential: 1-3                        [1, 3]                    --\n\u2502    \u2514\u2500Flatten: 2-11                     [1, 2560]                 --\n\u2502    \u2514\u2500Linear: 2-12                      [1, 3]                    7,683\n==========================================================================================\nTotal params: 10,693\nTrainable params: 10,693\nNon-trainable params: 0\nTotal mult-adds (M): 6.75\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.91\n==========================================================================================</pre> <p>Nice!</p> <p>The output of <code>torchinfo.summary()</code> gives us a whole bunch of information about our model.</p> <p>Such as <code>Total params</code>, the total number of parameters in our model, the <code>Estimated Total Size (MB)</code> which is the size of our model.</p> <p>You can also see the change in input and output shapes as data of a certain <code>input_size</code> moves through our model.</p> <p>Right now, our parameter numbers and total model size is low.</p> <p>This because we're starting with a small model.</p> <p>And if we need to increase its size later, we can.</p> In\u00a0[44]: Copied! <pre>def train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n</pre> def train_step(model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer):     # Put model in train mode     model.train()          # Setup train loss and train accuracy values     train_loss, train_acc = 0, 0          # Loop through data loader data batches     for batch, (X, y) in enumerate(dataloader):         # Send data to target device         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate  and accumulate loss         loss = loss_fn(y_pred, y)         train_loss += loss.item()           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Calculate and accumulate accuracy metric across all batches         y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)         train_acc += (y_pred_class == y).sum().item()/len(y_pred)      # Adjust metrics to get average loss and accuracy per batch      train_loss = train_loss / len(dataloader)     train_acc = train_acc / len(dataloader)     return train_loss, train_acc <p>Woohoo! <code>train_step()</code> function done.</p> <p>Now let's do the same for the <code>test_step()</code> function.</p> <p>The main difference here will be the <code>test_step()</code> won't take in an optimizer and therefore won't perform gradient descent.</p> <p>But since we'll be doing inference, we'll make sure to turn on the <code>torch.inference_mode()</code> context manager for making predictions.</p> In\u00a0[45]: Copied! <pre>def test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n</pre> def test_step(model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module):     # Put model in eval mode     model.eval()           # Setup test loss and test accuracy values     test_loss, test_acc = 0, 0          # Turn on inference context manager     with torch.inference_mode():         # Loop through DataLoader batches         for batch, (X, y) in enumerate(dataloader):             # Send data to target device             X, y = X.to(device), y.to(device)                  # 1. Forward pass             test_pred_logits = model(X)              # 2. Calculate and accumulate loss             loss = loss_fn(test_pred_logits, y)             test_loss += loss.item()                          # Calculate and accumulate accuracy             test_pred_labels = test_pred_logits.argmax(dim=1)             test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))                  # Adjust metrics to get average loss and accuracy per batch      test_loss = test_loss / len(dataloader)     test_acc = test_acc / len(dataloader)     return test_loss, test_acc <p>Excellent!</p> In\u00a0[46]: Copied! <pre>from tqdm.auto import tqdm\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n</pre> from tqdm.auto import tqdm  # 1. Take in various parameters required for training and test steps def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),           epochs: int = 5):          # 2. Create empty results dictionary     results = {\"train_loss\": [],         \"train_acc\": [],         \"test_loss\": [],         \"test_acc\": []     }          # 3. Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer)         test_loss, test_acc = test_step(model=model,             dataloader=test_dataloader,             loss_fn=loss_fn)                  # 4. Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # 5. Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)      # 6. Return the filled results at the end of the epochs     return results In\u00a0[47]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Recreate an instance of TinyVGG model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_0  model_0_results = train(model=model_0,                          train_dataloader=train_dataloader_simple,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1360 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0847 | train_acc: 0.4258 | test_loss: 1.1620 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.1157 | train_acc: 0.2930 | test_loss: 1.1697 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.0956 | train_acc: 0.4141 | test_loss: 1.1384 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.0985 | train_acc: 0.2930 | test_loss: 1.1426 | test_acc: 0.1979\nTotal training time: 4.935 seconds\n</pre> <p>Hmm...</p> <p>It looks like our model performed pretty poorly.</p> <p>But that's okay for now, we'll keep persevering.</p> <p>What are some ways you could potentially improve it?</p> <p>Note: Check out the Improving a model (from a model perspective) section in notebook 02 for ideas on improving our TinyVGG model.</p> In\u00a0[48]: Copied! <pre># Check the model_0_results keys\nmodel_0_results.keys()\n</pre> # Check the model_0_results keys model_0_results.keys() Out[48]: <pre>dict_keys(['train_loss', 'train_acc', 'test_loss', 'test_acc'])</pre> <p>We'll need to extract each of these keys and turn them into a plot.</p> In\u00a0[49]: Copied! <pre>def plot_loss_curves(results: Dict[str, List[float]]):\n\"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n</pre> def plot_loss_curves(results: Dict[str, List[float]]):     \"\"\"Plots training curves of a results dictionary.      Args:         results (dict): dictionary containing list of values, e.g.             {\"train_loss\": [...],              \"train_acc\": [...],              \"test_loss\": [...],              \"test_acc\": [...]}     \"\"\"          # Get the loss values of the results dictionary (training and test)     loss = results['train_loss']     test_loss = results['test_loss']      # Get the accuracy values of the results dictionary (training and test)     accuracy = results['train_acc']     test_accuracy = results['test_acc']      # Figure out how many epochs there were     epochs = range(len(results['train_loss']))      # Setup a plot      plt.figure(figsize=(15, 7))      # Plot loss     plt.subplot(1, 2, 1)     plt.plot(epochs, loss, label='train_loss')     plt.plot(epochs, test_loss, label='test_loss')     plt.title('Loss')     plt.xlabel('Epochs')     plt.legend()      # Plot accuracy     plt.subplot(1, 2, 2)     plt.plot(epochs, accuracy, label='train_accuracy')     plt.plot(epochs, test_accuracy, label='test_accuracy')     plt.title('Accuracy')     plt.xlabel('Epochs')     plt.legend(); <p>Okay, let's test our <code>plot_loss_curves()</code> function out.</p> In\u00a0[50]: Copied! <pre>plot_loss_curves(model_0_results)\n</pre> plot_loss_curves(model_0_results) <p>Woah.</p> <p>Looks like things are all over the place...</p> <p>But we kind of knew that because our model's print out results during training didn't show much promise.</p> <p>You could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon.</p> In\u00a0[51]: Copied! <pre># Create training transform with TrivialAugment\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# Create testing transform (no data augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Create training transform with TrivialAugment train_transform_trivial_augment = transforms.Compose([     transforms.Resize((64, 64)),     transforms.TrivialAugmentWide(num_magnitude_bins=31),     transforms.ToTensor()  ])  # Create testing transform (no data augmentation) test_transform = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>Wonderful!</p> <p>Now let's turn our images into <code>Dataset</code>'s using <code>torchvision.datasets.ImageFolder()</code> and then into <code>DataLoader</code>'s with <code>torch.utils.data.DataLoader()</code>.</p> In\u00a0[52]: Copied! <pre># Turn image folders into Datasets\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n</pre> # Turn image folders into Datasets train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment) test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)  train_data_augmented, test_data_simple Out[52]: <pre>(Dataset ImageFolder\n     Number of datapoints: 225\n     Root location: data/pizza_steak_sushi/train\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n                ToTensor()\n            ),\n Dataset ImageFolder\n     Number of datapoints: 75\n     Root location: data/pizza_steak_sushi/test\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                ToTensor()\n            ))</pre> <p>And we'll make <code>DataLoader</code>'s with a <code>batch_size=32</code> and with <code>num_workers</code> set to the number of CPUs available on our machine (we can get this using Python's <code>os.cpu_count()</code>).</p> In\u00a0[53]: Copied! <pre># Turn Datasets into DataLoader's\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n</pre> # Turn Datasets into DataLoader's import os BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count()  torch.manual_seed(42) train_dataloader_augmented = DataLoader(train_data_augmented,                                          batch_size=BATCH_SIZE,                                          shuffle=True,                                         num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_augmented, test_dataloader Out[53]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f53c6d64040&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9de50&gt;)</pre> In\u00a0[54]: Copied! <pre># Create model_1 and send it to the target device\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n</pre> # Create model_1 and send it to the target device torch.manual_seed(42) model_1 = TinyVGG(     input_shape=3,     hidden_units=10,     output_shape=len(train_data_augmented.classes)).to(device) model_1 Out[54]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>Model ready!</p> <p>Time to train!</p> <p>Since we've already got functions for the training loop (<code>train_step()</code>) and testing loop (<code>test_step()</code>) and a function to put them together in <code>train()</code>, let's reuse those.</p> <p>We'll use the same setup as <code>model_0</code> with only the <code>train_dataloader</code> parameter varying:</p> <ul> <li>Train for 5 epochs.</li> <li>Use <code>train_dataloader=train_dataloader_augmented</code> as the training data in <code>train()</code>.</li> <li>Use <code>torch.nn.CrossEntropyLoss()</code> as the loss function (since we're working with multi-class classification).</li> <li>Use <code>torch.optim.Adam()</code> with <code>lr=0.001</code> as the learning rate as the optimizer.</li> </ul> In\u00a0[55]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_1 model_1_results = train(model=model_1,                          train_dataloader=train_dataloader_augmented,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1074 | train_acc: 0.2500 | test_loss: 1.1058 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0791 | train_acc: 0.4258 | test_loss: 1.1382 | test_acc: 0.2604\nEpoch: 3 | train_loss: 1.0803 | train_acc: 0.4258 | test_loss: 1.1685 | test_acc: 0.2604\nEpoch: 4 | train_loss: 1.1285 | train_acc: 0.3047 | test_loss: 1.1623 | test_acc: 0.2604\nEpoch: 5 | train_loss: 1.0880 | train_acc: 0.4258 | test_loss: 1.1472 | test_acc: 0.2604\nTotal training time: 4.924 seconds\n</pre> <p>Hmm...</p> <p>It doesn't look like our model performed very well again.</p> <p>Let's check out its loss curves.</p> In\u00a0[56]: Copied! <pre>plot_loss_curves(model_1_results)\n</pre> plot_loss_curves(model_1_results) <p>Wow...</p> <p>These don't look very good either...</p> <p>Is our model underfitting or overfitting?</p> <p>Or both?</p> <p>Ideally we'd like it have higher accuracy and lower loss right?</p> <p>What are some methods you could try to use to achieve these?</p> In\u00a0[57]: Copied! <pre>import pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n</pre> import pandas as pd model_0_df = pd.DataFrame(model_0_results) model_1_df = pd.DataFrame(model_1_results) model_0_df Out[57]: train_loss train_acc test_loss test_acc 0 1.107833 0.257812 1.136041 0.260417 1 1.084713 0.425781 1.162014 0.197917 2 1.115697 0.292969 1.169704 0.197917 3 1.095564 0.414062 1.138373 0.197917 4 1.098520 0.292969 1.142631 0.197917 <p>And now we can write some plotting code using <code>matplotlib</code> to visualize the results of <code>model_0</code> and <code>model_1</code> together.</p> In\u00a0[58]: Copied! <pre># Setup a plot \nplt.figure(figsize=(15, 10))\n\n# Get number of epochs\nepochs = range(len(model_0_df))\n\n# Plot train loss\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test loss\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot train accuracy\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test accuracy\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Setup a plot  plt.figure(figsize=(15, 10))  # Get number of epochs epochs = range(len(model_0_df))  # Plot train loss plt.subplot(2, 2, 1) plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\") plt.title(\"Train Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test loss plt.subplot(2, 2, 2) plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\") plt.title(\"Test Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot train accuracy plt.subplot(2, 2, 3) plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\") plt.title(\"Train Accuracy\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test accuracy plt.subplot(2, 2, 4) plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\") plt.title(\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.legend(); <p>It looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).</p> <p>If you built <code>model_2</code>, what would you do differently to try and improve performance?</p> In\u00a0[59]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\") <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> In\u00a0[60]: Copied! <pre>import torchvision\n\n# Read in custom image\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n</pre> import torchvision  # Read in custom image custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))  # Print out image data print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\") print(f\"Custom image shape: {custom_image_uint8.shape}\\n\") print(f\"Custom image dtype: {custom_image_uint8.dtype}\") <pre>Custom image tensor:\ntensor([[[154, 173, 181,  ...,  21,  18,  14],\n         [146, 165, 181,  ...,  21,  18,  15],\n         [124, 146, 172,  ...,  18,  17,  15],\n         ...,\n         [ 72,  59,  45,  ..., 152, 150, 148],\n         [ 64,  55,  41,  ..., 150, 147, 144],\n         [ 64,  60,  46,  ..., 149, 146, 143]],\n\n        [[171, 190, 193,  ...,  22,  19,  15],\n         [163, 182, 193,  ...,  22,  19,  16],\n         [141, 163, 184,  ...,  19,  18,  16],\n         ...,\n         [ 55,  42,  28,  ..., 107, 104, 103],\n         [ 47,  38,  24,  ..., 108, 104, 102],\n         [ 47,  43,  29,  ..., 107, 104, 101]],\n\n        [[119, 138, 147,  ...,  17,  14,  10],\n         [111, 130, 145,  ...,  17,  14,  11],\n         [ 87, 111, 136,  ...,  14,  13,  11],\n         ...,\n         [ 35,  22,   8,  ...,  52,  52,  48],\n         [ 27,  18,   4,  ...,  50,  49,  44],\n         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.uint8\n</pre> <p>Nice! Looks like our image is in tensor format, however, is this image format compatible with our model?</p> <p>Our <code>custom_image</code> tensor is of datatype <code>torch.uint8</code> and its values are between <code>[0, 255]</code>.</p> <p>But our model takes image tensors of datatype <code>torch.float32</code> and with values between <code>[0, 1]</code>.</p> <p>So before we use our custom image with our model, we'll need to convert it to the same format as the data our model is trained on.</p> <p>If we don't do this, our model will error.</p> In\u00a0[61]: Copied! <pre># Try to make a prediction on image in uint8 format (this will error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n</pre> # Try to make a prediction on image in uint8 format (this will error) model_1.eval() with torch.inference_mode():     model_1(custom_image_uint8.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [61], in &lt;cell line: 3&gt;()\n      2 model_1.eval()\n      3 with torch.inference_mode():\n----&gt; 4     model_1(custom_image_uint8.to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [41], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:457, in Conv2d.forward(self, input)\n    456 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 457     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:453, in Conv2d._conv_forward(self, input, weight, bias)\n    449 if self.padding_mode != 'zeros':\n    450     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    451                     weight, bias, self.stride,\n    452                     _pair(0), self.dilation, self.groups)\n--&gt; 453 return F.conv2d(input, weight, bias, self.stride,\n    454 self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</pre> <p>If we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:</p> <p><code>RuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</code></p> <p>Let's fix this by converting our custom image to the same datatype as what our model was trained on (<code>torch.float32</code>).</p> In\u00a0[62]: Copied! <pre># Load in custom image and convert the tensor values to float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divide the image pixel values by 255 to get them between [0, 1]\ncustom_image = custom_image / 255. \n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n</pre> # Load in custom image and convert the tensor values to float32 custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)  # Divide the image pixel values by 255 to get them between [0, 1] custom_image = custom_image / 255.   # Print out image data print(f\"Custom image tensor:\\n{custom_image}\\n\") print(f\"Custom image shape: {custom_image.shape}\\n\") print(f\"Custom image dtype: {custom_image.dtype}\") <pre>Custom image tensor:\ntensor([[[0.6039, 0.6784, 0.7098,  ..., 0.0824, 0.0706, 0.0549],\n         [0.5725, 0.6471, 0.7098,  ..., 0.0824, 0.0706, 0.0588],\n         [0.4863, 0.5725, 0.6745,  ..., 0.0706, 0.0667, 0.0588],\n         ...,\n         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],\n         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],\n         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],\n\n        [[0.6706, 0.7451, 0.7569,  ..., 0.0863, 0.0745, 0.0588],\n         [0.6392, 0.7137, 0.7569,  ..., 0.0863, 0.0745, 0.0627],\n         [0.5529, 0.6392, 0.7216,  ..., 0.0745, 0.0706, 0.0627],\n         ...,\n         [0.2157, 0.1647, 0.1098,  ..., 0.4196, 0.4078, 0.4039],\n         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4078, 0.4000],\n         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],\n\n        [[0.4667, 0.5412, 0.5765,  ..., 0.0667, 0.0549, 0.0392],\n         [0.4353, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],\n         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],\n         ...,\n         [0.1373, 0.0863, 0.0314,  ..., 0.2039, 0.2039, 0.1882],\n         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1922, 0.1725],\n         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.float32\n</pre> In\u00a0[63]: Copied! <pre># Plot custom image\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n</pre> # Plot custom image plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error plt.title(f\"Image shape: {custom_image.shape}\") plt.axis(False); <p>Two thumbs up!</p> <p>Now how could we get our image to be the same size as the images our model was trained on?</p> <p>One way to do so is with <code>torchvision.transforms.Resize()</code>.</p> <p>Let's compose a transform pipeline to do so.</p> In\u00a0[64]: Copied! <pre># Create transform pipleine to resize image\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transform target image\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Print out original shape and new shape\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n</pre> # Create transform pipleine to resize image custom_image_transform = transforms.Compose([     transforms.Resize((64, 64)), ])  # Transform target image custom_image_transformed = custom_image_transform(custom_image)  # Print out original shape and new shape print(f\"Original shape: {custom_image.shape}\") print(f\"New shape: {custom_image_transformed.shape}\") <pre>Original shape: torch.Size([3, 4032, 3024])\nNew shape: torch.Size([3, 64, 64])\n</pre> <p>Woohoo!</p> <p>Let's finally make a prediction on our own custom image.</p> In\u00a0[65]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [65], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [41], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:457, in Conv2d.forward(self, input)\n    456 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 457     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:453, in Conv2d._conv_forward(self, input, weight, bias)\n    449 if self.padding_mode != 'zeros':\n    450     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    451                     weight, bias, self.stride,\n    452                     _pair(0), self.dilation, self.groups)\n--&gt; 453 return F.conv2d(input, weight, bias, self.stride,\n    454 self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)</pre> <p>Oh my goodness...</p> <p>Despite our preparations our custom image and model are on different devices.</p> <p>And we get the error:</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)</code></p> <p>Let's fix that by putting our <code>custom_image_transformed</code> on the target device.</p> In\u00a0[66]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [66], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed.to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [41], in TinyVGG.forward(self, x)\n     42 x = self.conv_block_2(x)\n     43 # print(x.shape)\n---&gt; 44 x = self.classifier(x)\n     45 # print(x.shape)\n     46 return x\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)</pre> <p>What now?</p> <p>It looks like we're getting a shape error.</p> <p>Why might this be?</p> <p>We converted our custom image to be the same size as the images our model was trained on...</p> <p>Oh wait...</p> <p>There's one dimension we forgot about.</p> <p>The batch size.</p> <p>Our model expects image tensors with a batch size dimension at the start (<code>NCHW</code> where <code>N</code> is the batch size).</p> <p>Except our custom image is currently only <code>CHW</code>.</p> <p>We can add a batch size dimension using <code>torch.unsqueeze(dim=0)</code> to add an extra dimension our image and finally make a prediction.</p> <p>Essentially we'll be telling our model to predict on a single image (an image with a <code>batch_size</code> of 1).</p> In\u00a0[67]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n</pre> model_1.eval() with torch.inference_mode():     # Add an extra dimension to image     custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)          # Print out different shapes     print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")     print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")          # Make a prediction on image with an extra dimension     custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device)) <pre>Custom image transformed shape: torch.Size([3, 64, 64])\nUnsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n</pre> <p>Yes!!!</p> <p>It looks like it worked!</p> <p>Note: What we've just gone through are three of the classical and most common deep learning and PyTorch issues:</p> <ol> <li>Wrong datatypes - our model expects <code>torch.float32</code> where our original custom image was <code>uint8</code>.</li> <li>Wrong device - our model was on the target <code>device</code> (in our case, the GPU) whereas our target data hadn't been moved to the target <code>device</code> yet.</li> <li>Wrong shapes - our model expected an input image of shape <code>[N, C, H, W]</code> or <code>[batch_size, color_channels, height, width]</code> whereas our custom image tensor was of shape <code>[color_channels, height, width]</code>.</li> </ol> <p>Keep in mind, these errors aren't just for predicting on custom images.</p> <p>They will be present with almost every kind of data type (text, audio, structured data) and problem you work with.</p> <p>Now let's take a look at our model's predictions.</p> In\u00a0[68]: Copied! <pre>custom_image_pred\n</pre> custom_image_pred Out[68]: <pre>tensor([[ 0.1172,  0.0160, -0.1425]], device='cuda:0')</pre> <p>Alright, these are still in logit form (the raw outputs of a model are called logits).</p> <p>Let's convert them from logits -&gt; prediction probabilities -&gt; prediction labels.</p> In\u00a0[69]: Copied! <pre># Print out prediction logits\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convert prediction probabilities -&gt; prediction labels\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n</pre> # Print out prediction logits print(f\"Prediction logits: {custom_image_pred}\")  # Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification) custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1) print(f\"Prediction probabilities: {custom_image_pred_probs}\")  # Convert prediction probabilities -&gt; prediction labels custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1) print(f\"Prediction label: {custom_image_pred_label}\") <pre>Prediction logits: tensor([[ 0.1172,  0.0160, -0.1425]], device='cuda:0')\nPrediction probabilities: tensor([[0.3738, 0.3378, 0.2883]], device='cuda:0')\nPrediction label: tensor([0], device='cuda:0')\n</pre> <p>Alright!</p> <p>Looking good.</p> <p>But of course our prediction label is still in index/tensor form.</p> <p>We can convert it to a string class name prediction by indexing on the <code>class_names</code> list.</p> In\u00a0[70]: Copied! <pre># Find the predicted label\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n</pre> # Find the predicted label custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error custom_image_pred_class Out[70]: <pre>'pizza'</pre> <p>Wow.</p> <p>It looks like the model gets the prediction right, even though it was performing poorly based on our evaluation metrics.</p> <p>Note: The model in its current form will predict \"pizza\", \"steak\" or \"sushi\" no matter what image it's given. If you wanted your model to predict on a different class, you'd have to train it to do so.</p> <p>But if we check the <code>custom_image_pred_probs</code>, we'll notice that the model gives almost equal weight (the values are similar) to every class.</p> In\u00a0[71]: Copied! <pre># The values of the prediction probabilities are quite similar\ncustom_image_pred_probs\n</pre> # The values of the prediction probabilities are quite similar custom_image_pred_probs Out[71]: <pre>tensor([[0.3738, 0.3378, 0.2883]], device='cuda:0')</pre> <p>Having prediction probabilities this similar could mean a couple of things:</p> <ol> <li>The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi).</li> <li>The model doesn't really know what it wants to predict and is in turn just assigning similar values to each of the classes.</li> </ol> <p>Our case is number 2, since our model is poorly trained, it is basically guessing the prediction.</p> In\u00a0[72]: Copied! <pre>def pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n\"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n</pre> def pred_and_plot_image(model: torch.nn.Module,                          image_path: str,                          class_names: List[str] = None,                          transform=None,                         device: torch.device = device):     \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"          # 1. Load in image and convert the tensor values to float32     target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)          # 2. Divide the image pixel values by 255 to get them between [0, 1]     target_image = target_image / 255.           # 3. Transform if necessary     if transform:         target_image = transform(target_image)          # 4. Make sure the model is on the target device     model.to(device)          # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():         # Add an extra dimension to the image         target_image = target_image.unsqueeze(dim=0)              # Make a prediction on image with an extra dimension and send it to the target device         target_image_pred = model(target_image.to(device))              # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 7. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)          # 8. Plot the image alongside the prediction and prediction probability     plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib     if class_names:         title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     else:          title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     plt.title(title)     plt.axis(False); <p>What a nice looking function, let's test it out.</p> In\u00a0[73]: Copied! <pre># Pred on our custom image\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n</pre> # Pred on our custom image pred_and_plot_image(model=model_1,                     image_path=custom_image_path,                     class_names=class_names,                     transform=custom_image_transform,                     device=device) <p>Two thumbs up again!</p> <p>Looks like our model got the prediction right just by guessing.</p> <p>This won't always be the case with other images though...</p> <p>The image is pixelated too because we resized it to <code>[64, 64]</code> using <code>custom_image_transform</code>.</p> <p>Exercise: Try making a prediction with one of your own images of pizza, steak or sushi and see what happens.</p>"},{"location":"04_pytorch_custom_datasets/#04-pytorch-custom-datasets","title":"04. PyTorch Custom Datasets\u00b6","text":"<p>In the last notebook, notebook 03, we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).</p> <p>The steps we took are similar across many different problems in machine learning.</p> <p>Find a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.</p> <p>PyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, you'll often want to use your own custom dataset.</p>"},{"location":"04_pytorch_custom_datasets/#what-is-a-custom-dataset","title":"What is a custom dataset?\u00b6","text":"<p>A custom dataset is a collection of data relating to a specific problem you're working on.</p> <p>In essence, a custom dataset can be comprised of almost anything.</p> <p>For example, if we were building a food image classification app like Nutrify, our custom dataset might be images of food.</p> <p>Or if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.</p> <p>Or if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.</p> <p>Or if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.</p> <p>PyTorch includes many existing functions to load in various custom datasets in the <code>TorchVision</code>, <code>TorchText</code>, <code>TorchAudio</code> and <code>TorchRec</code> domain libraries.</p> <p>But sometimes these existing functions may not be enough.</p> <p>In that case, we can always subclass <code>torch.utils.data.Dataset</code> and customize it to our liking.</p>"},{"location":"04_pytorch_custom_datasets/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to be applying the PyTorch Workflow we covered in notebook 01 and notebook 02 to a computer vision problem.</p> <p>But instead of using an in-built PyTorch dataset, we're going to be using our own dataset of pizza, steak and sushi images.</p> <p>The goal will be to load these images and then build a model to train and predict on them.</p> <p>What we're going to build. We'll use <code>torchvision.datasets</code> as well as our own custom <code>Dataset</code> class to load in images of food and then we'll build a PyTorch computer vision model to hopefully be able to classify them.</p> <p>Specifically, we're going to cover:</p> Topic Contents 0. Importing PyTorch and setting up device-agnostic code Let's get PyTorch loaded and then follow best practice to setup our code to be device-agnostic. 1. Get data We're going to be using our own custom dataset of pizza, steak and sushi images. 2. Become one with the data (data preparation) At the beginning of any new machine learning problem, it's paramount to understand the data you're working with. Here we'll take some steps to figure out what data we have. 3. Transforming data Often, the data you get won't be 100% ready to use with a machine learning model, here we'll look at some steps we can take to transform our images so they're ready to be used with a model. 4. Loading data with <code>ImageFolder</code> (option 1) PyTorch has many in-built data loading functions for common types of data. <code>ImageFolder</code> is helpful if our images are in standard image classification format. 5. Loading image data with a custom <code>Dataset</code> What if PyTorch didn't have an in-built function to load data with? This is where we can build our own custom subclass of <code>torch.utils.data.Dataset</code>. 6. Other forms of transforms (data augmentation) Data augmentation is a common technique for expanding the diversity of your training data. Here we'll explore some of <code>torchvision</code>'s in-built data augmentation functions. 7. Model 0: TinyVGG without data augmentation By this stage, we'll have our data ready, let's build a model capable of fitting it. We'll also create some training and testing functions for training and evaluating our model. 8. Exploring loss curves Loss curves are a great way to see how your model is training/improving over time. They're also a good way to see if your model is underfitting or overfitting. 9. Model 1: TinyVGG with data augmentation By now, we've tried a model without, how about we try one with data augmentation? 10. Compare model results Let's compare our different models' loss curves and see which performed better and discuss some options for improving performance. 11. Making a prediction on a custom image Our model is trained to on a dataset of pizza, steak and sushi images. In this section we'll cover how to use our trained model to predict on an image outside of our existing dataset."},{"location":"04_pytorch_custom_datasets/#where-can-can-you-get-help","title":"Where can can you get help?\u00b6","text":"<p>All of the materials for this course live on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page there too.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"04_pytorch_custom_datasets/#0-importing-pytorch-and-setting-up-device-agnostic-code","title":"0. Importing PyTorch and setting up device-agnostic code\u00b6","text":""},{"location":"04_pytorch_custom_datasets/#1-get-data","title":"1. Get data\u00b6","text":"<p>First thing's first we need some data.</p> <p>And like any good cooking show, some data has already been prepared for us.</p> <p>We're going to start small.</p> <p>Because we're not looking to train the biggest model or use the biggest dataset yet.</p> <p>Machine learning is an iterative process, start small, get something working and increase when necessary.</p> <p>The data we're going to be using is a subset of the Food101 dataset.</p> <p>Food101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).</p> <p>Can you think of 101 different foods?</p> <p>Can you think of a computer program to classify 101 foods?</p> <p>I can.</p> <p>A machine learning model!</p> <p>Specifically, a PyTorch computer vision model like we covered in notebook 03.</p> <p>Instead of 101 food classes though, we're going to start with 3: pizza, steak and sushi.</p> <p>And instead of 1,000 images per class, we're going to start with a random 10% (start small, increase when necessary).</p> <p>If you'd like to see where the data came from you see the following resources:</p> <ul> <li>Original Food101 dataset and paper website.</li> <li><code>torchvision.datasets.Food101</code> - the version of the data I downloaded for this notebook.</li> <li><code>extras/04_custom_data_creation.ipynb</code> - a notebook I used to format the Food101 dataset to use for this notebook.</li> <li><code>data/pizza_steak_sushi.zip</code> - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.</li> </ul> <p>Let's write some code to download the formatted data from GitHub.</p> <p>Note: The dataset we're about to use has been pre-formatted for what we'd like to use it for. However, you'll often have to format your own datasets for whatever problem you're working on. This is a regular practice in the machine learning world.</p>"},{"location":"04_pytorch_custom_datasets/#2-become-one-with-the-data-data-preparation","title":"2. Become one with the data (data preparation)\u00b6","text":"<p>Dataset downloaded!</p> <p>Time to become one with it.</p> <p>This is another important step before building a model.</p> <p>As Abraham Lossfunction said...</p> <p>Data preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here? Source: @mrdbourke Twitter.</p> <p>What's inspecting the data and becoming one with it?</p> <p>Before starting a project or building any kind of model, it's important to know what data you're working with.</p> <p>In our case, we have images of pizza, steak and sushi in standard image classification format.</p> <p>Image classification format contains separate classes of images in seperate directories titled with a particular class name.</p> <p>For example, all images of <code>pizza</code> are contained in the <code>pizza/</code> directory.</p> <p>This format is popular across many different image classification benchmarks, including ImageNet (of the most popular computer vision benchmark datasets).</p> <p>You can see an example of the storage format below, the images numbers are arbitrary.</p> <pre><code>pizza_steak_sushi/ &lt;- overall dataset folder\n    train/ &lt;- training images\n        pizza/ &lt;- class name as folder name\n            image01.jpeg\n            image02.jpeg\n            ...\n        steak/\n            image24.jpeg\n            image25.jpeg\n            ...\n        sushi/\n            image37.jpeg\n            ...\n    test/ &lt;- testing images\n        pizza/\n            image101.jpeg\n            image102.jpeg\n            ...\n        steak/\n            image154.jpeg\n            image155.jpeg\n            ...\n        sushi/\n            image167.jpeg\n            ...\n</code></pre> <p>The goal will be to take this data storage structure and turn it into a dataset usable with PyTorch.</p> <p>Note: The structure of the data you work with will vary depending on the problem you're working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.</p> <p>We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.</p> <p>To do so, we'll use Python's in-built <code>os.walk()</code>.</p>"},{"location":"04_pytorch_custom_datasets/#21-visualize-an-image","title":"2.1 Visualize an image\u00b6","text":"<p>Okay, we've seen how our directory structure is formatted.</p> <p>Now in the spirit of the data explorer, it's time to visualize, visualize, visualize!</p> <p>Let's write some code to:</p> <ol> <li>Get all of the image paths using <code>pathlib.Path.glob()</code> to find all of the files ending in <code>.jpg</code>.</li> <li>Pick a random image path using Python's <code>random.choice()</code>.</li> <li>Get the image class name using <code>pathlib.Path.parent.stem</code>.</li> <li>And since we're working with images, we'll open the random image path using <code>PIL.Image.open()</code> (PIL stands for Python Image Library).</li> <li>We'll then show the image and print some metadata.</li> </ol>"},{"location":"04_pytorch_custom_datasets/#3-transforming-data","title":"3. Transforming data\u00b6","text":"<p>Now what if we wanted to load our image data into PyTorch?</p> <p>Before we can use our image data with PyTorch we need to:</p> <ol> <li>Turn it into tensors (numerical representations of our images).</li> <li>Turn it into a <code>torch.utils.data.Dataset</code> and subsequently a <code>torch.utils.data.DataLoader</code>, we'll call these <code>Dataset</code> and <code>DataLoader</code> for short.</li> </ol> <p>There are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you're working on.</p> Problem space Pre-built Datasets and Functions Vision <code>torchvision.datasets</code> Audio <code>torchaudio.datasets</code> Text <code>torchtext.datasets</code> Recommendation system <code>torchrec.datasets</code> <p>Since we're working with a vision problem, we'll be looking at <code>torchvision.datasets</code> for our data loading functions as well as <code>torchvision.transforms</code> for preparing our data.</p> <p>Let's import some base libraries.</p>"},{"location":"04_pytorch_custom_datasets/#31-transforming-data-with-torchvisiontransforms","title":"3.1 Transforming data with <code>torchvision.transforms</code>\u00b6","text":"<p>We've got folders of images but before we can use them with PyTorch, we need to convert them into tensors.</p> <p>One of the ways we can do this is by using the <code>torchvision.transforms</code> module.</p> <p><code>torchvision.transforms</code> contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, we'll see this later on) purposes .</p> <p>To get experience with <code>torchvision.transforms</code>, let's write a series of transform steps that:</p> <ol> <li>Resize the images using <code>transforms.Resize()</code> (from about 512x512 to 64x64, the same shape as the images on the CNN Explainer website).</li> <li>Flip our images randomly on the horizontal using <code>transforms.RandomHorizontalFlip()</code> (this could be considered a form of data augmentation because it will artificially change our image data).</li> <li>Turn our images from a PIL image to a PyTorch tensor using <code>transforms.ToTensor()</code>.</li> </ol> <p>We can compile all of these steps using <code>torchvision.transforms.Compose()</code>.</p>"},{"location":"04_pytorch_custom_datasets/#4-option-1-loading-image-data-using-imagefolder","title":"4. Option 1: Loading Image Data Using <code>ImageFolder</code>\u00b6","text":"<p>Alright, time to turn our image data into a <code>Dataset</code> capable of being used with PyTorch.</p> <p>Since our data is in standard image classification format, we can use the class <code>torchvision.datasets.ImageFolder</code>.</p> <p>Where we can pass it the file path of a target image directory as well as a series of transforms we'd like to perform on our images.</p> <p>Let's test it out on our data folders <code>train_dir</code> and <code>test_dir</code> passing in <code>transform=data_transform</code> to turn our images into tensors.</p>"},{"location":"04_pytorch_custom_datasets/#41-turn-loaded-images-into-dataloaders","title":"4.1 Turn loaded images into <code>DataLoader</code>'s\u00b6","text":"<p>We've got our images as PyTorch <code>Dataset</code>'s but now let's turn them into <code>DataLoader</code>'s.</p> <p>We'll do so using <code>torch.utils.data.DataLoader</code>.</p> <p>Turning our <code>Dataset</code>'s into <code>DataLoader</code>'s makes them iterable so a model can go through learn the relationships between samples and targets (features and labels).</p> <p>To keep things simple, we'll use a <code>batch_size=1</code> and <code>num_workers=1</code>.</p> <p>What's <code>num_workers</code>?</p> <p>Good question.</p> <p>It defines how many subprocesses will be created to load your data.</p> <p>Think of it like this, the higher value <code>num_workers</code> is set to, the more compute power PyTorch will use to load your data.</p> <p>Personally, I usually set it to the total number of CPUs on my machine via Python's <code>os.cpu_count()</code>.</p> <p>This ensures the <code>DataLoader</code> recruits as many cores as possible to load data.</p> <p>Note: There are more parameters you can get familiar with using <code>torch.utils.data.DataLoader</code> in the PyTorch documentation.</p>"},{"location":"04_pytorch_custom_datasets/#5-option-2-loading-image-data-with-a-custom-dataset","title":"5. Option 2: Loading Image Data with a Custom <code>Dataset</code>\u00b6","text":"<p>What if a pre-built <code>Dataset</code> creator like <code>torchvision.datasets.ImageFolder()</code> didn't exist?</p> <p>Or one for your specific problem didn't exist?</p> <p>Well, you could build your own.</p> <p>But wait, what are the pros and cons of creating your own custom way to load <code>Dataset</code>'s?</p> Pros of creating a custom <code>Dataset</code> Cons of creating a custom <code>Dataset</code> Can create a <code>Dataset</code> out of almost anything. Even though you could create a <code>Dataset</code> out of almost anything, it doesn't mean it will work. Not limited to PyTorch pre-built <code>Dataset</code> functions. Using a custom <code>Dataset</code> often results in writing more code, which could be prone to errors or performance issues. <p>To see this in action, let's work towards replicating <code>torchvision.datasets.ImageFolder()</code> by subclassing <code>torch.utils.data.Dataset</code> (the base class for all <code>Dataset</code>'s in PyTorch).</p> <p>We'll start by importing the modules we need:</p> <ul> <li>Python's <code>os</code> for dealing with directories (our data is stored in directories).</li> <li>Python's <code>pathlib</code> for dealing with filepaths (each of our images has a unique filepath).</li> <li><code>torch</code> for all things PyTorch.</li> <li>PIL's <code>Image</code> class for loading images.</li> <li><code>torch.utils.data.Dataset</code> to subclass and create our own custom <code>Dataset</code>.</li> <li><code>torchvision.transforms</code> to turn our images into tensors.</li> <li>Various types from Python's <code>typing</code> module to add type hints to our code.</li> </ul> <p>Note: You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you'd like it.</p>"},{"location":"04_pytorch_custom_datasets/#51-creating-a-helper-function-to-get-class-names","title":"5.1 Creating a helper function to get class names\u00b6","text":"<p>Let's write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.</p> <p>To do so, we'll:</p> <ol> <li>Get the class names using <code>os.scandir()</code> to traverse a target directory (ideally the directory is in standard image classification format).</li> <li>Raise an error if the class names aren't found (if this happens, there might be something wrong with the directory structure).</li> <li>Turn the class names into a dictionary of numerical labels, one for each class.</li> </ol> <p>Let's see a small example of step 1 before we write the full function.</p>"},{"location":"04_pytorch_custom_datasets/#52-create-a-custom-dataset-to-replicate-imagefolder","title":"5.2 Create a custom <code>Dataset</code> to replicate <code>ImageFolder</code>\u00b6","text":"<p>Now we're ready to build our own custom <code>Dataset</code>.</p> <p>We'll build one to replicate the functionality of <code>torchvision.datasets.ImageFolder()</code>.</p> <p>This will be good practice, plus, it'll reveal a few of the required steps to make your own custom <code>Dataset</code>.</p> <p>It'll be a fair bit of a code... but nothing we can't handle!</p> <p>Let's break it down:</p> <ol> <li>Subclass <code>torch.utils.data.Dataset</code>.</li> <li>Initialize our subclass with a <code>targ_dir</code> parameter (the target data directory) and <code>transform</code> parameter (so we have the option to transform our data if needed).</li> <li>Create several attributes for <code>paths</code> (the paths of our target images), <code>transform</code> (the transforms we might like to use, this can be <code>None</code>), <code>classes</code> and <code>class_to_idx</code> (from our <code>find_classes()</code> function).</li> <li>Create a function to load images from file and return them, this could be using <code>PIL</code> or <code>torchvision.io</code> (for input/output of vision data).</li> <li>Overwrite the <code>__len__</code> method of <code>torch.utils.data.Dataset</code> to return the number of samples in the <code>Dataset</code>, this is recommended but not required. This is so you can call <code>len(Dataset)</code>.</li> <li>Overwrite the <code>__getitem__</code> method of <code>torch.utils.data.Dataset</code> to return a single sample from the <code>Dataset</code>, this is required.</li> </ol> <p>Let's do it!</p>"},{"location":"04_pytorch_custom_datasets/#53-create-a-function-to-display-random-images","title":"5.3 Create a function to display random images\u00b6","text":"<p>You know what time it is!</p> <p>Time to put on our data explorer's hat and visualize, visualize, visualize!</p> <p>Let's create a helper function called <code>display_random_images()</code> that helps us visualize images in our <code>Dataset'</code>s.</p> <p>Specifically, it'll:</p> <ol> <li>Take in a <code>Dataset</code> and a number of other parameters such as <code>classes</code> (the names of our target classes), the number of images to display (<code>n</code>) and a random seed.</li> <li>To prevent the display getting out of hand, we'll cap <code>n</code> at 10 images.</li> <li>Set the random seed for reproducible plots (if <code>seed</code> is set).</li> <li>Get a list of random sample indexes (we can use Python's <code>random.sample()</code> for this) to plot.</li> <li>Setup a <code>matplotlib</code> plot.</li> <li>Loop through the random sample indexes found in step 4 and plot them with <code>matplotlib</code>.</li> <li>Make sure the sample images are of shape <code>HWC</code> (height, width, color channels) so we can plot them.</li> </ol>"},{"location":"04_pytorch_custom_datasets/#54-turn-custom-loaded-images-into-dataloaders","title":"5.4 Turn custom loaded images into <code>DataLoader</code>'s\u00b6","text":"<p>We've got a way to turn our raw images into <code>Dataset</code>'s (features mapped to labels or <code>X</code>'s mapped to <code>y</code>'s) through our <code>ImageFolderCustom</code> class.</p> <p>Now how could we turn our custom <code>Dataset</code>'s into <code>DataLoader</code>'s?</p> <p>If you guessed by using <code>torch.utils.data.DataLoader()</code>, you'd be right!</p> <p>Because our custom <code>Dataset</code>'s subclass <code>torch.utils.data.Dataset</code>, we can use them directly with <code>torch.utils.data.DataLoader()</code>.</p> <p>And we can do using very similar steps to before except this time we'll be using our custom created <code>Dataset</code>'s.</p>"},{"location":"04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation","title":"6. Other forms of transforms (data augmentation)\u00b6","text":"<p>We've seen a couple of transforms on our data already but there's plenty more.</p> <p>You can see them all in the <code>torchvision.transforms</code> documentation.</p> <p>The purpose of tranforms is to alter your images in some way.</p> <p>That may be turning your images into a tensor (as we've seen before).</p> <p>Or cropping it or randomly erasing a portion or randomly rotating them.</p> <p>Doing this kinds of transforms is often referred to as data augmentation.</p> <p>Data augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.</p> <p>Training a model on this artificially altered dataset hopefully results in a model that is capable of better generalization (the patterns it learns are more robust to future unseen examples).</p> <p>You can see many different examples of data augmentation performed on images using <code>torchvision.transforms</code> in PyTorch's Illustration of Transforms example.</p> <p>But let's try one out ourselves.</p> <p>Machine learning is all about harnessing the power of randomness and research shows that random transforms (like <code>transforms.RandAugment()</code> and <code>transforms.TrivialAugmentWide()</code>) generally perform better than hand-picked transforms.</p> <p>The idea behind TrivialAugment is... well, trivial.</p> <p>You have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).</p> <p>The PyTorch team even used TrivialAugment it to train their latest state-of-the-art vision models.</p> <p></p> <p>TrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.</p> <p>How about we test it out on some of our own images?</p> <p>The main parameter to pay attention to in <code>transforms.TrivialAugmentWide()</code> is <code>num_magnitude_bins=31</code>.</p> <p>It defines how much of a range an intensity value will be picked to apply a certain transform, <code>0</code> being no range and <code>31</code> being maximum range (highest chance for highest intensity).</p> <p>We can incorporate <code>transforms.TrivialAugmentWide()</code> into <code>transforms.Compose()</code>.</p>"},{"location":"04_pytorch_custom_datasets/#7-model-0-tinyvgg-without-data-augmentation","title":"7. Model 0: TinyVGG without data augmentation\u00b6","text":"<p>Alright, we've seen how to turn our data from images in folders to transformed tensors.</p> <p>Now let's construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.</p> <p>To begin, we'll start with a simple transform, only resizing the images to <code>(64, 64)</code> and turning them into tensors.</p>"},{"location":"04_pytorch_custom_datasets/#71-creating-transforms-and-loading-data-for-model-0","title":"7.1 Creating transforms and loading data for Model 0\u00b6","text":""},{"location":"04_pytorch_custom_datasets/#72-create-tinyvgg-model-class","title":"7.2 Create TinyVGG model class\u00b6","text":"<p>In notebook 03, we used the TinyVGG model from the CNN Explainer website.</p> <p>Let's recreate the same model, except this time we'll be using color images instead of grayscale (<code>in_channels=3</code> instead of <code>in_channels=1</code> for RGB pixels).</p>"},{"location":"04_pytorch_custom_datasets/#73-try-a-forward-pass-on-a-single-image-to-test-the-model","title":"7.3 Try a forward pass on a single image (to test the model)\u00b6","text":"<p>A good way to test a model is to do a forward pass on a single piece of data.</p> <p>It's also handy way to test the input and output shapes of our different layers.</p> <p>To do a forward pass on a single image, let's:</p> <ol> <li>Get a batch of images and labels from the <code>DataLoader</code>.</li> <li>Get a single image from the batch and <code>unsqueeze()</code> the image so it has a batch size of <code>1</code> (so its shape fits the model).</li> <li>Perform inference on a single image (making sure to send the image to the target <code>device</code>).</li> <li>Print out what's happening and convert the model's raw output logits to prediction probabilities with <code>torch.softmax()</code> (since we're working with multi-class data) and convert the prediction probabilities to prediction labels with <code>torch.argmax()</code>.</li> </ol>"},{"location":"04_pytorch_custom_datasets/#74-use-torchinfo-to-get-an-idea-of-the-shapes-going-through-our-model","title":"7.4 Use <code>torchinfo</code> to get an idea of the shapes going through our model\u00b6","text":"<p>Printing out our model with <code>print(model)</code> gives us an idea of what's going on with our model.</p> <p>And we can print out the shapes of our data throughout the <code>forward()</code> method.</p> <p>However, a helpful way to get information from our model is to use <code>torchinfo</code>.</p> <p><code>torchinfo</code> comes with a <code>summary()</code> method that takes a PyTorch model as well as an <code>input_shape</code> and returns what happens as a tensor moves through your model.</p> <p>Note: If you're using Google Colab, you'll need to install <code>torchinfo</code>.</p>"},{"location":"04_pytorch_custom_datasets/#75-create-train-test-loop-functions","title":"7.5 Create train &amp; test loop functions\u00b6","text":"<p>We've got data and we've got a model.</p> <p>Now let's make some training and test loop functions to train our model on the training data and evaluate our model on the testing data.</p> <p>And to make sure we can use these the training and testing loops again, we'll functionize them.</p> <p>Specifically, we're going to make three functions:</p> <ol> <li><code>train_step()</code> - takes in a model, a <code>DataLoader</code>, a loss function and an optimizer and trains the model on the <code>DataLoader</code>.</li> <li><code>test_step()</code> - takes in a model, a <code>DataLoader</code> and a loss function and evaluates the model on the <code>DataLoader</code>.</li> <li><code>train()</code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</li> </ol> <p>Note: We covered the steps in a PyTorch opimization loop in notebook 01, as well as the Unofficial PyTorch Optimization Loop Song and we've built similar functions in notebook 03.</p> <p>Let's start by building <code>train_step()</code>.</p> <p>Because we're dealing with batches in the <code>DataLoader</code>'s, we'll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them.</p>"},{"location":"04_pytorch_custom_datasets/#76-creating-a-train-function-to-combine-train_step-and-test_step","title":"7.6 Creating a <code>train()</code> function to combine <code>train_step()</code> and <code>test_step()</code>\u00b6","text":"<p>Now we need a way to put our <code>train_step()</code> and <code>test_step()</code> functions together.</p> <p>To do so, we'll package them up in a <code>train()</code> function.</p> <p>This function will train the model as well as evaluate it.</p> <p>Specificially, it'll:</p> <ol> <li>Take in a model, a <code>DataLoader</code> for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for.</li> <li>Create an empty results dictionary for <code>train_loss</code>, <code>train_acc</code>, <code>test_loss</code> and <code>test_acc</code> values (we can fill this up as training goes on).</li> <li>Loop through the training and test step functions for a number of epochs.</li> <li>Print out what's happening at the end of each epoch.</li> <li>Update the empty results dictionary with the updated metrics each epoch.</li> <li>Return the filled</li> </ol> <p>To keep track of the number of epochs we've been through, let's import <code>tqdm</code> from <code>tqdm.auto</code> (<code>tqdm</code> is one of the most popular progress bar libraries for Python and <code>tqdm.auto</code> automatically decides what kind of progress bar is best for your computing environment, e.g. Jupyter Notebook vs. Python script).</p>"},{"location":"04_pytorch_custom_datasets/#77-train-and-evaluate-model-0","title":"7.7 Train and Evaluate Model 0\u00b6","text":"<p>Alright, alright, alright we've got all of the ingredients we need to train and evaluate our model.</p> <p>Time to put our <code>TinyVGG</code> model, <code>DataLoader</code>'s and <code>train()</code> function together to see if we can build a model capable of discerning between pizza, steak and sushi!</p> <p>Let's recreate <code>model_0</code> (we don't need to but we will for completeness) then call our <code>train()</code> function passing in the necessary parameters.</p> <p>To keep our experiments quick, we'll train our model for 5 epochs (though you could increase this if you want).</p> <p>As for an optimizer and loss function, we'll use <code>torch.nn.CrossEntropyLoss()</code> (since we're working with multi-class classification data) and <code>torch.optim.Adam()</code> with a learning rate of <code>1e-3</code> respecitvely.</p> <p>To see how long things take, we'll import Python's <code>timeit.default_timer()</code> method to calculate the training time.</p>"},{"location":"04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0","title":"7.8 Plot the loss curves of Model 0\u00b6","text":"<p>From the print outs of our <code>model_0</code> training, it didn't look like it did too well.</p> <p>But we can further evaluate it by plotting the model's loss curves.</p> <p>Loss curves show the model's results over time.</p> <p>And they're a great way to see how your model performs on different datasets (e.g. training and test).</p> <p>Let's create a function to plot the values in our <code>model_0_results</code> dictionary.</p>"},{"location":"04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like","title":"8. What should an ideal loss curve look like?\u00b6","text":"<p>Looking at training and test loss curves is a great way to see if your model is overfitting.</p> <p>An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.</p> <p>If your training loss is far lower than your test loss, your model is overfitting.</p> <p>As in, it's learning the patterns in the training too well and those patterns aren't generalizing to the test data.</p> <p>The other side is when your training and test loss are not as low as you'd like, this is considered underfitting.</p> <p>The ideal position for a training and test loss curve is for them to line up closely with each other.</p> <p>Left: If your training and test loss curves aren't as low as you'd like, this is considered underfitting. Middle:* When your test/validation loss is higher than your training loss this is considered overfitting. Right: The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google's Interpreting Loss Curves guide.*</p>"},{"location":"04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting","title":"8.1 How to deal with overfitting\u00b6","text":"<p>Since the main problem with overfitting is that you're model is fitting the training data too well, you'll want to use techniques to \"reign it in\".</p> <p>A common technique of preventing overfitting is known as regularization.</p> <p>I like to think of this as \"making our models more regular\", as in, capable of fitting more kinds of data.</p> <p>Let's discuss a few methods to prevent overfitting.</p> Method to prevent overfitting What is it? Get more data Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples. Simplify your model If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer. Use data augmentation Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data. Use transfer learning Transfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images. Use dropout layers Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See <code>torch.nn.Dropout()</code> for more. Use learning rate decay The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to convergence, the smaller you'll want your weight updates to be. Use early stopping Early stopping stops model training before it begins to overfit. As in, say the model's loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior). <p>There are more methods for dealing with overfitting but these are some of the main ones.</p> <p>As you start to build more and more deep models, you'll find because deep learnings are so good at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning.</p>"},{"location":"04_pytorch_custom_datasets/#82-how-to-deal-with-underfitting","title":"8.2 How to deal with underfitting\u00b6","text":"<p>When a model is underfitting it is considered to have poor predictive power on the training and test sets.</p> <p>In essence, an underfitting model will fail to reduce the loss values to a desired level.</p> <p>Right now, looking at our current loss curves, I'd considered our <code>TinyVGG</code> model, <code>model_0</code>, to be underfitting the data.</p> <p>The main idea behind dealing with underfitting is to increase your model's predictive power.</p> <p>There are several ways to do this.</p> Method to prevent underfitting What is it? Add more layers/units to your model If your model is underfitting, it may not have enough capability to learn the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers. Tweak the learning rate Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens. Use transfer learning Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem. Train for longer Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance. Use less regularization Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better."},{"location":"04_pytorch_custom_datasets/#83-the-balance-between-overfitting-and-underfitting","title":"8.3 The balance between overfitting and underfitting\u00b6","text":"<p>None of the methods discussed above are silver bullets, meaning, they don't always work.</p> <p>And preventing overfitting and underfitting is possibly the most active area of machine learning research.</p> <p>Since everone wants their models to fit better (less underfitting) but not so good they don't generalize well and perform in the real world (less overfitting).</p> <p>There's a fine line between overfitting and underfitting.</p> <p>Because too much of each can cause the other.</p> <p>Transfer learning is perhaps one of the most powerful techniques when it comes to dealing with both overfitting and underfitting on your own problems.</p> <p>Rather than handcraft different overfitting and underfitting techniques, transfer learning enables you to take an already working model in a similar problem space to yours (say one from paperswithcode.com/sota or Hugging Face models) and apply it to your own dataset.</p> <p>We'll see the power of transfer learning in a later notebook.</p>"},{"location":"04_pytorch_custom_datasets/#9-model-1-tinyvgg-with-data-augmentation","title":"9. Model 1: TinyVGG with Data Augmentation\u00b6","text":"<p>Time to try out another model!</p> <p>This time, let's load in the data and use data augmentation to see if it improves our results in anyway.</p> <p>First, we'll compose a training transform to include <code>transforms.TrivialAugmentWide()</code> as well as resize and turn our images into tensors.</p> <p>We'll do the same for a testing transform except without the data augmentation.</p>"},{"location":"04_pytorch_custom_datasets/#91-create-transform-with-data-augmentation","title":"9.1 Create transform with data augmentation\u00b6","text":""},{"location":"04_pytorch_custom_datasets/#92-create-train-and-test-datasets-and-dataloaders","title":"9.2 Create train and test <code>Dataset</code>'s and <code>DataLoader</code>'s\u00b6","text":"<p>We'll make sure the train <code>Dataset</code> uses the <code>train_transform_trivial_augment</code> and the test <code>Dataset</code> uses the <code>test_transform</code>.</p>"},{"location":"04_pytorch_custom_datasets/#93-construct-and-train-model-1","title":"9.3 Construct and train Model 1\u00b6","text":"<p>Data loaded!</p> <p>Now to build our next model, <code>model_1</code>, we can reuse our <code>TinyVGG</code> class from before.</p> <p>We'll make sure to send it to the target device.</p>"},{"location":"04_pytorch_custom_datasets/#94-plot-the-loss-curves-of-model-1","title":"9.4 Plot the loss curves of Model 1\u00b6","text":"<p>Since we've got the results of <code>model_1</code> saved in a results dictionary, <code>model_1_results</code>, we can plot them using <code>plot_loss_curves()</code>.</p>"},{"location":"04_pytorch_custom_datasets/#10-compare-model-results","title":"10. Compare model results\u00b6","text":"<p>Even though our models our performing quite poorly, we can still write code to compare them.</p> <p>Let's first turn our model results in pandas DataFrames.</p>"},{"location":"04_pytorch_custom_datasets/#11-make-a-prediction-on-a-custom-image","title":"11. Make a prediction on a custom image\u00b6","text":"<p>If you've trained a model on a certain dataset, chances are you'd like to make a prediction on on your own custom data.</p> <p>In our case, since we've trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images?</p> <p>To do so, we can load an image and then preprocess it in a way that matches the type of data our model was trained on.</p> <p>In other words, we'll have to convert our own custom image to a tensor and make sure it's in the right datatype before passing it to our model.</p> <p>Let's start by downloading a custom image.</p> <p>Since our model predicts whether an image contains pizza, steak or sushi, let's download a photo of my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub.</p> <p>We download the image using Python's <code>requests</code> module.</p> <p>Note: If you're using Google Colab, you can also upload an image to the current session by going to the left hand side menu -&gt; Files -&gt; Upload to session storage. Beware though, this image will delete when your Google Colab session ends.</p>"},{"location":"04_pytorch_custom_datasets/#111-loading-in-a-custom-image-with-pytorch","title":"11.1 Loading in a custom image with PyTorch\u00b6","text":"<p>Excellent!</p> <p>Looks like we've got a custom image downloaded and ready to go at <code>data/04-pizza-dad.jpeg</code>.</p> <p>Time to load it in.</p> <p>PyTorch's <code>torchvision</code> has several input and output (\"IO\" or \"io\" for short) methods for reading and writing images and video in <code>torchvision.io</code>.</p> <p>Since we want to load in an image, we'll use <code>torchvision.io.read_image()</code>.</p> <p>This method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale <code>torch.Tensor</code> with values of datatype <code>uint8</code> in range <code>[0, 255]</code>.</p> <p>Let's try it out.</p>"},{"location":"04_pytorch_custom_datasets/#112-predicting-on-custom-images-with-a-trained-pytorch-model","title":"11.2 Predicting on custom images with a trained PyTorch model\u00b6","text":"<p>Beautiful, it looks like our image data is now in the same format our model was trained on.</p> <p>Except for one thing...</p> <p>It's <code>shape</code>.</p> <p>Our model was trained on images with shape <code>[3, 64, 64]</code>, whereas our custom image is currently <code>[3, 4032, 3024]</code>.</p> <p>How could we make sure our custom image is the same shape as the images our model was trained on?</p> <p>Are there any <code>torchvision.transforms</code> that could help?</p> <p>Before we answer that question, let's plot the image with <code>matplotlib</code> to make sure it looks okay, remember we'll have to permute the dimensions from <code>CHW</code> to <code>HWC</code> to suit <code>matplotlib</code>'s requirements.</p>"},{"location":"04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function","title":"11.3 Putting custom image prediction together: building a function\u00b6","text":"<p>Doing all of the above steps every time you'd like to make a prediction on a custom image would quickly become tedious.</p> <p>So let's put them all together in a function we can easily use over and over again.</p> <p>Specifically, let's make a function that:</p> <ol> <li>Takes in a target image path and converts to the right datatype for our model (<code>torch.float32</code>).</li> <li>Makes sure the target image pixel values are in the range <code>[0, 1]</code>.</li> <li>Transforms the target image if necessary.</li> <li>Makes sure the model is on the target device.</li> <li>Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model).</li> <li>Converts the model's output logits to prediction probabilities.</li> <li>Converts the prediction probabilities to prediction labels.</li> <li>Plots the target image alongside the model prediction and prediction probability.</li> </ol> <p>A fair few steps but we've got this!</p>"},{"location":"04_pytorch_custom_datasets/#main-takeaways","title":"Main takeaways\u00b6","text":"<p>We've covered a fair bit in this module.</p> <p>Let's summarise it with a few dot points.</p> <ul> <li>PyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.</li> <li>If PyTorch's built-in data loading functions don't suit your requirements, you can write code to create your own custom datasets by subclassing <code>torch.utils.data.Dataset</code>.</li> <li><code>torch.utils.data.DataLoader</code>'s in PyTorch help turn your <code>Dataset</code>'s into iterables that can be used when training and testing a model.</li> <li>A lot of machine learning is dealing with the balance between overfitting and underfitting (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).</li> <li>Predicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:<ol> <li>Wrong datatypes - Your model expected <code>torch.float32</code> when your data is <code>torch.uint8</code>.</li> <li>Wrong data shapes - Your model expected <code>[batch_size, color_channels, height, width]</code> when your data is <code>[color_channels, height, width]</code>.</li> <li>Wrong devices - Your model is on the GPU but your data is on the CPU.</li> </ol> </li> </ul>"},{"location":"04_pytorch_custom_datasets/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code in the sections above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 04</li> <li>Example solutions notebook for 04 (try the exercises before looking at this)</li> </ul> <ol> <li>Our models are underperforming (not fitting the data well). What are 3 methods for preventing underfitting? Write them down and explain each with a sentence.</li> <li>Recreate the data loading functions we built in sections 1, 2, 3 and 4. You should have train and test <code>DataLoader</code>'s ready to use.</li> <li>Recreate <code>model_0</code> we built in section 7.</li> <li>Create training and testing functions for <code>model_0</code>.</li> <li>Try training the model you made in exercise 3 for 5, 20 and 50 epochs, what happens to the results?<ul> <li>Use <code>torch.optim.Adam()</code> with a learning rate of 0.001 as the optimizer.</li> </ul> </li> <li>Double the number of hidden units in your model and train it for 20 epochs, what happens to the results?</li> <li>Double the data you're using with your model and train it for 20 epochs, what happens to the results?<ul> <li>Note: You can use the custom data creation notebook to scale up your Food101 dataset.</li> <li>You can also find the already formatted double data (20% instead of 10% subset) dataset on GitHub, you will need to write download code like in exercise 2 to get it into this notebook.</li> </ul> </li> <li>Make a prediction on your own custom image of pizza/steak/sushi (you could even download one from the internet) and share your prediction.<ul> <li>Does the model you trained in exercise 7 get it right?</li> <li>If not, what do you think you could do to improve it?</li> </ul> </li> </ol>"},{"location":"04_pytorch_custom_datasets/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>To practice your knowledge of PyTorch <code>Dataset</code>'s and <code>DataLoader</code>'s through PyTorch datasets and dataloaders tutorial notebook.</li> <li>Spend 10-minutes reading the PyTorch <code>torchvision.transforms</code> documentation.<ul> <li>You can see demos of transforms in action in the illustrations of transforms tutorial.</li> </ul> </li> <li>Spend 10-minutes reading the PyTorch <code>torchvision.datasets</code> documentation.<ul> <li>What are some datasets that stand out to you?</li> <li>How could you try building a model on these?</li> </ul> </li> <li>TorchData is currently in beta (as of April 2022), it'll be a future way of loading data in PyTorch, but you can start to check it out now.</li> <li>To speed up deep learning models, you can do a few tricks to improve compute, memory and overhead computations, for more read the post Making Deep Learning Go Brrrr From First Principles by Horace He.</li> </ul>"},{"location":"05_pytorch_going_modular/","title":"05. PyTorch Going Modular","text":"<p>View Source Code | View Slides </p>"},{"location":"05_pytorch_going_modular/#05-pytorch-going-modular","title":"05. PyTorch Going Modular","text":"<p>This section answers the question, \"how do I turn my notebook code into Python scripts?\"</p> <p>To do so, we're going to turn the most useful code cells in notebook 04. PyTorch Custom Datasets into a series of Python scripts saved to a directory called <code>going_modular</code>.</p>"},{"location":"05_pytorch_going_modular/#what-is-going-modular","title":"What is going modular?","text":"<p>Going modular involves turning notebook code (from a Jupyter Notebook or Google Colab notebook) into a series of different Python scripts that offer similar functionality.</p> <p>For example, we could turn our notebook code from a series of cells into the following Python files:</p> <ul> <li><code>data_setup.py</code> - a file to prepare and download data if needed.</li> <li><code>engine.py</code> - a file containing various training functions.</li> <li><code>model_builder.py</code> or <code>model.py</code> - a file to create a PyTorch model.</li> <li><code>train.py</code> - a file to leverage all other files and train a target PyTorch model.</li> <li><code>utils.py</code> - a file dedicated to helpful utility functions.</li> </ul> <p>Note: The naming and layout of the above files will depend on your use case and code requirements. Python scripts are as general as individual notebook cells, meaning, you could create one for almost any kind of functionality.</p>"},{"location":"05_pytorch_going_modular/#why-would-you-want-to-go-modular","title":"Why would you want to go modular?","text":"<p>Notebooks are fantastic for iteratively exploring and running experiments quickly.</p> <p>However, for larger scale projects you may find Python scripts more reproducible and easier to run.</p> <p>Though this is a debated topic, as companies like Netflix have shown how they use notebooks for production code.</p> <p>Production code is code that runs to offer a service to someone or something.</p> <p>For example, if you have an app running online that other people can access and use, the code running that app is considered production code.</p> <p>And libraries like fast.ai's <code>nb-dev</code> (short for notebook development) enable you to write whole Python libraries (including documentation) with Jupyter Notebooks.</p>"},{"location":"05_pytorch_going_modular/#pros-and-cons-of-notebooks-vs-python-scripts","title":"Pros and cons of notebooks vs Python scripts","text":"<p>There's arguments for both sides.</p> <p>But this list sums up a few of the main topics.</p> Pros Cons Notebooks Easy to experiment/get started Versioning can be hard Easy to share (e.g. a link to a Google Colab notebook) Hard to use only specific parts Very visual Text and graphics can get in the way of code Pros Cons Python scripts Can package code together (saves rewriting similar code across different notebooks) Experimenting isn't as visual (usually have to run the whole script rather than one cell) Can use git for versioning Many open source projects use scripts Larger projects can be run on cloud vendors (not as much support for notebooks)"},{"location":"05_pytorch_going_modular/#my-workflow","title":"My workflow","text":"<p>I usually start machine learning projects in Jupyter/Google Colab notebooks for quick experimentation and visualization.</p> <p>Then when I've got something working, I move the most useful pieces of code to Python scripts.</p> <p></p> <p>There are many possible workflows for writing machine learning code. Some prefer to start with scripts, others (like me) prefer to start with notebooks and go to scripts later on.</p>"},{"location":"05_pytorch_going_modular/#pytorch-in-the-wild","title":"PyTorch in the wild","text":"<p>In your travels, you'll see many code repositories for PyTorch-based ML projects have instructions on how to run the PyTorch code in the form of Python scripts.</p> <p>For example, you might be instructed to run code like the following in a terminal/command line to train a model:</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre> <p> </p> <p>Running a PyTorch <code>train.py</code> script on the command line with various hyperparameter settings.</p> <p>In this case, <code>train.py</code> is the target Python script, it'll likely contain functions to train a PyTorch model.</p> <p>And <code>--model</code>, <code>--batch_size</code>, <code>--lr</code> and <code>--num_epochs</code> are known as argument flags.</p> <p>You can set these to whatever values you like and if they're compatible with <code>train.py</code>, they'll work, if not, they'll error.</p> <p>For example, let's say we wanted to train our TinyVGG model from notebook 04 for 10 epochs with a batch size of 32 and a learning rate of 0.001:</p> <pre><code>python train.py --model tinyvgg --batch_size 32 --lr 0.001 --num_epochs 10\n</code></pre> <p>You could setup any number of these argument flags in your <code>train.py</code> script to suit your needs.</p> <p>The PyTorch blog post for training state-of-the-art computer vision models uses this style.</p> <p></p> <p>PyTorch command line training script recipe for training state-of-the-art computer vision models with 8 GPUs. Source: PyTorch blog.</p>"},{"location":"05_pytorch_going_modular/#what-were-going-to-cover","title":"What we're going to cover","text":"<p>The main concept of this section is: turn useful notebook code cells into reusable Python files.</p> <p>Doing this will save us writing the same code over and over again.</p> <p>There are two notebooks for this section:</p> <ol> <li>05. Going Modular: Part 1 (cell mode) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of notebook 04.</li> <li>05. Going Modular: Part 2 (script mode) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, <code>data_setup.py</code> and <code>train.py</code>. </li> </ol> <p>The text in this document focuses on the code cells 05. Going Modular: Part 2 (script mode), the ones with <code>%%writefile ...</code> at the top.</p>"},{"location":"05_pytorch_going_modular/#why-two-parts","title":"Why two parts?","text":"<p>Because sometimes the best way to learn something is to see how it differs from something else.</p> <p>If you run each notebook side-by-side you'll see how they differ and that's where the key learnings are.</p> <p></p> <p>Running the two notebooks for section 05 side-by-side. You'll notice that the script mode notebook has extra code cells to turn code from the cell mode notebook into Python scripts.</p>"},{"location":"05_pytorch_going_modular/#what-were-working-towards","title":"What we're working towards","text":"<p>By the end of this section we want to have two things:</p> <ol> <li>The ability to train the model we built in notebook 04 (Food Vision Mini) with one line of code on the command line: <code>python train.py</code>.</li> <li>A directory structure of reusable Python scripts, such as: </li> </ol> <pre><code>going_modular/\n\u251c\u2500\u2500 going_modular/\n\u2502   \u251c\u2500\u2500 data_setup.py\n\u2502   \u251c\u2500\u2500 engine.py\n\u2502   \u251c\u2500\u2500 model_builder.py\n\u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 05_going_modular_cell_mode_tinyvgg_model.pth\n\u2502   \u2514\u2500\u2500 05_going_modular_script_mode_tinyvgg_model.pth\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 pizza_steak_sushi/\n        \u251c\u2500\u2500 train/\n        \u2502   \u251c\u2500\u2500 pizza/\n        \u2502   \u2502   \u251c\u2500\u2500 image01.jpeg\n        \u2502   \u2502   \u2514\u2500\u2500 ...\n        \u2502   \u251c\u2500\u2500 steak/\n        \u2502   \u2514\u2500\u2500 sushi/\n        \u2514\u2500\u2500 test/\n            \u251c\u2500\u2500 pizza/\n            \u251c\u2500\u2500 steak/\n            \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"05_pytorch_going_modular/#things-to-note","title":"Things to note","text":"<ul> <li>Docstrings - Writing reproducible and understandable code is important. And with this in mind, each of the functions/classes we'll be putting into scripts has been created with Google's Python docstring style in mind.</li> <li>Imports at the top of scripts - Since all of the Python scripts we're going to create could be considered a small program on their own, all of the scripts require their input modules be imported at the start of the script for example:</li> </ul> <pre><code># Import modules required for train.py\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n</code></pre>"},{"location":"05_pytorch_going_modular/#where-can-you-get-help","title":"Where can you get help?","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch. </p>"},{"location":"05_pytorch_going_modular/#0-cell-mode-vs-script-mode","title":"0. Cell mode vs. script mode","text":"<p>A cell mode notebook such as 05. Going Modular Part 1 (cell mode) is a notebook run normally, each cell in the notebook is either code or markdown.</p> <p>A script mode notebook such as 05. Going Modular Part 2 (script mode) is very similar to a cell mode notebook, however, many of the code cells may be turned into Python scripts.</p> <p>Note: You don't need to create Python scripts via a notebook, you can create them directly through an IDE (integrated developer environment) such as VS Code. Having the script mode notebook as part of this section is just to demonstrate one way of going from notebooks to Python scripts.</p>"},{"location":"05_pytorch_going_modular/#1-get-data","title":"1. Get data","text":"<p>Getting the data in each of the 05 notebooks happens the same as in notebook 04.</p> <p>A call is made to GitHub via Python's <code>requests</code> module to download a <code>.zip</code> file and unzip it.</p> <pre><code>import os\nimport requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n\n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n\n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")\n</code></pre> <p>This results in having a file called <code>data</code> that contains another directory called <code>pizza_steak_sushi</code> with images of pizza, steak and sushi in standard image classification format.</p> <pre><code>data/\n\u2514\u2500\u2500 pizza_steak_sushi/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 pizza/\n    \u2502   \u2502   \u251c\u2500\u2500 train_image01.jpeg\n    \u2502   \u2502   \u251c\u2500\u2500 test_image02.jpeg\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 steak/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 sushi/\n    \u2502       \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 test/\n        \u251c\u2500\u2500 pizza/\n        \u2502   \u251c\u2500\u2500 test_image01.jpeg\n        \u2502   \u2514\u2500\u2500 test_image02.jpeg\n        \u251c\u2500\u2500 steak/\n        \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy","title":"2. Create Datasets and DataLoaders (<code>data_setup.py</code>)","text":"<p>Once we've got data, we can then turn it into PyTorch <code>Dataset</code>'s and <code>DataLoader</code>'s (one for training data and one for testing data).</p> <p>We convert the useful <code>Dataset</code> and <code>DataLoader</code> creation code into a function called <code>create_dataloaders()</code>.</p> <p>And we write it to file using the line <code>%%writefile going_modular/data_setup.py</code>. </p> data_setup.py<pre><code>%%writefile going_modular/data_setup.py\n\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n\"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names\n</code></pre> <p>If we'd like to make <code>DataLoader</code>'s we can now use the function within <code>data_setup.py</code> like so:</p> <pre><code># Import data_setup.py\nfrom going_modular import data_setup\n\n# Create train/test dataloader and get class names as a list\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(...)\n</code></pre>"},{"location":"05_pytorch_going_modular/#3-making-a-model-model_builderpy","title":"3. Making a model (<code>model_builder.py</code>)","text":"<p>Over the past few notebooks (notebook 03 and notebook 04), we've built the TinyVGG model a few times.</p> <p>So it makes sense to put the model into its file so we can reuse it again and again.</p> <p>Let's put our <code>TinyVGG()</code> model class into a script with the line <code>%%writefile going_modular/model_builder.py</code>:</p> model_builder.py<pre><code>%%writefile going_modular/model_builder.py\n\"\"\"\nContains PyTorch model code to instantiate a TinyVGG model.\n\"\"\"\nimport torch\nfrom torch import nn \n\nclass TinyVGG(nn.Module):\n\"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n\n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n</code></pre> <p>Now instead of coding the TinyVGG model from scratch every time, we can import it using:</p> <pre><code>import torch\n# Import model_builder.py\nfrom going_modular import model_builder\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model from the \"model_builder.py\" script\ntorch.manual_seed(42)\nmodel = model_builder.TinyVGG(input_shape=3,\n                              hidden_units=10, \n                              output_shape=len(class_names)).to(device)\n</code></pre>"},{"location":"05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them","title":"4. Creating <code>train_step()</code> and <code>test_step()</code> functions and <code>train()</code> to combine them","text":"<p>We wrote several training functions in notebook 04:</p> <ol> <li><code>train_step()</code> - takes in a model, a <code>DataLoader</code>, a loss function and an optimizer and trains the model on the <code>DataLoader</code>.</li> <li><code>test_step()</code> - takes in a model, a <code>DataLoader</code> and a loss function and evaluates the model on the <code>DataLoader</code>.</li> <li><code>train()</code> - performs 1. and 2. together for a given number of epochs and returns a results dictionary.</li> </ol> <p>Since these will be the engine of our model training, we can put them all into a Python script called <code>engine.py</code> with the line <code>%%writefile going_modular/engine.py</code>:</p> engine.py<pre><code>%%writefile going_modular/engine.py\n\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -&gt; Tuple[float, float]:\n\"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -&gt; Tuple[float, float]:\n\"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results\n</code></pre> <p>Now we've got the <code>engine.py</code> script, we can import functions from it via:</p> <pre><code># Import engine.py\nfrom going_modular import engine\n\n# Use train() by calling it from engine.py\nengine.train(...)\n</code></pre>"},{"location":"05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy","title":"5. Creating a function to save the model (<code>utils.py</code>)","text":"<p>Often you'll want to save a model whilst it's training or after training.</p> <p>Since we've written the code to save a model a few times now in previous notebooks, it makes sense to turn it into a function and save it to file.</p> <p>It's common practice to store helper functions in a file called <code>utils.py</code> (short for utilities).</p> <p>Let's save our <code>save_model()</code> function to a file called <code>utils.py</code> with the line <code>%%writefile going_modular/utils.py</code>: </p> utils.py<pre><code>%%writefile going_modular/utils.py\n\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nimport torch\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n\"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)\n</code></pre> <p>Now if we wanted to use our <code>save_model()</code> function, instead of writing it all over again, we can import it and use it via:</p> <pre><code># Import utils.py\nfrom going_modular import utils\n\n# Save a model to file\nsave_model(model=...\n           target_dir=...,\n           model_name=...)\n</code></pre>"},{"location":"05_pytorch_going_modular/#6-train-evaluate-and-save-the-model-trainpy","title":"6. Train, evaluate and save the model (<code>train.py</code>)","text":"<p>As previously discussed, you'll often come across PyTorch repositories that combine all of their functionality together in a <code>train.py</code> file.</p> <p>This file is essentially saying \"train the model using whatever data is available\".</p> <p>In our <code>train.py</code> file, we'll combine all of the functionality of the other Python scripts we've created and use it to train a model.</p> <p>This way we can train a PyTorch model using a single line of code on the command line:</p> <pre><code>python train.py\n</code></pre> <p>To create <code>train.py</code> we'll go through the following steps:</p> <ol> <li>Import the various dependencies, namely <code>torch</code>, <code>os</code>, <code>torchvision.transforms</code> and all of the scripts from the <code>going_modular</code> directory, <code>data_setup</code>, <code>engine</code>, <code>model_builder</code>, <code>utils</code>.</li> <li>Note: Since <code>train.py</code> will be inside the <code>going_modular</code> directory, we can import the other modules via <code>import ...</code> rather than <code>from going_modular import ...</code>.</li> <li>Setup various hyperparameters such as batch size, number of epochs, learning rate and number of hidden units (these could be set in the future via Python's <code>argparse</code>).</li> <li>Setup the training and test directories.</li> <li>Setup device-agnostic code.</li> <li>Create the necessary data transforms.</li> <li>Create the DataLoaders using <code>data_setup.py</code>.</li> <li>Create the model using <code>model_builder.py</code>.</li> <li>Setup the loss function and optimizer.</li> <li>Train the model using <code>engine.py</code>.</li> <li>Save the model using <code>utils.py</code>. </li> </ol> <p>And we can create the file from a notebook cell using the line <code>%%writefile going_modular/train.py</code>:</p> train.py<pre><code>%%writefile going_modular/train.py\n\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n\n# Setup hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nHIDDEN_UNITS = 10\nLEARNING_RATE = 0.001\n\n# Setup directories\ntrain_dir = \"data/pizza_steak_sushi/train\"\ntest_dir = \"data/pizza_steak_sushi/test\"\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create transforms\ndata_transform = transforms.Compose([\n  transforms.Resize((64, 64)),\n  transforms.ToTensor()\n])\n\n# Create DataLoaders with help from data_setup.py\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=data_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create model with help from model_builder.py\nmodel = model_builder.TinyVGG(\n    input_shape=3,\n    hidden_units=HIDDEN_UNITS,\n    output_shape=len(class_names)\n).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             test_dataloader=test_dataloader,\n             loss_fn=loss_fn,\n             optimizer=optimizer,\n             epochs=NUM_EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nutils.save_model(model=model,\n                 target_dir=\"models\",\n                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")\n</code></pre> <p>Woohoo!</p> <p>Now we can train a PyTorch model by running the following line on the command line:</p> <pre><code>python train.py\n</code></pre> <p>Doing this will leverage all of the other code scripts we've created.</p> <p>And if we wanted to, we could adjust our <code>train.py</code> file to use argument flag inputs with Python's <code>argparse</code> module, this would allow us to provide different hyperparameter settings like previously discussed:</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre>"},{"location":"05_pytorch_going_modular/#exercises","title":"Exercises","text":"<p>Resources:</p> <ul> <li>Exercise template notebook for 05</li> <li>Example solutions notebook for 05<ul> <li>Live coding run through of solutions notebook for 05 on YouTube</li> </ul> </li> </ul> <p>Exercises:</p> <ol> <li>Turn the code to get the data (from section 1. Get Data above) into a Python script, such as <code>get_data.py</code>.<ul> <li>When you run the script using <code>python get_data.py</code> it should check if the data already exists and skip downloading if it does.</li> <li>If the data download is successful, you should be able to access the <code>pizza_steak_sushi</code> images from the <code>data</code> directory.</li> </ul> </li> <li>Use Python's <code>argparse</code> module to be able to send the <code>train.py</code> custom hyperparameter values for training procedures.<ul> <li>Add an argument for using a different:<ul> <li>Training/testing directory</li> <li>Learning rate</li> <li>Batch size</li> <li>Number of epochs to train for</li> <li>Number of hidden units in the TinyVGG model</li> </ul> </li> <li>Keep the default values for each of the above arguments as what they already are (as in notebook 05).</li> <li>For example, you should be able to run something similar to the following line to train a TinyVGG model with a learning rate of 0.003 and a batch size of 64 for 20 epochs: <code>python train.py --learning_rate 0.003 --batch_size 64 --num_epochs 20</code>.</li> <li>Note: Since <code>train.py</code> leverages the other scripts we created in section 05, such as, <code>model_builder.py</code>, <code>utils.py</code> and <code>engine.py</code>, you'll have to make sure they're available to use too. You can find these in the <code>going_modular</code> folder on the course GitHub. </li> </ul> </li> <li>Create a script to predict (such as <code>predict.py</code>) on a target image given a file path with a saved model.<ul> <li>For example, you should be able to run the command <code>python predict.py some_image.jpeg</code> and have a trained PyTorch model predict on the image and return its prediction.</li> <li>To see example prediction code, check out the predicting on a custom image section in notebook 04. </li> <li>You may also have to write code to load in a trained model.</li> </ul> </li> </ol>"},{"location":"05_pytorch_going_modular/#extra-curriculum","title":"Extra-curriculum","text":"<ul> <li>To learn more about structuring a Python project, check out Real Python's guide on Python Application Layouts. </li> <li>For ideas on styling your PyTorch code, check out the PyTorch style guide by Igor Susmelj (much of styling in this chapter is based off this guide + various similar PyTorch repositories).</li> <li>For an example <code>train.py</code> script and various other PyTorch scripts written by the PyTorch team to train state-of-the-art image classification models, check out their <code>classification</code> repository on GitHub. </li> </ul>"},{"location":"06_pytorch_transfer_learning/","title":"06. PyTorch Transfer Learning","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Now let's setup device agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n</pre> import os import zipfile  from pathlib import Path  import requests  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path)      # Remove .zip file     os.remove(data_path / \"pizza_steak_sushi.zip\") <pre>data/pizza_steak_sushi directory exists.\n</pre> <p>Excellent!</p> <p>Now we've got the same dataset we've been using previously, a series of images of pizza, steak and sushi in standard image classification format.</p> <p>Let's now create paths to our training and test directories.</p> In\u00a0[5]: Copied! <pre># Setup Dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup Dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[6]: Copied! <pre># Create a transforms pipeline manually (required for torchvision &lt; 0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n</pre> # Create a transforms pipeline manually (required for torchvision &lt; 0.13) manual_transforms = transforms.Compose([     transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)     transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1      transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)                          std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel), ]) <p>Wonderful!</p> <p>Now we've got a manually created series of transforms ready to prepare our images, let's create training and testing DataLoaders.</p> <p>We can create these using the <code>create_dataloaders</code> function from the <code>data_setup.py</code> script we created in 05. PyTorch Going Modular Part 2.</p> <p>We'll set <code>batch_size=32</code> so our model see's mini-batches of 32 samples at a time.</p> <p>And we can transform our images using the transform pipeline we created above by setting <code>transform=manual_transforms</code>.</p> <p>Note: I've included this manual creation of transforms in this notebook because you may come across resources that use this style. It's also important to note that because these transforms are manually created, they're also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could.</p> In\u00a0[7]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a3a60&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a37c0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n</pre> # Get a set of pretrained model weights weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet weights Out[8]: <pre>EfficientNet_B0_Weights.IMAGENET1K_V1</pre> <p>And now to access the transforms assosciated with our <code>weights</code>, we can use the <code>transforms()</code> method.</p> <p>This is essentially saying \"get the data transforms that were used to train the <code>EfficientNet_B0_Weights</code> on ImageNet\".</p> In\u00a0[9]: Copied! <pre># Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms\n</pre> # Get the transforms used to create our pretrained weights auto_transforms = weights.transforms() auto_transforms Out[9]: <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)</pre> <p>Notice how <code>auto_transforms</code> is very similar to <code>manual_transforms</code>, the only difference is that <code>auto_transforms</code> came with the model architecture we chose, where as we had to create <code>manual_transforms</code> by hand.</p> <p>The benefit of automatically creating a transform through <code>weights.transforms()</code> is that you ensure you're using the same data transformation as the pretrained model used when it was trained.</p> <p>However, the tradeoff of using automatically created transforms is a lack of customization.</p> <p>We can use <code>auto_transforms</code> to create DataLoaders with <code>create_dataloaders()</code> just as before.</p> In\u00a0[10]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=auto_transforms, # perform same data transforms on our own data as the pretrained model                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[10]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951460&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951550&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[11]: Copied! <pre># OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n\n# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n#model # uncomment to output (it's very long)\n</pre> # OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13) # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)  # NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights  model = torchvision.models.efficientnet_b0(weights=weights).to(device)  #model # uncomment to output (it's very long) <p>Note: In previous versions of <code>torchvision</code>, you'd create a pretrained model with code like:</p> <p><code>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)</code></p> <p>However, running this using <code>torchvision</code> v0.13+ will result in errors such as the following:</p> <p><code>UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.</code></p> <p>And...</p> <p><code>UserWarning: Arguments other than a weight enum or None for weights are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing weights=EfficientNet_B0_Weights.IMAGENET1K_V1. You can also use weights=EfficientNet_B0_Weights.DEFAULT to get the most up-to-date weights.</code></p> <p>If we print the model, we get something similar to the following:</p> <p>Lots and lots and lots of layers.</p> <p>This is one of the benefits of transfer learning, taking an existing model, that's been crafted by some of the best engineers in the world and applying to your own problem.</p> <p>Our <code>efficientnet_b0</code> comes in three main parts:</p> <ol> <li><code>features</code> - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as features or feature extractor, \"the base layers of the model learn the different features of images\").</li> <li><code>avgpool</code> - Takes the average of the output of the <code>features</code> layer(s) and turns it into a feature vector.</li> <li><code>classifier</code> - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since <code>efficientnet_b0</code> is pretrained on ImageNet and because ImageNet has 1000 classes, <code>out_features=1000</code> is the default).</li> </ol> In\u00a0[12]: Copied! <pre># Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # Print a summary using torchinfo (uncomment for actual output) summary(model=model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] )  Out[12]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n============================================================================================================================================\nTotal params: 5,288,548\nTrainable params: 5,288,548\nNon-trainable params: 0\nTotal mult-adds (G): 12.35\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.35\nParams size (MB): 21.15\nEstimated Total Size (MB): 3492.77\n============================================================================================================================================</pre> <p>Woah!</p> <p>Now that's a big model!</p> <p>From the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.</p> <p>And there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data.</p> <p>For reference, our model from previous sections, TinyVGG had 8,083 parameters vs. 5,288,548 parameters for <code>efficientnet_b0</code>, an increase of ~654x!</p> <p>What do you think, will this mean better performance?</p> In\u00a0[13]: Copied! <pre># Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False for param in model.features.parameters():     param.requires_grad = False <p>Feature extractor layers frozen!</p> <p>Let's now adjust the output layer or the <code>classifier</code> portion of our pretrained model to our needs.</p> <p>Right now our pretrained model has <code>out_features=1000</code> because there are 1000 classes in ImageNet.</p> <p>However, we don't have 1000 classes, we only have three, pizza, steak and sushi.</p> <p>We can change the <code>classifier</code> portion of our model by creating a new series of layers.</p> <p>The current <code>classifier</code> consists of:</p> <pre><code>(classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n</code></pre> <p>We'll keep the <code>Dropout</code> layer the same using <code>torch.nn.Dropout(p=0.2, inplace=True)</code>.</p> <p>Note: Dropout layers randomly remove connections between two neural network layers with a probability of <code>p</code>. For example, if <code>p=0.2</code>, 20% of connections between neural network layers will be removed at random each pass. This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections (hopefully these remaining features are more general).</p> <p>And we'll keep <code>in_features=1280</code> for our <code>Linear</code> output layer but we'll change the <code>out_features</code> value to the length of our <code>class_names</code> (<code>len(['pizza', 'steak', 'sushi']) = 3</code>).</p> <p>Our new <code>classifier</code> layer should be on the same device as our <code>model</code>.</p> In\u00a0[14]: Copied! <pre># Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n</pre> # Set the manual seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Get the length of class_names (one output unit for each class) output_shape = len(class_names)  # Recreate the classifier layer and seed it to the target device model.classifier = torch.nn.Sequential(     torch.nn.Dropout(p=0.2, inplace=True),      torch.nn.Linear(in_features=1280,                      out_features=output_shape, # same number of output units as our number of classes                     bias=True)).to(device) <p>Nice!</p> <p>Output layer updated, let's get another summary of our model and see what's changed.</p> In\u00a0[15]: Copied! <pre># # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output) summary(model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)         verbose=0,         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) Out[15]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n============================================================================================================================================\nTotal params: 4,011,391\nTrainable params: 3,843\nNon-trainable params: 4,007,548\nTotal mult-adds (G): 12.31\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.09\nParams size (MB): 16.05\nEstimated Total Size (MB): 3487.41\n============================================================================================================================================</pre> <p>Ho, ho! There's a fair few changes here!</p> <p>Let's go through them:</p> <ul> <li>Trainable column - You'll see that many of the base layers (the ones in the <code>features</code> portion) have their Trainable value as <code>False</code>. This is because we set their attribute <code>requires_grad=False</code>. Unless we change this, these layers won't be updated during furture training.</li> <li>Output shape of <code>classifier</code> - The <code>classifier</code> portion of the model now has an Output Shape value of <code>[32, 3]</code> instead of <code>[32, 1000]</code>. It's Trainable value is also <code>True</code>. This means its parameters will be updated during training. In essence, we're using the <code>features</code> portion to feed our <code>classifier</code> portion a base representation of an image and then our <code>classifier</code> layer is going to learn how to base representation aligns with our problem.</li> <li>Less trainable parameters - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the <code>classifier</code> as trainable, there's now only 3,843 trainable parameters (even less than our TinyVGG model). Though there's also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our <code>classifier</code> layer.</li> </ul> <p>Note: The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem.</p> In\u00a0[16]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) <p>Wonderful!</p> <p>To train our model, we can use <code>train()</code> function we defined in the 05. PyTorch Going Modular section 04.</p> <p>The <code>train()</code> function is in the <code>engine.py</code> script inside the <code>going_modular</code> directory.</p> <p>Let's see how long it takes to train our model for 5 epochs.</p> <p>Note: We're only going to be training the parameters <code>classifier</code> here as all of the other parameters in our model have been frozen.</p> In\u00a0[17]: Copied! <pre># Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set the random seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Setup training and save the results results = engine.train(model=model,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=5,                        device=device)  # End the timer and print out how long it took end_time = timer() print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8717 | train_acc: 0.7773 | test_loss: 0.7912 | test_acc: 0.8153\nEpoch: 3 | train_loss: 0.7648 | train_acc: 0.7930 | test_loss: 0.7463 | test_acc: 0.8561\nEpoch: 4 | train_loss: 0.7108 | train_acc: 0.7539 | test_loss: 0.6372 | test_acc: 0.8655\nEpoch: 5 | train_loss: 0.6254 | train_acc: 0.7852 | test_loss: 0.6260 | test_acc: 0.8561\n[INFO] Total training time: 8.977 seconds\n</pre> <p>Wow!</p> <p>Our model trained quite fast (~5 seconds on my local machine with a NVIDIA TITAN RTX GPU/about 15 seconds on Google Colab with a NVIDIA P100 GPU).</p> <p>And it looks like it smashed our previous model results out of the park!</p> <p>With an <code>efficientnet_b0</code> backbone, our model achieves almost 85%+ accuracy on the test dataset, almost double what we were able to achieve with TinyVGG.</p> <p>Not bad for a model we downloaded with a few lines of code.</p> In\u00a0[18]: Copied! <pre># Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Plot the loss curves of our model\nplot_loss_curves(results)\n</pre> # Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it try:     from helper_functions import plot_loss_curves except:     print(\"[INFO] Couldn't find helper_functions.py, downloading...\")     with open(\"helper_functions.py\", \"wb\") as f:         import requests         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")         f.write(request.content)     from helper_functions import plot_loss_curves  # Plot the loss curves of our model plot_loss_curves(results) <p>Those are some excellent looking loss curves!</p> <p>It looks like the loss for both datasets (train and test) is heading in the right direction.</p> <p>The same with the accuracy values, trending upwards.</p> <p>That goes to show the power of transfer learning. Using a pretrained model often leads to pretty good results with a small amount of data in less time.</p> <p>I wonder what would happen if you tried to train the model for longer? Or if we added more data?</p> <p>Question: Looking at the loss curves, does our model look like it's overfitting or underfitting? Or perhaps neither? Hint: Check out notebook 04. PyTorch Custom Datasets part 8. What should an ideal loss curve look like? for ideas.</p> In\u00a0[19]: Copied! <pre>from typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Take in a trained model, class names, image path, image size, a transform and target device\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n</pre> from typing import List, Tuple  from PIL import Image  # 1. Take in a trained model, class names, image path, image size, a transform and target device def pred_and_plot_image(model: torch.nn.Module,                         image_path: str,                          class_names: List[str],                         image_size: Tuple[int, int] = (224, 224),                         transform: torchvision.transforms = None,                         device: torch.device=device):               # 2. Open image     img = Image.open(image_path)      # 3. Create transformation for image (if one doesn't exist)     if transform is not None:         image_transform = transform     else:         image_transform = transforms.Compose([             transforms.Resize(image_size),             transforms.ToTensor(),             transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225]),         ])      ### Predict on image ###       # 4. Make sure the model is on the target device     model.to(device)      # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():       # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])       transformed_image = image_transform(img).unsqueeze(dim=0)        # 7. Make a prediction on image with an extra dimension and send it to the target device       target_image_pred = model(transformed_image.to(device))      # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 9. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)      # 10. Plot image with predicted label and probability      plt.figure()     plt.imshow(img)     plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")     plt.axis(False); <p>What a good looking function!</p> <p>Let's test it out by making predictions on a few random images from the test set.</p> <p>We can get a list of all the test image paths using <code>list(Path(test_dir).glob(\"*/*.jpg\"))</code>, the stars in the <code>glob()</code> method say \"any file matching this pattern\", in other words, any file ending in <code>.jpg</code> (all of our images).</p> <p>And then we can randomly sample a number of these using Python's <code>random.sample(populuation, k)</code> where <code>population</code> is the sequence to sample and <code>k</code> is the number of samples to retrieve.</p> In\u00a0[20]: Copied! <pre># Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n</pre> # Get a random list of image paths from test set import random num_images_to_plot = 3 test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data  test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths                                        k=num_images_to_plot) # randomly select 'k' image paths to pred and plot  # Make predictions on and plot the images for image_path in test_image_path_sample:     pred_and_plot_image(model=model,                          image_path=image_path,                         class_names=class_names,                         # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights                         image_size=(224, 224)) <p>Woohoo!</p> <p>Those predictions look far better than the ones our TinyVGG model was previously making.</p> In\u00a0[21]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Two thumbs up!</p> <p>Looks like our model go it right again!</p> <p>But this time the prediction probability is higher than the one from TinyVGG (<code>0.373</code>) in 04. PyTorch Custom Datasets section 11.3.</p> <p>This indicates our <code>efficientnet_b0</code> model is more confident in its prediction where as our TinyVGG model was par with just guessing.</p>"},{"location":"06_pytorch_transfer_learning/#06-pytorch-transfer-learning","title":"06. PyTorch Transfer Learning\u00b6","text":"<p>Note: This notebook uses <code>torchvision</code>'s new multi-weight support API (available in <code>torchvision</code> v0.13+).</p> <p>We've built a few models by hand so far.</p> <p>But their performance has been poor.</p> <p>You might be thinking, is there a well-performing model that already exists for our problem?</p> <p>And in the world of deep learning, the answer is often yes.</p> <p>We'll see how by using a powerful technique called transfer learning.</p>"},{"location":"06_pytorch_transfer_learning/#what-is-transfer-learning","title":"What is transfer learning?\u00b6","text":"<p>Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.</p> <p>For example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.</p> <p>Or we could take the patterns from a language model (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.</p> <p>The premise remains: find a well-performing existing model and apply it to your own problem.</p> <p>Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.</p>"},{"location":"06_pytorch_transfer_learning/#why-use-transfer-learning","title":"Why use transfer learning?\u00b6","text":"<p>There are two main benefits to using transfer learning:</p> <ol> <li>Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.</li> <li>Can leverage a working model which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.</li> </ol> <p>We'll be putting these to the test for our FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.</p> <p>Both research and practice support the use of transfer learning too.</p> <p>A finding from a recent machine learning research paper recommended practioner's use transfer learning wherever possible.</p> <p>A study into the effects of whether training from scratch or using transfer learning was better from a practioner's point of view, found transfer learning to be far more beneficial in terms of cost and time. Source:* How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers paper section 6 (conclusion).*</p> <p>And Jeremy Howard (founder of fastai) is a big proponent of transfer learning.</p> <p>The things that really make a difference (transfer learning), if we can do better at transfer learning, it\u2019s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. \u2014 Jeremy Howard on the Lex Fridman Podcast</p>"},{"location":"06_pytorch_transfer_learning/#where-to-find-pretrained-models","title":"Where to find pretrained models\u00b6","text":"<p>The world of deep learning is an amazing place.</p> <p>So amazing that many people around the world share their work.</p> <p>Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.</p> <p>And there are several places you can find pretrained models to use for your own problems.</p> Location What's there? Link(s) PyTorch domain libraries Each of the PyTorch domain libraries (<code>torchvision</code>, <code>torchtext</code>) come with pretrained models of some form. The models there work right within PyTorch. <code>torchvision.models</code>, <code>torchtext.models</code>, <code>torchaudio.models</code>, <code>torchrec.models</code> HuggingFace Hub A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. https://huggingface.co/models, https://huggingface.co/datasets <code>timm</code> (PyTorch Image Models) library Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. https://github.com/rwightman/pytorch-image-models Paperswithcode A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. https://paperswithcode.com/ <p>With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"</p> <p>Exercise: Spend 5-minutes going through <code>torchvision.models</code> as well as the HuggingFace Hub Models page, what do you find? (there's no right answers here, it's just to practice exploring)</p>"},{"location":"06_pytorch_transfer_learning/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to take a pretrained model from <code>torchvision.models</code> and customise it to work on (and hopefully improve) our FoodVision Mini problem.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Get and customise a pretrained model Here we'll download a pretrained model from <code>torchvision.models</code> and customise it to our own problem. 4. Train model Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. 5. Evaluate the model by plotting loss curves How did our first transfer learning model go? Did it overfit or underfit? 6. Make predictions on images from the test set It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's visualize, visualize, visualize!"},{"location":"06_pytorch_transfer_learning/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"06_pytorch_transfer_learning/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>Let's get started by importing/downloading the required modules for this section.</p> <p>To save us writing extra code, we're going to be leveraging some of the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in the previous section, 05. PyTorch Going Modular.</p> <p>Specifically, we're going to download the <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>Note: As of June 2022, this notebook uses the nightly versions of <code>torch</code> and <code>torchvision</code> as <code>torchvision</code> v0.13+ is required for using the updated multi-weights API. You can install these using the command below.</p>"},{"location":"06_pytorch_transfer_learning/#1-get-data","title":"1. Get data\u00b6","text":"<p>Before we can start to use transfer learning, we'll need a dataset.</p> <p>To see how transfer learning compares to our previous attempts at model building, we'll download the same dataset we've been using for FoodVision Mini.</p> <p>Let's write some code to download the <code>pizza_steak_sushi.zip</code> dataset from the course GitHub and then unzip it.</p> <p>We can also make sure if we've already got the data, it doesn't redownload.</p>"},{"location":"06_pytorch_transfer_learning/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Since we've downloaded the <code>going_modular</code> directory, we can use the <code>data_setup.py</code> script we created in section 05. PyTorch Going Modular to prepare and setup our DataLoaders.</p> <p>But since we'll be using a pretrained model from <code>torchvision.models</code>, there's a specific transform we need to prepare our images first.</p>"},{"location":"06_pytorch_transfer_learning/#21-creating-a-transform-for-torchvisionmodels-manual-creation","title":"2.1 Creating a transform for <code>torchvision.models</code> (manual creation)\u00b6","text":"<p>Note: As of <code>torchvision</code> v0.13+, there's an update to how data transforms can be created using <code>torchvision.models</code>. I've called the previous method \"manual creation\" and the new method \"auto creation\". This notebook showcases both.</p> <p>When using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.</p> <p>Prior to <code>torchvision</code> v0.13+, to create a transform for a pretrained model in <code>torchvision.models</code>, the documentation stated:</p> <p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.</p> <p>The images have to be loaded in to a range of <code>[0, 1]</code> and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>.</p> <p>You can use the following transform to normalize:</p> <pre><code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</code></pre> <p>The good news is, we can achieve the above transformations with a combination of:</p> Transform number Transform required Code to perform transform 1 Mini-batches of size <code>[batch_size, 3, height, width]</code> where height and width are at least 224x224^. <code>torchvision.transforms.Resize()</code> to resize images into <code>[3, 224, 224]</code>^ and <code>torch.utils.data.DataLoader()</code> to create batches of images. 2 Values between 0 &amp; 1. <code>torchvision.transforms.ToTensor()</code> 3 A mean of <code>[0.485, 0.456, 0.406]</code> (values across each colour channel). <code>torchvision.transforms.Normalize(mean=...)</code> to adjust the mean of our images. 4 A standard deviation of <code>[0.229, 0.224, 0.225]</code> (values across each colour channel). <code>torchvision.transforms.Normalize(std=...)</code> to adjust the standard deviation of our images. <p>Note: ^some pretrained models from <code>torchvision.models</code> in different sizes to <code>[3, 224, 224]</code>, for example, some might take them in <code>[3, 240, 240]</code>. For specific input image sizes, see the documentation.</p> <p>Question: Where did the mean and standard deviation values come from? Why do we need to do this?</p> <p>These were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.</p> <p>We also don't need to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (they'll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.</p> <p>Let's compose a series of <code>torchvision.transforms</code> to perform the above steps.</p>"},{"location":"06_pytorch_transfer_learning/#22-creating-a-transform-for-torchvisionmodels-auto-creation","title":"2.2 Creating a transform for <code>torchvision.models</code> (auto creation)\u00b6","text":"<p>As previously stated, when using a pretrained model, it's important that your custom data going into the model is prepared in the same way as the original training data that went into the model.</p> <p>Above we saw how to manually create a transform for a pretrained model.</p> <p>But as of <code>torchvision</code> v0.13+, an automatic transform creation feature has been added.</p> <p>When you setup a model from <code>torchvision.models</code> and select the pretrained model weights you'd like to use, for example, say we'd like to use:</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n</pre> <p>Where,</p> <ul> <li><code>EfficientNet_B0_Weights</code> is the model architecture weights we'd like to use (there are many differnt model architecture options in <code>torchvision.models</code>).</li> <li><code>DEFAULT</code> means the best available weights (the best performance in ImageNet).<ul> <li>Note: Depending on the model architecture you choose, you may also see other options such as <code>IMAGENET_V1</code> and <code>IMAGENET_V2</code> where generally the higher version number the better. Though if you want the best available, <code>DEFAULT</code> is the easiest option. See the <code>torchvision.models</code> documentation for more.</li> </ul> </li> </ul> <p>Let's try it out.</p>"},{"location":"06_pytorch_transfer_learning/#3-getting-a-pretrained-model","title":"3. Getting a pretrained model\u00b6","text":"<p>Alright, here comes the fun part!</p> <p>Over the past few notebooks we've been building PyTorch neural networks from scratch.</p> <p>And while that's a good skill to have, our models haven't been performing as well as we'd like.</p> <p>That's where transfer learning comes in.</p> <p>The whole idea of transfer learning is to take an already well-performing model on a problem-space similar to yours and then customising it to your use case.</p> <p>Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in <code>torchvision.models</code>.</p> <p>Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:</p> Architecuture backbone Code ResNet's <code>torchvision.models.resnet18()</code>, <code>torchvision.models.resnet50()</code>... VGG (similar to what we used for TinyVGG) <code>torchvision.models.vgg16()</code> EfficientNet's <code>torchvision.models.efficientnet_b0()</code>, <code>torchvision.models.efficientnet_b1()</code>... VisionTransformer (ViT's) <code>torchvision.models.vit_b_16()</code>, <code>torchvision.models.vit_b_32()</code>... ConvNeXt <code>torchvision.models.convnext_tiny()</code>,  <code>torchvision.models.convnext_small()</code>... More available in <code>torchvision.models</code> <code>torchvision.models...</code>"},{"location":"06_pytorch_transfer_learning/#31-which-pretrained-model-should-you-use","title":"3.1 Which pretrained model should you use?\u00b6","text":"<p>It depends on your problem/the device you're working with.</p> <p>Generally, the higher number in the model name (e.g. <code>efficientnet_b0()</code> -&gt; <code>efficientnet_b1()</code> -&gt; <code>efficientnet_b7()</code>) means better performance but a larger model.</p> <p>You might think better performance is always better, right?</p> <p>That's true but some better performing models are too big for some devices.</p> <p>For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.</p> <p>But if you've got unlimited compute power, as The Bitter Lesson states, you'd likely take the biggest, most compute hungry model you can.</p> <p>Understanding this performance vs. speed vs. size tradeoff will come with time and practice.</p> <p>For me, I've found a nice balance in the <code>efficientnet_bX</code> models.</p> <p>As of May 2022, Nutrify (the machine learning powered app I'm working on) is powered by an <code>efficientnet_b0</code>.</p> <p>Comma.ai (a company that makes open source self-driving car software) uses an <code>efficientnet_b2</code> to learn a representation of the road.</p> <p>Note: Even though we're using <code>efficientnet_bX</code>, it's important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem.</p>"},{"location":"06_pytorch_transfer_learning/#32-setting-up-a-pretrained-model","title":"3.2 Setting up a pretrained model\u00b6","text":"<p>The pretrained model we're going to be using is <code>torchvision.models.efficientnet_b0()</code>.</p> <p>The architecture is from the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.</p> <p>Example of what we're going to create, a pretrained <code>EfficientNet_B0</code> model from <code>torchvision.models</code> with the output layer adjusted for our use case of classifying pizza, steak and sushi images.</p> <p>We can setup the <code>EfficientNet_B0</code> pretrained ImageNet weights using the same code as we used to create the transforms.</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights for ImageNet\n</pre> <p>This means the model has already been trained on millions of images and has a good base representation of image data.</p> <p>The PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNet's 1000 classes.</p> <p>We'll also send it to the target device.</p>"},{"location":"06_pytorch_transfer_learning/#33-getting-a-summary-of-our-model-with-torchinfosummary","title":"3.3 Getting a summary of our model with <code>torchinfo.summary()</code>\u00b6","text":"<p>To learn more about our model, let's use <code>torchinfo</code>'s <code>summary()</code> method.</p> <p>To do so, we'll pass in:</p> <ul> <li><code>model</code> - the model we'd like to get a summary of.</li> <li><code>input_size</code> - the shape of the data we'd like to pass to our model, for the case of <code>efficientnet_b0</code>, the input size is <code>(batch_size, 3, 224, 224)</code>, though other variants of <code>efficientnet_bX</code> have different input sizes.<ul> <li>Note: Many modern models can handle input images of varying sizes thanks to <code>torch.nn.AdaptiveAvgPool2d()</code>, this layer adaptively adjusts the <code>output_size</code> of a given input as required. You can try this out by passing different size input images to <code>summary()</code> or your models.</li> </ul> </li> <li><code>col_names</code> - the various information columns we'd like to see about our model.</li> <li><code>col_width</code> - how wide the columns should be for the summary.</li> <li><code>row_settings</code> - what features to show in a row.</li> </ul>"},{"location":"06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs","title":"3.4 Freezing the base model and changing the output layer to suit our needs\u00b6","text":"<p>The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the <code>features</code> section) and then adjust the output layers (also called head/classifier layers) to suit your needs.</p> <p>You can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original <code>torchvision.models.efficientnet_b0()</code> comes with <code>out_features=1000</code> because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need <code>out_features=3</code>.</p> <p>Let's freeze all of the layers/parameters in the <code>features</code> section of our <code>efficientnet_b0</code> model.</p> <p>Note: To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.</p> <p>We can freeze all of the layers/parameters in the <code>features</code> section by setting the attribute <code>requires_grad=False</code>.</p> <p>For parameters with <code>requires_grad=False</code>, PyTorch doesn't track gradient updates and in turn, these parameters won't be changed by our optimizer during training.</p> <p>In essence, a parameter with <code>requires_grad=False</code> is \"untrainable\" or \"frozen\" in place.</p>"},{"location":"06_pytorch_transfer_learning/#4-train-model","title":"4. Train model\u00b6","text":"<p>Now we've got a pretraiend model that's semi-frozen and has a customised <code>classifier</code>, how about we see transfer learning in action?</p> <p>To begin training, let's create a loss function and an optimizer.</p> <p>Because we're still working with multi-class classification, we'll use <code>nn.CrossEntropyLoss()</code> for the loss function.</p> <p>And we'll stick with <code>torch.optim.Adam()</code> as our optimizer with <code>lr=0.001</code>.</p>"},{"location":"06_pytorch_transfer_learning/#5-evaluate-model-by-plotting-loss-curves","title":"5. Evaluate model by plotting loss curves\u00b6","text":"<p>Our model looks like it's performing pretty well.</p> <p>Let's plot it's loss curves to see what the training looks like over time.</p> <p>We can plot the loss curves using the function <code>plot_loss_curves()</code> we created in 04. PyTorch Custom Datasets section 7.8.</p> <p>The function is stored in the <code>helper_functions.py</code> script so we'll try to import it and download the script if we don't have it.</p>"},{"location":"06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set","title":"6. Make predictions on images from the test set\u00b6","text":"<p>It looks like our model performs well quantitatively but how about qualitatively?</p> <p>Let's find out by making some predictions with our model on images from the test set (these aren't seen during training) and plotting them.</p> <p>Visualize, visualize, visualize!</p> <p>One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.</p> <p>This means we'll need to make sure our images have:</p> <ul> <li>Same shape - If our images are different shapes to what our model was trained on, we'll get shape errors.</li> <li>Same datatype - If our images are a different datatype (e.g. <code>torch.int8</code> vs. <code>torch.float32</code>) we'll get datatype errors.</li> <li>Same device - If our images are on a different device to our model, we'll get device errors.</li> <li>Same transformations - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.</li> </ul> <p>Note: These requirements go for all kinds of data if you're trying to make predictions with a trained model. Data you'd like to predict on should be in the same format as your model was trained on.</p> <p>To do all of this, we'll create a function <code>pred_and_plot_image()</code> to:</p> <ol> <li>Take in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.</li> <li>Open an image with <code>PIL.Image.open()</code>.</li> <li>Create a transform for the image (this will default to the <code>manual_transforms</code> we created above or it could use a transform generated from <code>weights.transforms()</code>).</li> <li>Make sure the model is on the target device.</li> <li>Turn on model eval mode with <code>model.eval()</code> (this turns off layers like <code>nn.Dropout()</code>, so they aren't used for inference) and the inference mode context manager.</li> <li>Transform the target image with the transform made in step 3 and add an extra batch dimension with <code>torch.unsqueeze(dim=0)</code> so our input image has shape <code>[batch_size, color_channels, height, width]</code>.</li> <li>Make a prediction on the image by passing it to the model ensuring it's on the target device.</li> <li>Convert the model's output logits to prediction probabilities with <code>torch.softmax()</code>.</li> <li>Convert model's prediction probabilities to prediction labels with <code>torch.argmax()</code>.</li> <li>Plot the image with <code>matplotlib</code> and set the title to the prediction label from step 9 and prediction probability from step 8.</li> </ol> <p>Note: This is a similar function to 04. PyTorch Custom Datasets section 11.3's <code>pred_and_plot_image()</code> with a few tweaked steps.</p>"},{"location":"06_pytorch_transfer_learning/#61-making-predictions-on-a-custom-image","title":"6.1 Making predictions on a custom image\u00b6","text":"<p>It looks like our model does well qualitatively on data from the test set.</p> <p>But how about on our own custom image?</p> <p>That's where the real fun of machine learning is!</p> <p>Predicting on your own custom data, outisde of any training or test set.</p> <p>To test our model on a custom image, let's import the old faithful <code>pizza-dad.jpeg</code> image (an image of my dad eating pizza).</p> <p>We'll then pass it to the <code>pred_and_plot_image()</code> function we created above and see what happens.</p>"},{"location":"06_pytorch_transfer_learning/#main-takeaways","title":"Main takeaways\u00b6","text":"<ul> <li>Transfer learning often allows to you get good results with a relatively small amount of custom data.</li> <li>Knowing the power of transfer learning, it's a good idea to ask at the start of every problem, \"does an existing well-performing model exist for my problem?\"</li> <li>When using a pretrained model, it's important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.</li> <li>The same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.</li> <li>There are several different places to find pretrained models from the PyTorch domain libraries, HuggingFace Hub and libraries such as <code>timm</code> (PyTorch Image Models).</li> </ul>"},{"location":"06_pytorch_transfer_learning/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 06</li> <li>Example solutions notebook for 06 (try the exercises before looking at this)<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all)</li> </ul> </li> </ul> <ol> <li>Make predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels. Check out 03. PyTorch Computer Vision section 10 for ideas.</li> <li>Get the \"most wrong\" of the predictions on the test dataset and plot the 5 \"most wrong\" images. You can do this by:<ul> <li>Predicting across all of the test dataset, storing the labels and predicted probabilities.</li> <li>Sort the predictions by wrong prediction and then descending predicted probabilities, this will give you the wrong predictions with the highest prediction probabilities, in other words, the \"most wrong\".</li> <li>Plot the top 5 \"most wrong\" images, why do you think the model got these wrong?</li> </ul> </li> <li>Predict on your own image of pizza/steak/sushi - how does the model go? What happens if you predict on an image that isn't pizza/steak/sushi?</li> <li>Train the model from section 4 above for longer (10 epochs should do), what happens to the performance?</li> <li>Train the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.<ul> <li>You can find the 20% Pizza, Steak, Sushi dataset on the course GitHub. It was created with the notebook <code>extras/04_custom_data_creation.ipynb</code>.</li> </ul> </li> <li>Try a different model from <code>torchvision.models</code> on the Pizza, Steak, Sushi data, how does this model perform?<ul> <li>You'll have to change the size of the classifier layer to suit our problem.</li> <li>You may want to try an EfficientNet with a higher number than our B0, perhaps <code>torchvision.models.efficientnet_b2()</code>?</li> </ul> </li> </ol>"},{"location":"06_pytorch_transfer_learning/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Look up what \"model fine-tuning\" is and spend 30-minutes researching different methods to perform it with PyTorch. How would we change our code to fine-tine? Tip: fine-tuning usually works best if you have lots of custom data, where as, feature extraction is typically better if you have less custom data.</li> <li>Check out the new/upcoming PyTorch multi-weights API (still in beta at time of writing, May 2022), it's a new way to perform transfer learning in PyTorch. What changes to our code would need to made to use the new API?</li> <li>Try to create your own classifier on two classes of images, for example, you could collect 10 photos of your dog and your friends dog and train a model to classify the two dogs. This would be a good way to practice creating a dataset as well as building a model on that dataset.</li> </ul>"},{"location":"07_pytorch_experiment_tracking/","title":"07. PyTorch Experiment Tracking","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> <p>Note: If you're using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> (0.12+) and <code>torchvision</code> (0.13+).</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>Now let's setup device agnostic code.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Set seeds\ndef set_seeds(seed: int=42):\n\"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n</pre> # Set seeds def set_seeds(seed: int=42):     \"\"\"Sets random sets for torch operations.      Args:         seed (int, optional): Random seed to set. Defaults to 42.     \"\"\"     # Set the seed for general torch operations     torch.manual_seed(seed)     # Set the seed for CUDA torch operations (ones that happen on the GPU)     torch.cuda.manual_seed(seed) In\u00a0[5]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n\"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    Returns:\n        pathlib.Path to downloaded data.\n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> import os import zipfile  from pathlib import Path  import requests  def download_data(source: str,                    destination: str,                   remove_source: bool = True) -&gt; Path:     \"\"\"Downloads a zipped dataset from source and unzips to destination.      Args:         source (str): A link to a zipped file containing data.         destination (str): A target directory to unzip data to.         remove_source (bool): Whether to remove the source after downloading and extracting.          Returns:         pathlib.Path to downloaded data.          Example usage:         download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                       destination=\"pizza_steak_sushi\")     \"\"\"     # Setup path to data folder     data_path = Path(\"data/\")     image_path = data_path / destination      # If the image folder doesn't exist, download it and prepare it...      if image_path.is_dir():         print(f\"[INFO] {image_path} directory exists, skipping download.\")     else:         print(f\"[INFO] Did not find {image_path} directory, creating one...\")         image_path.mkdir(parents=True, exist_ok=True)                  # Download pizza, steak, sushi data         target_file = Path(source).name         with open(data_path / target_file, \"wb\") as f:             request = requests.get(source)             print(f\"[INFO] Downloading {target_file} from {source}...\")             f.write(request.content)          # Unzip pizza, steak, sushi data         with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:             print(f\"[INFO] Unzipping {target_file} data...\")              zip_ref.extractall(image_path)          # Remove .zip file         if remove_source:             os.remove(data_path / target_file)          return image_path  image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[5]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>Excellent! Looks like we've got our pizza, steak and sushi images in standard image classification format ready to go.</p> In\u00a0[6]: Copied! <pre># Setup directories\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup directories train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])            print(f\"Manually created transforms: {manual_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n</pre> Out[6]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[7]: Copied! <pre># Setup dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup pretrained weights (plenty of these available in torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Get transforms from weights (these are the transforms that were used to obtain the weights)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup pretrained weights (plenty of these available in torchvision.models) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # Get transforms from weights (these are the transforms that were used to obtain the weights) automatic_transforms = weights.transforms()  print(f\"Automatically created transforms: {automatic_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=automatic_transforms, # use automatic created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Automatically created transforms: ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n</pre> Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n\n# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Setup the model with the pretrained weights and send it to the target device\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# View the output of the model\n# model\n</pre> # Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions. # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD   # Download the pretrained weights for EfficientNet_B0 weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"  # Setup the model with the pretrained weights and send it to the target device model = torchvision.models.efficientnet_b0(weights=weights).to(device)  # View the output of the model # model <p>Wonderful!</p> <p>Now we've got a pretrained model let's turn into a feature extractor model.</p> <p>In essence, we'll freeze the base layers of the model (we'll use these to extract features from our input images) and we'll change the classifier head (output layer) to suit the number of classes we're working with (we've got 3 classes: pizza, steak, sushi).</p> <p>Note: The idea of creating a feature extractor model (what we're doing here) was covered in more depth in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model.</p> In\u00a0[9]: Copied! <pre># Freeze all base layers by setting requires_grad attribute to False\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Since we're creating a new layer with random weights (torch.nn.Linear), \n# let's set the seeds\nset_seeds() \n\n# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n</pre> # Freeze all base layers by setting requires_grad attribute to False for param in model.features.parameters():     param.requires_grad = False      # Since we're creating a new layer with random weights (torch.nn.Linear),  # let's set the seeds set_seeds()   # Update the classifier head to suit our problem model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features=1280,                out_features=len(class_names),               bias=True).to(device)) <p>Base layers frozen, classifier head changed, let's get a summary of our model with <code>torchinfo.summary()</code>.</p> In\u00a0[10]: Copied! <pre>from torchinfo import summary\n\n# # Get a summary of the model (uncomment for full output)\n# summary(model, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n#         verbose=0,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Get a summary of the model (uncomment for full output) # summary(model,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width) #         verbose=0, #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>Output of <code>torchinfo.summary()</code> with our feature extractor EffNetB0 model, notice how the base layers are frozen (not trainable) and the output layers are customized to our own problem.</p> In\u00a0[11]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[12]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n\n# Create a writer with all default settings\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter  # Create a writer with all default settings writer = SummaryWriter() <p>Now to use the writer, we could write a new training loop or we could adjust the existing <code>train()</code> function we created in 05. PyTorch Going Modular section 4.</p> <p>Let's take the latter option.</p> <p>We'll get the <code>train()</code> function from <code>engine.py</code> and adjust it to use <code>writer</code>.</p> <p>Specifically, we'll add the ability for our <code>train()</code> function to log our model's training and test loss and accuracy values.</p> <p>We can do this with <code>writer.add_scalars(main_tag, tag_scalar_dict)</code>, where:</p> <ul> <li><code>main_tag</code> (string) - the name for the scalars being tracked (e.g. \"Accuracy\")</li> <li><code>tag_scalar_dict</code> (dict) - a dictionary of the values being tracked (e.g. <code>{\"train_loss\": 0.3454}</code>)<ul> <li> Note: The method is called <code>add_scalars()</code> because our loss and accuracy values are generally scalars (single values). </li> </ul> </li> </ul> <p>Once we've finished tracking values, we'll call <code>writer.close()</code> to tell the <code>writer</code> to stop looking for values to track.</p> <p>To start modifying <code>train()</code> we'll also import <code>train_step()</code> and <code>test_step()</code> from <code>engine.py</code>.</p> <p>Note: You can track information about your model almost anywhere in your code. But quite often experiments will be tracked while a model is training (inside a training/testing loop).</p> <p>The <code>torch.utils.tensorboard.SummaryWriter()</code> class also has many different methods to track different things about your model/data, such as <code>add_graph()</code> which tracks the computation graph of your model. For more options, check the <code>SummaryWriter()</code> documentation.</p> In\u00a0[13]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Import train() function from: \n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  from going_modular.going_modular.engine import train_step, test_step  # Import train() function from:  # https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").            Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer,                                            device=device)         test_loss, test_acc = test_step(model=model,                                         dataloader=test_dataloader,                                         loss_fn=loss_fn,                                         device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          ### New: Experiment tracking ###         # Add loss results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)          # Add accuracy results to SummaryWriter         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)                  # Track the PyTorch model architecture         writer.add_graph(model=model,                           # Pass in an example input                          input_to_model=torch.randn(32, 3, 224, 224).to(device))          # Close the writer     writer.close()          ### End new ###      # Return the filled results at the end of the epochs     return results <p>Woohoo!</p> <p>Our <code>train()</code> function is now updated to use a <code>SummaryWriter()</code> instance to track our model's results.</p> <p>How about we try it out for 5 epochs?</p> In\u00a0[14]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated to use writer\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated to use writer set_seeds() results = train(model=model,                 train_dataloader=train_dataloader,                 test_dataloader=test_dataloader,                 optimizer=optimizer,                 loss_fn=loss_fn,                 epochs=5,                 device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561\nEpoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864\nEpoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049\nEpoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737\n</pre> <p>Note: You might notice the results here are slightly different to what our model got in 06. PyTorch Transfer Learning. The difference comes from using the <code>engine.train()</code> and our modified <code>train()</code> function. Can you guess why? The PyTorch documentation on randomness may help more.</p> <p>Running the cell above we get similar outputs we got in 06. PyTorch Transfer Learning section 4: Train model but the difference is behind the scenes our <code>writer</code> instance has created a <code>runs/</code> directory storing our model's results.</p> <p>For example, the save location might look like:</p> <pre><code>runs/Jun21_00-46-03_daniels_macbook_pro\n</code></pre> <p>Where the default format is <code>runs/CURRENT_DATETIME_HOSTNAME</code>.</p> <p>We'll check these out in a second but just as a reminder, we were previously tracking our model's results in a dictionary.</p> In\u00a0[15]: Copied! <pre># Check out the model results\nresults\n</pre> # Check out the model results results Out[15]: <pre>{'train_loss': [1.0923754647374153,\n  0.8974628075957298,\n  0.803724929690361,\n  0.6769256368279457,\n  0.7064960040152073],\n 'train_acc': [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],\n 'test_loss': [0.9132757981618246,\n  0.7837507526079813,\n  0.6722926497459412,\n  0.6698453426361084,\n  0.6746167540550232],\n 'test_acc': [0.5397727272727273,\n  0.8560606060606061,\n  0.8863636363636364,\n  0.8049242424242425,\n  0.7736742424242425]}</pre> <p>Hmmm, we could format this to be a nice plot but could you imagine keeping track of a bunch of these dictionaries?</p> <p>There has to be a better way...</p> In\u00a0[16]: Copied! <pre># Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out) # %load_ext tensorboard # %tensorboard --logdir runs <p>If all went correctly, you should see something like the following:</p> <p>Viewing a single modelling experiment's results for accuracy and loss in TensorBoard.</p> <p>Note: For more information on running TensorBoard in notebooks or in other locations, see the following:</p> <ul> <li>Using TensorBoard in Notebooks guide by TensorFlow</li> <li>Get started with TensorBoard.dev (helpful for uploading your TensorBoard logs to a shareable link)</li> </ul> In\u00a0[17]: Copied! <pre>def create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n\"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n</pre> def create_writer(experiment_name: str,                    model_name: str,                    extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():     \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.      log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.      Where timestamp is the current date in YYYY-MM-DD format.      Args:         experiment_name (str): Name of experiment.         model_name (str): Name of model.         extra (str, optional): Anything extra to add to the directory. Defaults to None.      Returns:         torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.      Example usage:         # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"         writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb2\",                                extra=\"5_epochs\")         # The above is the same as:         writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")     \"\"\"     from datetime import datetime     import os      # Get timestamp of current date (all experiments on certain day live in same folder)     timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format      if extra:         # Create log directory path         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)     else:         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)              print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")     return SummaryWriter(log_dir=log_dir) <p>Beautiful!</p> <p>Now we've got a <code>create_writer()</code> function, let's try it out.</p> In\u00a0[18]: Copied! <pre># Create an example writer\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n</pre> # Create an example writer example_writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb0\",                                extra=\"5_epochs\") <pre>[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <p>Looking good, now we've got a way to log and trace back our various experiments.</p> In\u00a0[19]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Add writer parameter to train()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  # Add writer parameter to train() def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,            writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer           ) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Stores metrics to specified writer log_dir if present.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").       writer: A SummaryWriter() instance to log model results to.      Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                           dataloader=train_dataloader,                                           loss_fn=loss_fn,                                           optimizer=optimizer,                                           device=device)         test_loss, test_acc = test_step(model=model,           dataloader=test_dataloader,           loss_fn=loss_fn,           device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)           ### New: Use the writer parameter to track experiments ###         # See if there's a writer, if so, log to it         if writer:             # Add results to SummaryWriter             writer.add_scalars(main_tag=\"Loss\",                                 tag_scalar_dict={\"train_loss\": train_loss,                                                 \"test_loss\": test_loss},                                global_step=epoch)             writer.add_scalars(main_tag=\"Accuracy\",                                 tag_scalar_dict={\"train_acc\": train_acc,                                                 \"test_acc\": test_acc},                                 global_step=epoch)              # Close the writer             writer.close()         else:             pass     ### End new ###      # Return the filled results at the end of the epochs     return results In\u00a0[20]: Copied! <pre># Download 10 percent and 20 percent training data (if necessary)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n</pre> # Download 10 percent and 20 percent training data (if necessary) data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                                      destination=\"pizza_steak_sushi\")  data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\") <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n</pre> <p>Data downloaded!</p> <p>Now let's setup the filepaths to data we'll be using for the different experiments.</p> <p>We'll create different training directory paths but we'll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%).</p> In\u00a0[21]: Copied! <pre># Setup training directory paths\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Setup testing directory paths (note: use the same test dataset for both to compare the results)\ntest_dir = data_10_percent_path / \"test\"\n\n# Check the directories\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n</pre> # Setup training directory paths train_dir_10_percent = data_10_percent_path / \"train\" train_dir_20_percent = data_20_percent_path / \"train\"  # Setup testing directory paths (note: use the same test dataset for both to compare the results) test_dir = data_10_percent_path / \"test\"  # Check the directories print(f\"Training directory 10%: {train_dir_10_percent}\") print(f\"Training directory 20%: {train_dir_20_percent}\") print(f\"Testing directory: {test_dir}\") <pre>Training directory 10%: data/pizza_steak_sushi/train\nTraining directory 20%: data/pizza_steak_sushi_20_percent/train\nTesting directory: data/pizza_steak_sushi/test\n</pre> In\u00a0[22]: Copied! <pre>from torchvision import transforms\n\n# Create a transform to normalize data distribution to be inline with ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose transforms into a pipeline\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n</pre> from torchvision import transforms  # Create a transform to normalize data distribution to be inline with ImageNet normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]                                  std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]  # Compose transforms into a pipeline simple_transform = transforms.Compose([     transforms.Resize((224, 224)), # 1. Resize the images     transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1     normalize # 3. Normalize the images so their distributions match the ImageNet dataset  ]) <p>Transform ready!</p> <p>Now let's create our DataLoaders using the <code>create_dataloaders()</code> function from <code>data_setup.py</code> we created in 05. PyTorch Going Modular section 2.</p> <p>We'll create the DataLoaders with a batch size of 32.</p> <p>For all of our experiments we'll be using the same <code>test_dataloader</code> (to keep comparisons consistent).</p> In\u00a0[23]: Copied! <pre>BATCH_SIZE = 32\n\n# Create 10% training and test DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create 20% training and test data DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n</pre> BATCH_SIZE = 32  # Create 10% training and test DataLoaders train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,     test_dir=test_dir,      transform=simple_transform,     batch_size=BATCH_SIZE )  # Create 20% training and test data DataLoders train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,     test_dir=test_dir,     transform=simple_transform,     batch_size=BATCH_SIZE )  # Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments) print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\") print(f\"Number of classes: {len(class_names)}, class names: {class_names}\") <pre>Number of batches of size 32 in 10 percent training data: 8\nNumber of batches of size 32 in 20 percent training data: 15\nNumber of batches of size 32 in testing data: 8 (all experiments will use the same test set)\nNumber of classes: 3, class names: ['pizza', 'steak', 'sushi']\n</pre> In\u00a0[24]: Copied! <pre>import torchvision\nfrom torchinfo import summary\n\n# 1. Create an instance of EffNetB2 with pretrained weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n# 3. Get the number of in_features of the EfficientNetB2 classifier layer\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n</pre> import torchvision from torchinfo import summary  # 1. Create an instance of EffNetB2 with pretrained weights effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)  # # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )   # 3. Get the number of in_features of the EfficientNetB2 classifier layer print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\") <pre>Number of in_features to final layer of EfficientNetB2: 1408\n</pre> <p>Model summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.</p> <p>Now we know the required number of <code>in_features</code> for the EffNetB2 model, let's create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.</p> <p>We want these functions to:</p> <ol> <li>Get the base model from <code>torchvision.models</code></li> <li>Freeze the base layers in the model (set <code>requires_grad=False</code>)</li> <li>Set the random seeds (we don't need to do this but since we're running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment)</li> <li>Change the classifier head (to suit our problem)</li> <li>Give the model a name (e.g. \"effnetb0\" for EffNetB0)</li> </ol> In\u00a0[25]: Copied! <pre>import torchvision\nfrom torch import nn\n\n# Get num out features (one for each class pizza, steak, sushi)\nOUT_FEATURES = len(class_names)\n\n# Create an EffNetB0 feature extractor\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Create an EffNetB2 feature extractor\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n</pre> import torchvision from torch import nn  # Get num out features (one for each class pizza, steak, sushi) OUT_FEATURES = len(class_names)  # Create an EffNetB0 feature extractor def create_effnetb0():     # 1. Get the base mdoel with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT     model = torchvision.models.efficientnet_b0(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.2),         nn.Linear(in_features=1280, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb0\"     print(f\"[INFO] Created new {model.name} model.\")     return model  # Create an EffNetB2 feature extractor def create_effnetb2():     # 1. Get the base model with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     model = torchvision.models.efficientnet_b2(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.3),         nn.Linear(in_features=1408, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb2\"     print(f\"[INFO] Created new {model.name} model.\")     return model <p>Those are some nice looking functions!</p> <p>Let's test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their <code>summary()</code>.</p> In\u00a0[26]: Copied! <pre>effnetb0 = create_effnetb0() \n\n# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n# summary(model=effnetb0, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> effnetb0 = create_effnetb0()   # Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output) # summary(model=effnetb0,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb0 model.\n</pre> <p>Model summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</p> In\u00a0[27]: Copied! <pre>effnetb2 = create_effnetb2()\n\n# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> effnetb2 = create_effnetb2()  # Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb2 model.\n</pre> <p>Model summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).</p> <p>Looking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.</p> Model Total parameters (before freezing/changing head) Total parameters (after freezing/changing head) Total trainable parameters (after freezing/changing head) EfficientNetB0 5,288,548 4,011,391 3,843 EfficientNetB2 9,109,994 7,705,221 4,227 <p>This gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.</p> <p>However, the trainable parameters for each model (the classifier heads) aren't very different.</p> <p>Will these extra parameters lead to better results?</p> <p>We'll have to wait and see...</p> <p>Note: In the spirit of experimenting, you really could try almost any model from <code>torchvision.models</code> in a similar fashion to what we're doing here. I've only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like <code>torchvision.models.convnext_tiny()</code> or <code>torchvision.models.convnext_small()</code> into the mix.</p> In\u00a0[28]: Copied! <pre># 1. Create epochs list\nnum_epochs = [5, 10]\n\n# 2. Create models list (need to create a new model for each experiment)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Create dataloaders dictionary for various dataloaders\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n</pre> # 1. Create epochs list num_epochs = [5, 10]  # 2. Create models list (need to create a new model for each experiment) models = [\"effnetb0\", \"effnetb2\"]  # 3. Create dataloaders dictionary for various dataloaders train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,                      \"data_20_percent\": train_dataloader_20_percent} <p>Lists and dictionary created!</p> <p>Now we can write code to iterate through each of the different options and try out each of the different combinations.</p> <p>We'll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.</p> <p>Specifically, let's go through the following steps:</p> <ol> <li>Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results).</li> <li>Keep track of different experiment numbers (this is mostly for pretty print outs).</li> <li>Loop through the <code>train_dataloaders</code> dictionary items for each of the different training DataLoaders.</li> <li>Loop through the list of epoch numbers.</li> <li>Loop through the list of different model names.</li> <li>Create information print outs for the current running experiment (so we know what's happening).</li> <li>Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint).</li> <li>Create a new loss function (<code>torch.nn.CrossEntropyLoss()</code>) and optimizer (<code>torch.optim.Adam(params=model.parameters(), lr=0.001)</code>) for each new experiment.</li> <li>Train the model with the modified <code>train()</code> function passing the appropriate details to the <code>writer</code> parameter.</li> <li>Save the trained model with an appropriate file name to file with <code>save_model()</code> from <code>utils.py</code>.</li> </ol> <p>We can also use the <code>%%time</code> magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.</p> <p>Let's do it!</p> In\u00a0[29]: Copied! <pre>%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Set the random seeds\nset_seeds(seed=42)\n\n# 2. Keep track of experiment numbers\nexperiment_number = 0\n\n# 3. Loop through each DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n</pre> %%time from going_modular.going_modular.utils import save_model  # 1. Set the random seeds set_seeds(seed=42)  # 2. Keep track of experiment numbers experiment_number = 0  # 3. Loop through each DataLoader for dataloader_name, train_dataloader in train_dataloaders.items():      # 4. Loop through each number of epochs     for epochs in num_epochs:           # 5. Loop through each model name and create a new model based on the name         for model_name in models:              # 6. Create information print outs             experiment_number += 1             print(f\"[INFO] Experiment number: {experiment_number}\")             print(f\"[INFO] Model: {model_name}\")             print(f\"[INFO] DataLoader: {dataloader_name}\")             print(f\"[INFO] Number of epochs: {epochs}\")                # 7. Select the model             if model_name == \"effnetb0\":                 model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)             else:                 model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)                          # 8. Create a new loss and optimizer for every model             loss_fn = nn.CrossEntropyLoss()             optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)              # 9. Train target model with target dataloaders and track experiments             train(model=model,                   train_dataloader=train_dataloader,                   test_dataloader=test_dataloader,                    optimizer=optimizer,                   loss_fn=loss_fn,                   epochs=epochs,                   device=device,                   writer=create_writer(experiment_name=dataloader_name,                                        model_name=model_name,                                        extra=f\"{epochs}_epochs\"))                          # 10. Save the model to file so we can get back the best model             save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"             save_model(model=model,                        target_dir=\"models\",                        model_name=save_filepath)             print(\"-\"*50 + \"\\n\") <pre>[INFO] Experiment number: 1\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 2\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 3\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\nEpoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864\nEpoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864\nEpoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759\nEpoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873\nEpoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 4\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\nEpoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769\nEpoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977\nEpoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466\nEpoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 5\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 6\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 7\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\nEpoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072\nEpoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072\nEpoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873\nEpoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280\nEpoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 8\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\nEpoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977\nEpoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384\nEpoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280\nEpoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\nCPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s\nWall time: 2min 33s\n</pre> In\u00a0[30]: Copied! <pre># Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance) # %load_ext tensorboard # %tensorboard --logdir runs <p>Running the cell above we should get an output similar to the following.</p> <p>Note: Depending on the random seeds you used/hardware you used there's a chance your numbers aren't exactly the same as what's here. This is okay. It's due to the inheret randomness of deep learning. What matters most is the trend. Where your numbers are heading. If they're off by a large amount, perhaps there's something wrong and best to go back and check the code. But if they're off by a small amount (say a couple of decimal places or so), that's okay.</p> <p>Visualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB2 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.</p> <p>You can also upload your TensorBoard experiment results to tensorboard.dev to host them publically for free.</p> <p>For example, running code similiar to the following:</p> In\u00a0[31]: Copied! <pre># # Upload the results to TensorBoard.dev (uncomment to try it out)\n# !tensorboard dev upload --logdir runs \\\n#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n#     --description \"Comparing results of different model size, training data amount and training time.\"\n</pre> # # Upload the results to TensorBoard.dev (uncomment to try it out) # !tensorboard dev upload --logdir runs \\ #     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\ #     --description \"Comparing results of different model size, training data amount and training time.\" <p>Running the cell above results in the experiments from this notebook being publically viewable at: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/</p> <p>Note: Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they don't contain sensitive information.</p> In\u00a0[32]: Copied! <pre># Setup the best model filepath\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\nbest_model = create_effnetb2()\n\n# Load the saved best model state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n</pre> # Setup the best model filepath best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"  # Instantiate a new instance of EffNetB2 (to load the saved state_dict() to) best_model = create_effnetb2()  # Load the saved best model state_dict() best_model.load_state_dict(torch.load(best_model_path)) <pre>[INFO] Created new effnetb2 model.\n</pre> Out[32]: <pre>&lt;All keys matched successfully&gt;</pre> <p>Best model loaded!</p> <p>While we're here, let's check its filesize.</p> <p>This is an important consideration later on when deploying the model (incorporating it in an app).</p> <p>If the model is too large, it can be hard to deploy.</p> In\u00a0[33]: Copied! <pre># Check the model file size\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n</pre> # Check the model file size from pathlib import Path  # Get the model size in bytes then convert to megabytes effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024) print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\") <pre>EfficientNetB2 feature extractor model size: 29 MB\n</pre> <p>Looks like our best model so far is 29 MB in size. We'll keep this in mind if we wanted to deploy it later on.</p> <p>Time to make and visualize some predictions.</p> <p>We created a <code>pred_and_plot_image()</code> function use a trained model to make predictions on an image in 06. PyTorch Transfer Learning section 6.</p> <p>And we can reuse this function by importing it from <code>going_modular.going_modular.predictions.py</code> (I put the <code>pred_and_plot_image()</code> function in a script so we could reuse it).</p> <p>So to make predictions on various images the model hasn't seen before, we'll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then we'll randomly select a subset of these filepaths to pass to our <code>pred_and_plot_image()</code> function.</p> In\u00a0[34]: Copied! <pre># Import function to make predictions on images and plot them \n# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Get a random list of 3 images from 20% test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterate through random test image paths, make predictions on them and plot them\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n</pre> # Import function to make predictions on images and plot them  # See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set from going_modular.going_modular.predictions import pred_and_plot_image  # Get a random list of 3 images from 20% test set import random num_images_to_plot = 3 test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset test_image_path_sample = random.sample(population=test_image_path_list,                                        k=num_images_to_plot) # randomly select k number of images  # Iterate through random test image paths, make predictions on them and plot them for image_path in test_image_path_sample:     pred_and_plot_image(model=best_model,                         image_path=image_path,                         class_names=class_names,                         image_size=(224, 224)) <p>Nice!</p> <p>Running the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models we've built.</p> <p>This suggests the model is more confident in the decisions it's making.</p> In\u00a0[35]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = Path(\"data/04-pizza-dad.jpeg\")  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Woah!</p> <p>Two thumbs again!</p> <p>Our best model predicts \"pizza\" correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in 06. PyTorch Transfer Learning section 6.1.</p> <p>This again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.</p> <p>I wonder what could improve our model's performance even further?</p> <p>I'll leave that as a challenge for you to investigate.</p>"},{"location":"07_pytorch_experiment_tracking/#07-pytorch-experiment-tracking","title":"07. PyTorch Experiment Tracking\u00b6","text":"<p>Note: This notebook uses <code>torchvision</code>'s new multi-weight support API (available in <code>torchvision</code> v0.13+).</p> <p>We've trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).</p> <p>And so far we've keep track of them via Python dictionaries.</p> <p>Or just comparing them by the metric print outs during training.</p> <p>What if you wanted to run a dozen (or more) different models at once?</p> <p>Surely there's a better way...</p> <p>There is.</p> <p>Experiment tracking.</p> <p>And since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.</p> <p>So welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.</p> <p>We're going to answer the question: how do I track my machine learning experiments?</p>"},{"location":"07_pytorch_experiment_tracking/#what-is-experiment-tracking","title":"What is experiment tracking?\u00b6","text":"<p>Machine learning and deep learning are very experimental.</p> <p>You have to put on your artist's beret/chef's hat to cook up lots of different models.</p> <p>And you have to put on your scientist's coat to track the results of various combinations of data, model architectures and training regimes.</p> <p>That's where experiment tracking comes in.</p> <p>If you're running lots of different experiments, experiment tracking helps you figure out what works and what doesn't.</p>"},{"location":"07_pytorch_experiment_tracking/#why-track-experiments","title":"Why track experiments?\u00b6","text":"<p>If you're only running a handful of models (like we've done so far), it might be okay just to track their results in print outs and a few dictionaries.</p> <p>However, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.</p> <p>So if you're following the machine learning practitioner's motto of experiment, experiment, experiment!, you'll want a way to track them.</p> <p>After building a few models and tracking their results, you'll start to notice how quickly it can get out of hand.</p>"},{"location":"07_pytorch_experiment_tracking/#different-ways-to-track-machine-learning-experiments","title":"Different ways to track machine learning experiments\u00b6","text":"<p>There are as many different ways to track machine learning experiments as there is experiments to run.</p> <p>This table covers a few.</p> Method Setup Pros Cons Cost Python dictionaries, CSV files, print outs None Easy to setup, runs in pure Python Hard to keep track of large numbers of experiments Free TensorBoard Minimal, install <code>tensorboard</code> Extensions built into PyTorch, widely recognized and used, easily scales. User-experience not as nice as other options. Free Weights &amp; Biases Experiment Tracking Minimal, install <code>wandb</code>, make an account Incredible user experience, make experiments public, tracks almost anything. Requires external resource outside of PyTorch. Free for personal use MLFlow Minimal, install <code>mlflow</code> and starting tracking Fully open-source MLOps lifecycle management, many integrations. Little bit harder to setup a remote tracking server than other services. Free <p>Various places and techniques you can use to track your machine learning experiments. Note:* There are various other options similar to Weights &amp; Biases and open-source options similar to MLflow but I've left them out for brevity. You can find more by searching \"machine learning experiment tracking\".*</p>"},{"location":"07_pytorch_experiment_tracking/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.</p> <p>And due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.</p> <p>However, the principles we're going to cover are similar across all of the other tools for experiment tracking.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our FoodVision Mini model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Get and customise a pretrained model Just like the last section, 06. PyTorch Transfer Learning we'll download a pretrained model from <code>torchvision.models</code> and customise it to our own problem. 4. Train model amd track results Let's see what it's like to train and track the training results of a single model using TensorBoard. 5. View our model's results in TensorBoard Previously we visualized our model's loss curves with a helper function, now let's see what they look like in TensorBoard. 6. Creating a helper function to track experiments If we're going to be adhering to the machine learner practitioner's motto of experiment, experiment, experiment!, we best create a function that will help us save our modelling experiment results. 7. Setting up a series of modelling experiments Instead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times. 8. View modelling experiments in TensorBoard By this stage we'll have run eight modelling experiments in one go, a fair bit to keep track of, let's what their results look like in TensorBoard. 9. Load in the best model and make predictions with it The point of experiment tracking is to figure out which model performs the best, let's load in the best performing model and make some predictions with it to visualize, visualize, visualize!."},{"location":"07_pytorch_experiment_tracking/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"07_pytorch_experiment_tracking/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>Let's start by downloading all of the modules we'll need for this section.</p> <p>To save us writing extra code, we're going to be leveraging some of the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in section, 05. PyTorch Going Modular.</p> <p>Specifically, we're going to download the <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us visual summaries of our model(s).</p> <p>And since we're using a newer version of the <code>torchvision</code> package (v0.13 as of June 2022), we'll make sure we've got the latest versions.</p>"},{"location":"07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds","title":"Create a helper function to set seeds\u00b6","text":"<p>Since we've been setting random seeds a whole bunch throughout previous sections, how about we functionize it?</p> <p>Let's create a function to \"set the seeds\" called <code>set_seeds()</code>.</p> <p>Note: Recall a random seed is a way of flavouring the randomness generated by a computer. They aren't necessary to always set when running machine learning code, however, they help ensure there's an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally aren't required.</p>"},{"location":"07_pytorch_experiment_tracking/#1-get-data","title":"1. Get data\u00b6","text":"<p>As always, before we can run machine learning experiments, we'll need a dataset.</p> <p>We're going to continue trying to improve upon the results we've been getting on FoodVision Mini.</p> <p>In the previous section, 06. PyTorch Transfer Learning, we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.</p> <p>So how about we run some experiments and try to further improve our results?</p> <p>To do so, we'll use similar code to the previous section to download the <code>pizza_steak_sushi.zip</code> (if the data doesn't already exist) except this time its been functionised.</p> <p>This will allow us to use it again later.</p>"},{"location":"07_pytorch_experiment_tracking/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Now we've got some data, let's turn it into PyTorch DataLoaders.</p> <p>We can do so using the <code>create_dataloaders()</code> function we created in 05. PyTorch Going Modular part 2.</p> <p>And since we'll be using transfer learning and specifically pretrained models from <code>torchvision.models</code>, we'll create a transform to prepare our images correctly.</p> <p>To transform our images in tensors, we can use:</p> <ol> <li>Manually created transforms using <code>torchvision.transforms</code>.</li> <li>Automatically created transforms using <code>torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()</code>.<ul> <li>Where <code>MODEL_NAME</code> is a specific <code>torchvision.models</code> architecture, <code>MODEL_WEIGHTS</code> is a specific set of pretrained weights and <code>DEFAULT</code> means the \"best available weights\".</li> </ul> </li> </ol> <p>We saw an example of each of these in 06. PyTorch Transfer Learning section 2.</p> <p>Let's see first an example of manually creating a <code>torchvision.transforms</code> pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms don't match the pretrained model).</p> <p>The main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained <code>torchvision.models</code> are all pretrained on ImageNet).</p> <p>We can do this with:</p> <pre>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</pre>"},{"location":"07_pytorch_experiment_tracking/#21-create-dataloaders-using-manually-created-transforms","title":"2.1 Create DataLoaders using manually created transforms\u00b6","text":""},{"location":"07_pytorch_experiment_tracking/#22-create-dataloaders-using-automatically-created-transforms","title":"2.2 Create DataLoaders using automatically created transforms\u00b6","text":"<p>Data transformed and DataLoaders created!</p> <p>Let's now see what the same transformation pipeline looks like but this time by using automatic transforms.</p> <p>We can do this by first instantiating a set of pretrained weights (for example <code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT</code>)  we'd like to use and calling the <code>transforms()</code> method on it.</p>"},{"location":"07_pytorch_experiment_tracking/#3-getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head","title":"3. Getting a pretrained model, freezing the base layers and changing the classifier head\u00b6","text":"<p>Before we run and track multiple modelling experiments, let's see what it's like to run and track a single one.</p> <p>And since our data is ready, the next thing we'll need is a model.</p> <p>Let's download the pretrained weights for a <code>torchvision.models.efficientnet_b0()</code> model and prepare it for use with our own data.</p>"},{"location":"07_pytorch_experiment_tracking/#4-train-model-and-track-results","title":"4. Train model and track results\u00b6","text":"<p>Model ready to go!</p> <p>Let's get ready to train it by creating a loss function and an optimizer.</p> <p>Since we're working with multiple classes, we'll use <code>torch.nn.CrossEntropyLoss()</code> as the loss function.</p> <p>And we'll stick with <code>torch.optim.Adam()</code> with learning rate of <code>0.001</code> for the optimizer.</p>"},{"location":"07_pytorch_experiment_tracking/#adjust-train-function-to-track-results-with-summarywriter","title":"Adjust <code>train()</code> function to track results with <code>SummaryWriter()</code>\u00b6","text":"<p>Beautiful!</p> <p>All of the pieces of our training code are starting to come together.</p> <p>Let's now add the final piece to track our experiments.</p> <p>Previously, we've tracked our modelling experiments using multiple Python dictionaries (one for each model).</p> <p>But you can imagine this could get out of hand if we were running anything more than a few experiments.</p> <p>Not to worry, there's a better option!</p> <p>We can use PyTorch's <code>torch.utils.tensorboard.SummaryWriter()</code> class to save various parts of our model's training progress to file.</p> <p>By default, the <code>SummaryWriter()</code> class saves various information about our model to a file set by the <code>log_dir</code> parameter.</p> <p>The default location for <code>log_dir</code> is under <code>runs/CURRENT_DATETIME_HOSTNAME</code>, where the <code>HOSTNAME</code> is the name of your computer.</p> <p>But of course, you can change where your experiments are tracked (the filename is as customisable as you'd like).</p> <p>The outputs of the <code>SummaryWriter()</code> are saved in TensorBoard format.</p> <p>TensorBoard is a part of the TensorFlow deep learning library and is an excellent way to visualize different parts of your model.</p> <p>To start tracking our modelling experiments, let's create a default <code>SummaryWriter()</code> instance.</p>"},{"location":"07_pytorch_experiment_tracking/#5-view-our-models-results-in-tensorboard","title":"5. View our model's results in TensorBoard\u00b6","text":"<p>The <code>SummaryWriter()</code> class stores our model's results in a directory called <code>runs/</code> in TensorBoard format by default.</p> <p>TensorBoard is a visualization program created by the TensorFlow team to view and inspect information about models and data.</p> <p>You know what that means?</p> <p>It's time to follow the data visualizer's motto and visualize, visualize, visualize!</p> <p>You can view TensorBoard in a number of ways:</p> Code environment How to view TensorBoard Resource VS Code (notebooks or Python scripts) Press <code>SHIFT + CMD + P</code> to open the Command Palette and search for the command \"Python: Launch TensorBoard\". VS Code Guide on TensorBoard and PyTorch Jupyter and Colab Notebooks Make sure TensorBoard is installed, load it with <code>%load_ext tensorboard</code> and then view your results with <code>%tensorboard --logdir DIR_WITH_LOGS</code>. <code>torch.utils.tensorboard</code> and Get started with TensorBoard <p>You can also upload your experiments to tensorboard.dev to share them publicly with others.</p> <p>Running the following code in a Google Colab or Jupyter Notebook will start an interactive TensorBoard session to view TensorBoard files in the <code>runs/</code> directory.</p> <pre>%load_ext tensorboard # line magic to load TensorBoard\n%tensorboard --logdir runs # run TensorBoard session with the \"runs/\" directory\n</pre>"},{"location":"07_pytorch_experiment_tracking/#6-create-a-helper-function-to-build-summarywriter-instances","title":"6. Create a helper function to build <code>SummaryWriter()</code> instances\u00b6","text":"<p>The <code>SummaryWriter()</code> class logs various information to a directory specified by the <code>log_dir</code> parameter.</p> <p>How about we make a helper function to create a custom directory per experiment?</p> <p>In essence, each experiment gets its own logs directory.</p> <p>For example, say we'd like to track things like:</p> <ul> <li>Experiment date/timestamp - when did the experiment take place?</li> <li>Experiment name - is there something we'd like to call the experiment?</li> <li>Model name - what model was used?</li> <li>Extra - should anything else be tracked?</li> </ul> <p>You could track almost anything here and be as creative as you want but these should be enough to start.</p> <p>Let's create a helper function called <code>create_writer()</code> that produces a <code>SummaryWriter()</code> instance tracking to a custom <code>log_dir</code>.</p> <p>Ideally, we'd like the <code>log_dir</code> to be something like:</p> <p><code>runs/YYYY-MM-DD/experiment_name/model_name/extra</code></p> <p>Where <code>YYYY-MM-DD</code> is the date the experiment was run (you could add the time if you wanted to as well).</p>"},{"location":"07_pytorch_experiment_tracking/#61-update-the-train-function-to-include-a-writer-parameter","title":"6.1 Update the <code>train()</code> function to include a <code>writer</code> parameter\u00b6","text":"<p>Our <code>create_writer()</code> function works fantastic.</p> <p>How about we give our <code>train()</code> function the ability to take in a <code>writer</code> parameter so we actively update the <code>SummaryWriter()</code> instance we're using each time we call <code>train()</code>.</p> <p>For example, say we're running a series of experiments, calling <code>train()</code> multiple times for multiple different models, it would be good if each experiment used a different <code>writer</code>.</p> <p>One <code>writer</code> per experiment = one logs directory per experiment.</p> <p>To adjust the <code>train()</code> function we'll add a <code>writer</code> parameter to the function and then we'll add some code to see if there's a <code>writer</code> and if so, we'll track our information there.</p>"},{"location":"07_pytorch_experiment_tracking/#7-setting-up-a-series-of-modelling-experiments","title":"7. Setting up a series of modelling experiments\u00b6","text":"<p>It's to step things up a notch.</p> <p>Previously we've been running various experiments and inspecting the results one by one.</p> <p>But what if we could run multiple experiments and then inspect the results all together?</p> <p>You in?</p> <p>C'mon, let's go.</p>"},{"location":"07_pytorch_experiment_tracking/#71-what-kind-of-experiments-should-you-run","title":"7.1 What kind of experiments should you run?\u00b6","text":"<p>That's the million dollar question in machine learning.</p> <p>Because there's really no limit to the experiments you can run.</p> <p>Such a freedom is why machine learning is so exciting and terrifying at the same time.</p> <p>This is where you'll have to put on your scientist coat and remember the machine learning practitioner's motto: experiment, experiment, experiment!</p> <p>Every hyperparameter stands as a starting point for a different experiment:</p> <ul> <li>Change the number of epochs.</li> <li>Change the number of layers/hidden units.</li> <li>Change the amount of data.</li> <li>Change the learning rate.</li> <li>Try different kinds of data augmentation.</li> <li>Choose a different model architecture.</li> </ul> <p>With practice and running many different experiments, you'll start to build an intuition of what might help your model.</p> <p>I say might on purpose because there's no guarantees.</p> <p>But generally, in light of The Bitter Lesson (I've mentioned this twice now because it's an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.</p> <p>However, when you're first approaching a machine learning problem: start small and if something works, scale it up.</p> <p>Your first batch of experiments should take no longer than a few seconds to a few minutes to run.</p> <p>The quicker you can experiment, the faster you can work out what doesn't work, in turn, the faster you can work out what does work.</p>"},{"location":"07_pytorch_experiment_tracking/#72-what-experiments-are-we-going-to-run","title":"7.2 What experiments are we going to run?\u00b6","text":"<p>Our goal is to improve the model powering FoodVision Mini without it getting too big.</p> <p>In essence, our ideal model achieves a high level of test set accuracy (90%+) but doesn't take too long to train/perform inference (make predictions).</p> <p>We've got plenty of options but how about we keep things simple?</p> <p>Let's try a combination of:</p> <ol> <li>A different amount of data (10% of Pizza, Steak, Sushi vs. 20%)</li> <li>A different model (<code>torchvision.models.efficientnet_b0</code> vs. <code>torchvision.models.efficientnet_b2</code>)</li> <li>A different training time (5 epochs vs. 10 epochs)</li> </ol> <p>Breaking these down we get:</p> Experiment number Training Dataset Model (pretrained on ImageNet) Number of epochs 1 Pizza, Steak, Sushi 10% percent EfficientNetB0 5 2 Pizza, Steak, Sushi 10% percent EfficientNetB2 5 3 Pizza, Steak, Sushi 10% percent EfficientNetB0 10 4 Pizza, Steak, Sushi 10% percent EfficientNetB2 10 5 Pizza, Steak, Sushi 20% percent EfficientNetB0 5 6 Pizza, Steak, Sushi 20% percent EfficientNetB2 5 7 Pizza, Steak, Sushi 20% percent EfficientNetB0 10 8 Pizza, Steak, Sushi 20% percent EfficientNetB2 10 <p>Notice how we're slowly scaling things up.</p> <p>With each experiment we slowly increase the amount of data, the model size and the length of training.</p> <p>By the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.</p> <p>Note: I want to be clear that there truly is no limit to amount of experiments you can run. What we've designed here is only a very small subset of options. However, you can't test everything so best to try a few things to begin with and then follow the ones which work the best.</p> <p>And as a reminder, the datasets we're using are a subset of the Food101 dataset (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the <code>04_custom_data_creation.ipynb</code> notebook.</p>"},{"location":"07_pytorch_experiment_tracking/#73-download-different-datasets","title":"7.3 Download different datasets\u00b6","text":"<p>Before we start running our series of experiments, we need to make sure our datasets are ready.</p> <p>We'll need two forms of a training set:</p> <ol> <li>A training set with 10% of the data of Food101 pizza, steak, sushi images (we've already created this above but we'll do it again for completeness).</li> <li>A training set with 20% of the data of Food101 pizza, steak, sushi images.</li> </ol> <p>For consistency, all experiments will use the same testing dataset (the one from the 10% data split).</p> <p>We'll start by downloading the various datasets we need using the <code>download_data()</code> function we created earlier.</p> <p>Both datasets are available from the course GitHub:</p> <ol> <li>Pizza, steak, sushi 10% training data.</li> <li>Pizza, steak, sushi 20% training data.</li> </ol>"},{"location":"07_pytorch_experiment_tracking/#74-transform-datasets-and-create-dataloaders","title":"7.4 Transform Datasets and create DataLoaders\u00b6","text":"<p>Next we'll create a series of transforms to prepare our images for our model(s).</p> <p>To keep things consistent, we'll manually create a transform (just like we did above) and use the same transform across all of the datasets.</p> <p>The transform will:</p> <ol> <li>Resize all the images (we'll start with 224, 224 but this could be changed).</li> <li>Turn them into tensors with values between 0 &amp; 1.</li> <li>Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from <code>torchvision.models</code> have been pretrained on ImageNet).</li> </ol>"},{"location":"07_pytorch_experiment_tracking/#75-create-feature-extractor-models","title":"7.5 Create feature extractor models\u00b6","text":"<p>Time to start building our models.</p> <p>We're going to create two feature extractor models:</p> <ol> <li><code>torchvision.models.efficientnet_b0()</code> pretrained backbone + custom classifier head (EffNetB0 for short).</li> <li><code>torchvision.models.efficientnet_b2()</code> pretrained backbone + custom classifier head (EffNetB2 for short).</li> </ol> <p>To do this, we'll freeze the base layers (the feature layers) and update the model's classifier heads (output layers) to suit our problem just like we did in 06. PyTorch Transfer Learning section 3.4.</p> <p>We saw in the previous chapter the <code>in_features</code> parameter to the classifier head of EffNetB0 is <code>1280</code> (the backbone turns the input image into a feature vector of size <code>1280</code>).</p> <p>Since EffNetB2 has a different number of layers and parameters, we'll need to adapt it accordingly.</p> <p>Note: Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you'll know how you'll have to prepare your input data/update the model to have the correct output shape.</p> <p>We can find the input and output shapes of EffNetB2 using <code>torchinfo.summary()</code> and passing in the <code>input_size=(32, 3, 224, 224)</code> parameter (<code>(32, 3, 224, 224)</code> is equivalent to <code>(batch_size, color_channels, height, width)</code>, i.e we pass in an example of what a single batch of data would be to our model).</p> <p>Note: Many modern models can handle input images of varying sizes thanks to <code>torch.nn.AdaptiveAvgPool2d()</code> layer, this layer adaptively adjusts the <code>output_size</code> of a given input as required. You can try this out by passing different size input images to <code>torchinfo.summary()</code> or to your own models using the layer.</p> <p>To find the required input shape to the final layer of EffNetB2, let's:</p> <ol> <li>Create an instance of <code>torchvision.models.efficientnet_b2(pretrained=True)</code>.</li> <li>See the various input and output shapes by running <code>torchinfo.summary()</code>.</li> <li>Print out the number of <code>in_features</code> by inspecting <code>state_dict()</code> of the classifier portion of EffNetB2 and printing the length of the weight matrix.<ul> <li>Note: You could also just inspect the output of <code>effnetb2.classifier</code>.</li> </ul> </li> </ol>"},{"location":"07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code","title":"7.6 Create experiments and set up training code\u00b6","text":"<p>We've prepared our data and prepared our models, the time has come to setup some experiments!</p> <p>We'll start by creating two lists and a dictionary:</p> <ol> <li>A list of the number of epochs we'd like to test (<code>[5, 10]</code>)</li> <li>A list of the models we'd like to test (<code>[\"effnetb0\", \"effnetb2\"]</code>)</li> <li>A dictionary of the different training DataLoaders</li> </ol>"},{"location":"07_pytorch_experiment_tracking/#8-view-experiments-in-tensorboard","title":"8. View experiments in TensorBoard\u00b6","text":"<p>Ho, ho!</p> <p>Look at us go!</p> <p>Training eight models in one go?</p> <p>Now that's living up to the motto!</p> <p>Experiment, experiment, experiment!</p> <p>How about we check out the results in TensorBoard?</p>"},{"location":"07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it","title":"9. Load in the best model and make predictions with it\u00b6","text":"<p>Looking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).</p> <p>This is the experiment that used:</p> <ul> <li>EffNetB2 (double the parameters of EffNetB0)</li> <li>20% pizza, steak, sushi training data (double the original training data)</li> <li>10 epochs (double the original training time)</li> </ul> <p>In essence, our biggest model achieved the best results.</p> <p>Though it wasn't as if these results were far better than the other models.</p> <p>The same model on the same data achieved similar results in half the training time (experiment number 6).</p> <p>This suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.</p> <p>Inspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).</p> <p>More experiments could be done to further test this but for now, let's import our best performing model from experiment eight (saved to: <code>models/07_effnetb2_data_20_percent_10_epochs.pth</code>, you can download this model from the course GitHub) and perform some qualitative evaluations.</p> <p>In other words, let's visualize, visualize, visualize!</p> <p>We can import the best saved model by creating a new instance of EffNetB2 using the <code>create_effnetb2()</code> function and then load in the saved <code>state_dict()</code> with <code>torch.load()</code>.</p>"},{"location":"07_pytorch_experiment_tracking/#91-predict-on-a-custom-image-with-the-best-model","title":"9.1 Predict on a custom image with the best model\u00b6","text":"<p>Making predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.</p> <p>So let's import the trusty pizza dad image (a photo of my dad in front of a pizza) we've been using for the past couple of sections and see how our model performs on it.</p>"},{"location":"07_pytorch_experiment_tracking/#main-takeaways","title":"Main takeaways\u00b6","text":"<p>We've now gone full circle on the PyTorch workflow introduced in 01. PyTorch Workflow Fundamentals, we've gotten data ready, we've built and picked a pretrained model, we've used our various helper functions to train and evaluate the model and in this notebook we've improved our FoodVision Mini model by running and tracking a series of experiments.</p> <p>You should be proud of yourself, this is no small feat!</p> <p>The main ideas you should take away from this Milestone Project 1 are:</p> <ul> <li>The machine learning practioner's motto: experiment, experiment, experiment! (though we've been doing plenty of this already).</li> <li>In the beginning, keep your experiments small so you can work fast, your first few experiments shouldn't take more than a few seconds to a few minutes to run.</li> <li>The more experiments you do, the quicker you can figure out what doesn't work.</li> <li>Scale up when you find something that works. For example, since we've found a pretty good performing model with EffNetB2 as a feature extractor, perhaps you'd now like to see what happens when you scale it up to the whole Food101 dataset from <code>torchvision.datasets</code>.</li> <li>Programmatically tracking your experiments takes a few steps to set up but it's worth it in the long run so you can figure out what works and what doesn't.<ul> <li>There are many different machine learning experiment trackers out there so explore a few and try them out.</li> </ul> </li> </ul>"},{"location":"07_pytorch_experiment_tracking/#exercises","title":"Exercises\u00b6","text":"<p>Note: These exercises expect the use of <code>torchvision</code> v0.13+ (released July 2022), previous versions may work but will likely have errors.</p> <p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 07</li> <li>Example solutions notebook for 07 (try the exercises before looking at this)<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all)</li> </ul> </li> </ul> <ol> <li>Pick a larger model from <code>torchvision.models</code> to add to the list of experiments (for example, EffNetB3 or higher).<ul> <li>How does it perform compared to our existing models?</li> </ul> </li> <li>Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?<ul> <li>For example, you could have one training DataLoader that uses data augmentation (e.g. <code>train_dataloader_20_percent_aug</code> and <code>train_dataloader_20_percent_no_aug</code>) and then compare the results of two of the same model types training on these two DataLoaders.</li> <li>Note: You may need to alter the <code>create_dataloaders()</code> function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See 04. PyTorch Custom Datasets section 6 for examples of using data augmentation or the script below for an example:</li> </ul> </li> </ol> <pre># Note: Data augmentation transform like this should only be performed on training data\ntrain_transform_data_aug = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Helper function to view images in a DataLoader (works with data augmentation transforms or not) \ndef view_dataloader_images(dataloader, n=10):\n    if n &gt; 10:\n        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n        n = 10\n    imgs, labels = next(iter(dataloader))\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        # Min max scale the image for display purposes\n        targ_image = imgs[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # Plot images with appropriate axes information\n        plt.subplot(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n        plt.title(class_names[labels[i]])\n        plt.axis(False)\n\n# Have to update `create_dataloaders()` to handle different augmentations\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\nNUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n\n# Note: this is an update version of data_setup.create_dataloaders to handle\n# differnt train and test transforms.\ndef create_dataloaders(\n    train_dir, \n    test_dir, \n    train_transform, # add parameter for train transform (transforms on train dataset)\n    test_transform,  # add parameter for test transform (transforms on test dataset)\n    batch_size=32, num_workers=NUM_WORKERS\n):\n    # Use ImageFolder to create dataset(s)\n    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    # Get class names\n    class_names = train_data.classes\n\n    # Turn images into data loaders\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names\n</pre> <ol> <li>Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire Food101 dataset from <code>torchvision.models</code><ul> <li>You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.</li> <li>If you try more than one model, it would be good to have the model's results tracked.</li> <li>If you load the Food101 dataset from <code>torchvision.models</code>, you'll have to create PyTorch DataLoaders to use it in training.</li> <li>Note: Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train.</li> </ul> </li> </ol>"},{"location":"07_pytorch_experiment_tracking/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Read The Bitter Lesson blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.</li> <li>Go through the PyTorch YouTube/code tutorial for TensorBoard for 20-minutes and see how it compares to the code we've written in this notebook.</li> <li>Perhaps you may want to view and rearrange your model's TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), there's a guide for this in the TensorBoard documentation.</li> <li>If you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the  PyTorch Development in VSCode guide.</li> <li>To go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the PyTorch documentation for the PyTorch profiler.</li> <li>Made With ML is an outstanding resource for all things machine learning by Goku Mohandas and their guide on experiment tracking contains a fantastic introduction to tracking machine learning experiments with MLflow.</li> </ul>"},{"location":"08_pytorch_paper_replicating/","title":"08. PyTorch Paper Replicating","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.12.0+cu102\ntorchvision version: 0.13.0+cu102\n</pre> <p>Note: If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> and <code>torchvision</code>.</p> <p>Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the <code>helper_functions.py</code> script from GitHub.</p> <p>The <code>helper_functions.py</code> script contains several functions we created in previous sections:</p> <ul> <li><code>set_seeds()</code> to set the random seeds (created in 07. PyTorch Experiment Tracking section 0).</li> <li><code>download_data()</code> to download a data source given a link (created in 07. PyTorch Experiment Tracking section 1).</li> <li><code>plot_loss_curves()</code> to inspect our model's training results (created in 04. PyTorch Custom Datasets section 7.8)</li> </ul> <p>Note: It may be a better idea for many of the functions in the <code>helper_functions.py</code> script to be merged into <code>going_modular/going_modular/utils.py</code>, perhaps that's an extension you'd like to try.</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[4]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>Beautiful! Data downloaded, let's setup the training and test directories.</p> In\u00a0[5]: Copied! <pre># Setup directory paths to train and test images\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup directory paths to train and test images train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[6]: Copied! <pre># Create image size (from Table 3 in the ViT paper) \nIMG_SIZE = 224\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n</pre> # Create image size (from Table 3 in the ViT paper)  IMG_SIZE = 224  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((IMG_SIZE, IMG_SIZE)),     transforms.ToTensor(), ])            print(f\"Manually created transforms: {manual_transforms}\") <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n)\n</pre> In\u00a0[7]: Copied! <pre># Set the batch size\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Set the batch size BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=BATCH_SIZE )  train_dataloader, test_dataloader, class_names Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f18845ff0d0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f17f3f5f520&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Get a batch of images\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Get a single image from the batch\nimage, label = image_batch[0], label_batch[0]\n\n# View the batch shapes\nimage.shape, label\n</pre> # Get a batch of images image_batch, label_batch = next(iter(train_dataloader))  # Get a single image from the batch image, label = image_batch[0], label_batch[0]  # View the batch shapes image.shape, label Out[8]: <pre>(torch.Size([3, 224, 224]), tensor(2))</pre> <p>Wonderful!</p> <p>Now let's plot the image and its label with <code>matplotlib</code>.</p> In\u00a0[9]: Copied! <pre># Plot image with matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Plot image with matplotlib plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels] plt.title(class_names[label]) plt.axis(False); <p>Nice!</p> <p>Looks like our images are importing correctly, let's continue with the paper replication.</p> In\u00a0[10]: Copied! <pre># Create example values\nheight = 224 # H (\"The training resolution is 224.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n</pre> # Create example values height = 224 # H (\"The training resolution is 224.\") width = 224 # W color_channels = 3 # C patch_size = 16 # P  # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2) print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\") <pre>Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196\n</pre> <p>We've got the number of patches, how about we create the image output size as well?</p> <p>Better yet, let's replicate the input and output shapes of the patch embedding layer.</p> <p>Recall:</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> In\u00a0[11]: Copied! <pre># Input shape (this is the size of a single image)\nembedding_layer_input_shape = (height, width, color_channels)\n\n# Output shape\nembedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\nprint(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")\n</pre> # Input shape (this is the size of a single image) embedding_layer_input_shape = (height, width, color_channels)  # Output shape embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)  print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\") print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\") <pre>Input shape (single 2D image): (224, 224, 3)\nOutput shape (single 2D image flattened into patches): (196, 768)\n</pre> <p>Input and output shapes acquired!</p> In\u00a0[12]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); <p>We want to turn this image into patches of itself inline with Figure 1 of the ViT paper.</p> <p>How about we start by just visualizing the top row of patched pixels?</p> <p>We can do this by indexing on the different image dimensions.</p> In\u00a0[13]: Copied! <pre># Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) \nimage_permuted = image.permute(1, 2, 0)\n\n# Index to plot the top row of patched pixels\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n</pre> # Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels)  image_permuted = image.permute(1, 2, 0)  # Index to plot the top row of patched pixels patch_size = 16 plt.figure(figsize=(patch_size, patch_size)) plt.imshow(image_permuted[:patch_size, :, :]); <p>Now we've got the top row, let's turn it into patches.</p> <p>We can do this by iterating through the number of patches there'd be in the top row.</p> In\u00a0[14]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \nprint(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=1, \n                        ncols=img_size // patch_size, # one column for each patch\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Iterate through number of patches in the top row\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n    axs[i].set_xlabel(i+1) # set the label\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size  assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"  print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Create a series of subplots fig, axs = plt.subplots(nrows=1,                          ncols=img_size // patch_size, # one column for each patch                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Iterate through number of patches in the top row for i, patch in enumerate(range(0, img_size, patch_size)):     axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index     axs[i].set_xlabel(i+1) # set the label     axs[i].set_xticks([])     axs[i].set_yticks([]) <pre>Number of patches per row: 14.0\nPatch size: 16 pixels x 16 pixels\n</pre> <p>Those are some nice looking patches!</p> <p>How about we do it for the whole image?</p> <p>This time we'll iterate through the indexs for height and width and plot each patch as it's own subplot.</p> In\u00a0[15]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \nprint(f\"Number of patches per row: {num_patches}\\\n\\nNumber of patches per column: {num_patches}\\\n\\nTotal patches: {num_patches*num_patches}\\\n\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n                        ncols=img_size // patch_size, \n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Loop through height and width of image\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n        \n        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height \n                                        patch_width:patch_width+patch_size, # iterate through width\n                                        :]) # get all color channels\n        \n        # Set up label information, remove the ticks for clarity and set labels to outside\n        axs[i, j].set_ylabel(i+1, \n                             rotation=\"horizontal\", \n                             horizontalalignment=\"right\", \n                             verticalalignment=\"center\") \n        axs[i, j].set_xlabel(j+1) \n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# Set a super title\nfig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16)\nplt.show()\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size  assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"  print(f\"Number of patches per row: {num_patches}\\         \\nNumber of patches per column: {num_patches}\\         \\nTotal patches: {num_patches*num_patches}\\         \\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float                         ncols=img_size // patch_size,                          figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height     for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width                  # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))         axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height                                          patch_width:patch_width+patch_size, # iterate through width                                         :]) # get all color channels                  # Set up label information, remove the ticks for clarity and set labels to outside         axs[i, j].set_ylabel(i+1,                               rotation=\"horizontal\",                               horizontalalignment=\"right\",                               verticalalignment=\"center\")          axs[i, j].set_xlabel(j+1)          axs[i, j].set_xticks([])         axs[i, j].set_yticks([])         axs[i, j].label_outer()  # Set a super title fig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16) plt.show() <pre>Number of patches per row: 14.0        \nNumber of patches per column: 14.0        \nTotal patches: 196.0        \nPatch size: 16 pixels x 16 pixels\n</pre> <p>Image patchified!</p> <p>Woah, that looks cool.</p> <p>Now how do we turn each of these patches into an embedding and convert them into a sequence?</p> <p>Hint: we can use PyTorch layers. Can you guess which?</p> In\u00a0[16]: Copied! <pre>from torch import nn\n\n# Set the patch size\npatch_size=16\n\n# Create the Conv2d layer with hyperparameters from the ViT paper\nconv2d = nn.Conv2d(in_channels=3, # number of color channels\n                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n                   stride=patch_size,\n                   padding=0)\n</pre> from torch import nn  # Set the patch size patch_size=16  # Create the Conv2d layer with hyperparameters from the ViT paper conv2d = nn.Conv2d(in_channels=3, # number of color channels                    out_channels=768, # from Table 1: Hidden size D, this is the embedding size                    kernel_size=patch_size, # could also use (patch_size, patch_size)                    stride=patch_size,                    padding=0) <p>Now we've got a convoluational layer, let's see what happens when we pass a single image through it.</p> In\u00a0[17]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); In\u00a0[18]: Copied! <pre># Pass the image through the convolutional layer \nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)\nprint(image_out_of_conv.shape)\n</pre> # Pass the image through the convolutional layer  image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels) print(image_out_of_conv.shape) <pre>torch.Size([1, 768, 14, 14])\n</pre> <p>Passing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or $D$) feature/activation maps.</p> <p>So its output shape can be read as:</p> <pre>torch.Size([1, 768, 14, 14]) -&gt; [batch_size, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>Let's visualize five random feature maps and see what they look like.</p> In\u00a0[19]: Copied! <pre># Plot random 5 convolutional feature maps\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\nprint(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n\n# Create plot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# Plot random image feature maps\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n</pre> # Plot random 5 convolutional feature maps import random random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")  # Create plot fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))  # Plot random image feature maps for i, idx in enumerate(random_indexes):     image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer     axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())     axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]); <pre>Showing random convolutional feature maps from indexes: [571, 727, 734, 380, 90]\n</pre> <p>Notice how the feature maps all kind of represent the original image, after visualizing a few more you can start to see the different major outlines and some major features.</p> <p>The important thing to note is that these features may change over time as the neural network learns.</p> <p>And because of these, these feature maps can be considered a learnable embedding of our image.</p> <p>Let's check one out in numerical form.</p> In\u00a0[20]: Copied! <pre># Get a single feature map in tensor form\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n</pre> # Get a single feature map in tensor form single_feature_map = image_out_of_conv[:, 0, :, :] single_feature_map, single_feature_map.requires_grad Out[20]: <pre>(tensor([[[ 0.4732,  0.3567,  0.3377,  0.3736,  0.3208,  0.3913,  0.3464,\n            0.3702,  0.2541,  0.3594,  0.1984,  0.3982,  0.3741,  0.1251],\n          [ 0.4178,  0.4771,  0.3374,  0.3353,  0.3159,  0.4008,  0.3448,\n            0.3345,  0.5850,  0.4115,  0.2969,  0.2751,  0.6150,  0.4188],\n          [ 0.3209,  0.3776,  0.4970,  0.4272,  0.3301,  0.4787,  0.2754,\n            0.3726,  0.3298,  0.4631,  0.3087,  0.4915,  0.4129,  0.4592],\n          [ 0.4540,  0.4930,  0.5570,  0.2660,  0.2150,  0.2044,  0.2766,\n            0.2076,  0.3278,  0.3727,  0.2637,  0.2493,  0.2782,  0.3664],\n          [ 0.4920,  0.5671,  0.3298,  0.2992,  0.1437,  0.1701,  0.1554,\n            0.1375,  0.1377,  0.3141,  0.2694,  0.2771,  0.2412,  0.3700],\n          [ 0.5783,  0.5790,  0.4229,  0.5032,  0.1216,  0.1000,  0.0356,\n            0.1258, -0.0023,  0.1640,  0.2809,  0.2418,  0.2606,  0.3787],\n          [ 0.5334,  0.5645,  0.4781,  0.3307,  0.2391,  0.0461,  0.0095,\n            0.0542,  0.1012,  0.1331,  0.2446,  0.2526,  0.3323,  0.4120],\n          [ 0.5724,  0.2840,  0.5188,  0.3934,  0.1328,  0.0776,  0.0235,\n            0.1366,  0.3149,  0.2200,  0.2793,  0.2351,  0.4722,  0.4785],\n          [ 0.4009,  0.4570,  0.4972,  0.5785,  0.2261,  0.1447, -0.0028,\n            0.2772,  0.2697,  0.4008,  0.3606,  0.3372,  0.4535,  0.4492],\n          [ 0.5678,  0.5870,  0.5824,  0.3438,  0.5113,  0.0757,  0.1772,\n            0.3677,  0.3572,  0.3742,  0.3820,  0.4868,  0.3781,  0.4694],\n          [ 0.5845,  0.5877,  0.5826,  0.3212,  0.5276,  0.4840,  0.4825,\n            0.5523,  0.5308,  0.5085,  0.5606,  0.5720,  0.4928,  0.5581],\n          [ 0.5853,  0.5849,  0.5793,  0.3410,  0.4428,  0.4044,  0.3275,\n            0.4958,  0.4366,  0.5750,  0.5494,  0.5868,  0.5557,  0.5069],\n          [ 0.5880,  0.5888,  0.5796,  0.3377,  0.2635,  0.2347,  0.3145,\n            0.3486,  0.5158,  0.5722,  0.5347,  0.5753,  0.5816,  0.4378],\n          [ 0.5692,  0.5843,  0.5721,  0.5081,  0.2694,  0.2032,  0.1589,\n            0.3464,  0.5349,  0.5768,  0.5739,  0.5764,  0.5394,  0.4482]]],\n        grad_fn=&lt;SliceBackward0&gt;),\n True)</pre> <p>The <code>grad_fn</code> output of the <code>single_feature_map</code> and the <code>requires_grad=True</code> attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training.</p> In\u00a0[21]: Copied! <pre># Current tensor shape\nprint(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\")\n</pre> # Current tensor shape print(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\") <pre>Current tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>Well we've got the 768 part ( $(P^{2} \\cdot C)$ ) but we still need the number of patches ($N$).</p> <p>Reading back through section 3.1 of the ViT paper it says (bold mine):</p> <p>As a special case, the patches can have spatial size $1 \\times 1$, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension.</p> <p>Flattening the spatial dimensions of the feature map hey?</p> <p>What layer do we have in PyTorch that can flatten?</p> <p>How about <code>torch.nn.Flatten()</code>?</p> <p>But we don't want to flatten the whole tensor, we only want to flatten the \"spatial dimensions of the feature map\".</p> <p>Which in our case is the <code>feature_map_height</code> and <code>feature_map_width</code> dimensions of <code>image_out_of_conv</code>.</p> <p>So how about we create a <code>torch.nn.Flatten()</code> layer to only flatten those dimensions, we can use the <code>start_dim</code> and <code>end_dim</code> parameters to set that up?</p> In\u00a0[22]: Copied! <pre># Create flatten layer\nflatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n                     end_dim=3) # flatten feature_map_width (dimension 3)\n</pre> # Create flatten layer flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)                      end_dim=3) # flatten feature_map_width (dimension 3) <p>Nice! Now let's put it all together!</p> <p>We'll:</p> <ol> <li>Take a single image.</li> <li>Put in through the convolutional layer (<code>conv2d</code>) to turn the image into 2D feature maps (patch embeddings).</li> <li>Flatten the 2D feature map into a single sequence.</li> </ol> In\u00a0[23]: Copied! <pre># 1. View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"Original image shape: {image.shape}\")\n\n# 2. Turn image into feature maps\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\nprint(f\"Image feature map shape: {image_out_of_conv.shape}\")\n\n# 3. Flatten the feature maps\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")\n</pre> # 1. View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); print(f\"Original image shape: {image.shape}\")  # 2. Turn image into feature maps image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors print(f\"Image feature map shape: {image_out_of_conv.shape}\")  # 3. Flatten the feature maps image_out_of_conv_flattened = flatten(image_out_of_conv) print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\") <pre>Original image shape: torch.Size([3, 224, 224])\nImage feature map shape: torch.Size([1, 768, 14, 14])\nFlattened image feature map shape: torch.Size([1, 768, 196])\n</pre> <p>Woohoo! It looks like our <code>image_out_of_conv_flattened</code> shape is very close to our desired output shape:</p> <ul> <li>Desried output (flattened 2D patches): (196, 768) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> <li>Current shape: (1, 768, 196)</li> </ul> <p>The only difference is our current shape has a batch size and the dimensions are in a different order to the desired output.</p> <p>How could we fix this?</p> <p>Well, how about we rearrange the dimensions?</p> <p>We can do so with <code>torch.Tensor.permute()</code> just like we do when rearranging image tensors to plot them with matplotlib.</p> <p>Let's try.</p> In\u00a0[24]: Copied! <pre># Get flattened image patch embeddings in right shape \nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\nprint(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\")\n</pre> # Get flattened image patch embeddings in right shape  image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\") <pre>Patch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]\n</pre> <p>Yes!!!</p> <p>We've now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers.</p> <p>How about we visualize one of the flattened feature maps?</p> In\u00a0[25]: Copied! <pre># Get a single flattened feature map\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n\n# Plot the flattened feature map visually\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n</pre> # Get a single flattened feature map single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)  # Plot the flattened feature map visually plt.figure(figsize=(22, 22)) plt.imshow(single_flattened_feature_map.detach().numpy()) plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\") plt.axis(False); <p>Hmm, the flattened feature map doesn't look like much visually, but that's not what we're concerned about, this is what will be the output of the patching embedding layer and the input to the rest of the ViT architecture.</p> <p>Note: The original Transformer architecture was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why the input to the ViT architecture is processed in the way it is. We're essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.</p> <p>How about we view the flattened feature map in tensor form?</p> In\u00a0[26]: Copied! <pre># See the flattened feature map as a tensor\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n</pre> # See the flattened feature map as a tensor single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape Out[26]: <pre>(tensor([[ 0.4732,  0.3567,  0.3377,  0.3736,  0.3208,  0.3913,  0.3464,  0.3702,\n           0.2541,  0.3594,  0.1984,  0.3982,  0.3741,  0.1251,  0.4178,  0.4771,\n           0.3374,  0.3353,  0.3159,  0.4008,  0.3448,  0.3345,  0.5850,  0.4115,\n           0.2969,  0.2751,  0.6150,  0.4188,  0.3209,  0.3776,  0.4970,  0.4272,\n           0.3301,  0.4787,  0.2754,  0.3726,  0.3298,  0.4631,  0.3087,  0.4915,\n           0.4129,  0.4592,  0.4540,  0.4930,  0.5570,  0.2660,  0.2150,  0.2044,\n           0.2766,  0.2076,  0.3278,  0.3727,  0.2637,  0.2493,  0.2782,  0.3664,\n           0.4920,  0.5671,  0.3298,  0.2992,  0.1437,  0.1701,  0.1554,  0.1375,\n           0.1377,  0.3141,  0.2694,  0.2771,  0.2412,  0.3700,  0.5783,  0.5790,\n           0.4229,  0.5032,  0.1216,  0.1000,  0.0356,  0.1258, -0.0023,  0.1640,\n           0.2809,  0.2418,  0.2606,  0.3787,  0.5334,  0.5645,  0.4781,  0.3307,\n           0.2391,  0.0461,  0.0095,  0.0542,  0.1012,  0.1331,  0.2446,  0.2526,\n           0.3323,  0.4120,  0.5724,  0.2840,  0.5188,  0.3934,  0.1328,  0.0776,\n           0.0235,  0.1366,  0.3149,  0.2200,  0.2793,  0.2351,  0.4722,  0.4785,\n           0.4009,  0.4570,  0.4972,  0.5785,  0.2261,  0.1447, -0.0028,  0.2772,\n           0.2697,  0.4008,  0.3606,  0.3372,  0.4535,  0.4492,  0.5678,  0.5870,\n           0.5824,  0.3438,  0.5113,  0.0757,  0.1772,  0.3677,  0.3572,  0.3742,\n           0.3820,  0.4868,  0.3781,  0.4694,  0.5845,  0.5877,  0.5826,  0.3212,\n           0.5276,  0.4840,  0.4825,  0.5523,  0.5308,  0.5085,  0.5606,  0.5720,\n           0.4928,  0.5581,  0.5853,  0.5849,  0.5793,  0.3410,  0.4428,  0.4044,\n           0.3275,  0.4958,  0.4366,  0.5750,  0.5494,  0.5868,  0.5557,  0.5069,\n           0.5880,  0.5888,  0.5796,  0.3377,  0.2635,  0.2347,  0.3145,  0.3486,\n           0.5158,  0.5722,  0.5347,  0.5753,  0.5816,  0.4378,  0.5692,  0.5843,\n           0.5721,  0.5081,  0.2694,  0.2032,  0.1589,  0.3464,  0.5349,  0.5768,\n           0.5739,  0.5764,  0.5394,  0.4482]], grad_fn=&lt;SelectBackward0&gt;),\n True,\n torch.Size([1, 196]))</pre> <p>Beautiful!</p> <p>We've turned our single 2D image into a 1D learnable embedding vector (or \"Linear Projection of Flattned Patches\" in Figure 1 of the ViT paper).</p> In\u00a0[27]: Copied! <pre># 1. Create a class which subclasses nn.Module\nclass PatchEmbedding(nn.Module):\n\"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\" \n    # 2. Initialize the class with appropriate variables\n    def __init__(self, \n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n        \n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method \n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n        \n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched) \n        # 6. Make sure the output shape has the right order \n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\n</pre> # 1. Create a class which subclasses nn.Module class PatchEmbedding(nn.Module):     \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.          Args:         in_channels (int): Number of color channels for the input images. Defaults to 3.         patch_size (int): Size of patches to convert input image into. Defaults to 16.         embedding_dim (int): Size of embedding to turn image into. Defaults to 768.     \"\"\"      # 2. Initialize the class with appropriate variables     def __init__(self,                   in_channels:int=3,                  patch_size:int=16,                  embedding_dim:int=768):         super().__init__()                  # 3. Create a layer to turn an image into patches         self.patcher = nn.Conv2d(in_channels=in_channels,                                  out_channels=embedding_dim,                                  kernel_size=patch_size,                                  stride=patch_size,                                  padding=0)          # 4. Create a layer to flatten the patch feature maps into a single dimension         self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector                                   end_dim=3)      # 5. Define the forward method      def forward(self, x):         # Create assertion to check that inputs are the correct shape         image_resolution = x.shape[-1]         assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"                  # Perform the forward pass         x_patched = self.patcher(x)         x_flattened = self.flatten(x_patched)          # 6. Make sure the output shape has the right order          return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] <p><code>PatchEmbedding</code> layer created!</p> <p>Let's try it out on a single image.</p> In\u00a0[28]: Copied! <pre>set_seeds()\n\n# Create an instance of patch embedding layer\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# Pass a single image through\nprint(f\"Input image shape: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\nprint(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n</pre> set_seeds()  # Create an instance of patch embedding layer patchify = PatchEmbedding(in_channels=3,                           patch_size=16,                           embedding_dim=768)  # Pass a single image through print(f\"Input image shape: {image.unsqueeze(0).shape}\") patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error print(f\"Output patch embedding shape: {patch_embedded_image.shape}\") <pre>Input image shape: torch.Size([1, 3, 224, 224])\nOutput patch embedding shape: torch.Size([1, 196, 768])\n</pre> <p>Beautiful!</p> <p>The output shape matches the ideal input and output shapes we'd like to see from the patch embedding layer:</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a 1D sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>Where:</p> <ul> <li>$(H, W)$ is the resolution of the original image.</li> <li>$C$ is the number of channels.</li> <li>$(P, P)$ is the resolution of each image patch (patch size).</li> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</li> </ul> <p>We've now replicated the patch embedding for equation 1 but not the class token/position embedding.</p> <p>We'll get to these later on.</p> <p>Our <code>PatchEmbedding</code> class (right) replicates the patch embedding of the ViT architecture from Figure 1 and Equation 1 from the ViT paper (left). However, the learnable class embedding and position embeddings haven't been created yet. These will come soon.</p> <p>Let's now get a summary of our <code>PatchEmbedding</code> layer.</p> In\u00a0[29]: Copied! <pre># Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n# summary(PatchEmbedding(), \n#         input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # Create random input sizes random_input_image = (1, 3, 224, 224) random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size  # # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output) # summary(PatchEmbedding(),  #         input_size=random_input_image, # try swapping this for \"random_input_image_error\"  #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) In\u00a0[30]: Copied! <pre># View the patch embedding and patch embedding shape\nprint(patch_embedded_image) \nprint(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # View the patch embedding and patch embedding shape print(patch_embedded_image)  print(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],\n         [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],\n         [-0.7589,  0.2633, -0.1695,  ...,  0.5897, -0.3980,  0.0761],\n         ...,\n         [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],\n         [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],\n         [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],\n       grad_fn=&lt;PermuteBackward0&gt;)\nPatch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>To \"prepend a learnable embedding to the sequence of embedded patches\" we need to create a learnable embedding in the shape of the <code>embedding_dimension</code> ($D$) and then add it to the <code>number_of_patches</code> dimension.</p> <p>Or in pseudocode:</p> <pre>patch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = learnable_embedding\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n</pre> <p>Notice the concatenation (<code>torch.cat()</code>) happens on <code>dim=1</code> (the <code>number_of_patches</code> dimension).</p> <p>Let's create a learnable embedding for the class token.</p> <p>To do so, we'll get the batch size and embedding dimension shape and then we'll create a <code>torch.ones()</code> tensor in the shape <code>[batch_size, 1, embedding_dimension]</code>.</p> <p>And we'll make the tensor learnable by passing it to <code>nn.Parameter()</code> with <code>requires_grad=True</code>.</p> In\u00a0[31]: Copied! <pre># Get the batch size and embedding dimension\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n                           requires_grad=True) # make sure the embedding is learnable\n\n# Show the first 10 examples of the class_token\nprint(class_token[:, :, :10])\n\n# Print the class_token shape\nprint(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\")\n</pre> # Get the batch size and embedding dimension batch_size = patch_embedded_image.shape[0] embedding_dimension = patch_embedded_image.shape[-1]  # Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D) class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]                            requires_grad=True) # make sure the embedding is learnable  # Show the first 10 examples of the class_token print(class_token[:, :, :10])  # Print the class_token shape print(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nClass token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]\n</pre> <p>Note: Here we're only creating the class token embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you'd likely create the class token embedding with <code>torch.randn()</code> (since machine learning is all about harnessing the power of controlled randomness, you generally start with a random number and improve it over time).</p> <p>See how the <code>number_of_tokens</code> dimension of <code>class_token</code> is <code>1</code> since we only want to prepend one class token value to the start of the patch embedding sequence.</p> <p>Now we've got the class token embedding, let's prepend it to our sequence of image patches, <code>patch_embedded_image</code>.</p> <p>We can do so using <code>torch.cat()</code> and set <code>dim=1</code> (so <code>class_token</code>'s <code>number_of_tokens</code> dimension is preprended to <code>patch_embedded_image</code>'s <code>number_of_patches</code> dimension).</p> In\u00a0[32]: Copied! <pre># Add the class token embedding to the front of the patch embedding\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), \n                                                      dim=1) # concat on first dimension\n\n# Print the sequence of patch embeddings with the prepended class token embedding\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the class token embedding to the front of the patch embedding patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),                                                        dim=1) # concat on first dimension  # Print the sequence of patch embeddings with the prepended class token embedding print(patch_embedded_image_with_class_embedding) print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n         [-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],\n         [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],\n         ...,\n         [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],\n         [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],\n         [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],\n       grad_fn=&lt;CatBackward0&gt;)\nSequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Nice! Learnable class token prepended!</p> <p>Reviewing what we've done to create the learnable class token, we start with a sequence of image patch embeddings created by <code>PatchEmbedding()</code> on single image, we then created a learnable class token with one value for each of the embedding dimensions and then prepended it to the original sequence of patch embeddings. Note:* Using <code>torch.ones()</code> to create the learnable class token is mostly for demonstration purposes only, in practice, you'd like create it with <code>torch.randn()</code>.*</p> In\u00a0[33]: Copied! <pre># View the sequence of patch embeddings with the prepended class embedding\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n</pre> # View the sequence of patch embeddings with the prepended class embedding patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape Out[33]: <pre>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n          [-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],\n          [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],\n          ...,\n          [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],\n          [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],\n          [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],\n        grad_fn=&lt;CatBackward0&gt;),\n torch.Size([1, 197, 768]))</pre> <p>Equation 1 states that the position embeddings ($\\mathbf{E}_{\\text {pos }}$) should have the shape $(N + 1) \\times D$:</p> $$\\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}$$<p>Where:</p> <ul> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer (number of patches).</li> <li>$D$ is the size of the patch embeddings, different values for $D$ can be found in Table 1 (embedding dimension).</li> </ul> <p>Luckily we've got both of these values already.</p> <p>So let's make a learnable 1D embedding with <code>torch.ones()</code> to create $\\mathbf{E}_{\\text {pos }}$.</p> In\u00a0[34]: Copied! <pre># Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# Get embedding dimension\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# Create the learnable 1D position embedding\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1, \n                                             embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\nprint(position_embedding[:, :10, :10])\nprint(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2)  # Get embedding dimension embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]  # Create the learnable 1D position embedding position_embedding = nn.Parameter(torch.ones(1,                                              number_of_patches+1,                                               embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding print(position_embedding[:, :10, :10]) print(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nPosition embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Note: Only creating the position embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you'd likely create the position embedding with <code>torch.randn()</code> (start with a random number and improve via gradient descent).</p> <p>Position embeddings created!</p> <p>Let's add them to our sequence of patch embeddings with a prepended class token.</p> In\u00a0[35]: Copied! <pre># Add the position embedding to the patch and class token embedding\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the position embedding to the patch and class token embedding patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding print(patch_and_position_embedding) print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[ 2.0000,  2.0000,  2.0000,  ...,  2.0000,  2.0000,  2.0000],\n         [ 0.0855,  1.2454,  0.7708,  ...,  1.6768,  0.5485,  1.3496],\n         [ 0.2573,  1.1955,  0.6430,  ...,  1.5823,  0.6542,  1.3261],\n         ...,\n         [-0.0072,  1.2795,  0.7196,  ...,  1.7624,  0.5416,  1.3581],\n         [ 0.0161,  1.1652,  0.8424,  ...,  1.7489,  0.4522,  1.3486],\n         [ 0.0740,  1.1383,  0.8843,  ...,  1.5847,  0.5283,  1.3112]]],\n       grad_fn=&lt;AddBackward0&gt;)\nPatch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>Notice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with <code>torch.ones()</code>).</p> <p>Note: We could put both the class token embedding and position embedding into their own layer if we wanted to. But we'll see later on in section 8 how they can be incorporated into the overall ViT architecture's <code>forward()</code> method.</p> <p>The workflow we've used for adding the position embeddings to the sequence of patch embeddings and class token. Note:* <code>torch.ones()</code> only used to create embeddings for illustration purposes, in practice, you'd likely use <code>torch.randn()</code> to start with a random number.*</p> In\u00a0[36]: Copied! <pre>set_seeds()\n\n# 1. Set patch size\npatch_size = 16\n\n# 2. Print shape of original image tensor and get the image dimensions\nprint(f\"Image tensor shape: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. Get image tensor and add batch dimension\nx = image.unsqueeze(0)\nprint(f\"Input image with batch dimension shape: {x.shape}\")\n\n# 4. Create patch embedding layer\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. Pass image through patch embedding layer\npatch_embedding = patch_embedding_layer(x)\nprint(f\"Patching embedding shape: {patch_embedding.shape}\")\n\n# 6. Create class token embedding\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # make sure it's learnable\nprint(f\"Class token embedding shape: {class_token.shape}\")\n\n# 7. Prepend class token embedding to patch embedding\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n\n# 8. Create position embedding\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# 9. Add position embedding to patch embedding with class token\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n</pre> set_seeds()  # 1. Set patch size patch_size = 16  # 2. Print shape of original image tensor and get the image dimensions print(f\"Image tensor shape: {image.shape}\") height, width = image.shape[1], image.shape[2]  # 3. Get image tensor and add batch dimension x = image.unsqueeze(0) print(f\"Input image with batch dimension shape: {x.shape}\")  # 4. Create patch embedding layer patch_embedding_layer = PatchEmbedding(in_channels=3,                                        patch_size=patch_size,                                        embedding_dim=768)  # 5. Pass image through patch embedding layer patch_embedding = patch_embedding_layer(x) print(f\"Patching embedding shape: {patch_embedding.shape}\")  # 6. Create class token embedding batch_size = patch_embedding.shape[0] embedding_dimension = patch_embedding.shape[-1] class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),                            requires_grad=True) # make sure it's learnable print(f\"Class token embedding shape: {class_token.shape}\")  # 7. Prepend class token embedding to patch embedding patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1) print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")  # 8. Create position embedding number_of_patches = int((height * width) / patch_size**2) position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # 9. Add position embedding to patch embedding with class token patch_and_position_embedding = patch_embedding_class_token + position_embedding print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\") <pre>Image tensor shape: torch.Size([3, 224, 224])\nInput image with batch dimension shape: torch.Size([1, 3, 224, 224])\nPatching embedding shape: torch.Size([1, 196, 768])\nClass token embedding shape: torch.Size([1, 1, 768])\nPatch embedding with class token shape: torch.Size([1, 197, 768])\nPatch and position embedding shape: torch.Size([1, 197, 768])\n</pre> <p>Woohoo!</p> <p>From a single image to patch and position embeddings in a single cell of code.</p> <p>Mapping equation 1 from the ViT paper to our PyTorch code. This is the essence of paper replicating, taking a research paper and turning it into usable code.</p> <p>Now we've got a way to encode our images and pass them to the Transformer Encoder in Figure 1 of the ViT paper.</p> <p>Animating the entire ViT workflow: from patch embeddings to transformer encoder to MLP head.</p> <p>From a code perspective, creating the patch embedding is probably the largest section of replicating the ViT paper.</p> <p>Many of the other parts of the ViT paper such as the Multi-Head Attention and Norm layers can be created using existing PyTorch layers.</p> <p>Onwards!</p> In\u00a0[37]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n\"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n        \n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n        \n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=False) # do we need the weights or just the layer outputs?\n        return attn_output\n</pre> # 1. Create a class that inherits from nn.Module class MultiheadSelfAttentionBlock(nn.Module):     \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).     \"\"\"     # 2. Initialize the class with hyperparameters from Table 1     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks         super().__init__()                  # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)                  # 4. Create the Multi-Head Attention (MSA) layer         self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,                                                     num_heads=num_heads,                                                     dropout=attn_dropout,                                                     batch_first=True) # does our batch dimension come first?              # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         attn_output, _ = self.multihead_attn(query=x, # query embeddings                                               key=x, # key embeddings                                              value=x, # value embeddings                                              need_weights=False) # do we need the weights or just the layer outputs?         return attn_output <p>Note: Unlike Figure 1, our <code>MultiheadSelfAttentionBlock</code> doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell-1}$\" in equation 2), we'll include this when we create the entire Transformer Encoder later on in section 7.1.</p> <p>MSABlock created!</p> <p>Let's try it out by create an instance of our <code>MultiheadSelfAttentionBlock</code> and passing through the <code>patch_and_position_embedding</code> variable we created in section 4.8.</p> In\u00a0[38]: Copied! <pre># Create an instance of MSABlock\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1 \n                                                             num_heads=12) # from Table 1\n\n# Pass patch and position image embedding through MSABlock\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\nprint(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")\n</pre> # Create an instance of MSABlock multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1                                                               num_heads=12) # from Table 1  # Pass patch and position image embedding through MSABlock patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding) print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\") print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\") <pre>Input shape of MSA block: torch.Size([1, 197, 768])\nOutput shape MSA block: torch.Size([1, 197, 768])\n</pre> <p>Notice how the input and output shape of our data stays the same when it goes through the MSA block.</p> <p>This doesn't mean the data doesn't change as it goes through.</p> <p>You could try printing the input and output tensor to see how it changes (though this change will be across <code>1 * 197 * 768</code> values and could be hard to visualize).</p> <p>*Left: Vision Transformer architecture from Figure 1 with Multi-Head Attention and LayerNorm layers highlighted, these layers make up equation 2 from section 3.1 of the paper. Right: Replicating equation 2 (without the skip connection on the end) using PyTorch layers.*</p> <p>We've now officially replicated equation 2 (except for the residual connection on the end but we'll get to this in section 7.1)!</p> <p>Onto the next!</p> In\u00a0[39]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MLPBlock(nn.Module):\n\"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n        super().__init__()\n        \n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        # 4. Create the Multilayer perceptron (MLP) layer(s)\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n        )\n    \n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n</pre> # 1. Create a class that inherits from nn.Module class MLPBlock(nn.Module):     \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  dropout:float=0.1): # Dropout from Table 3 for ViT-Base         super().__init__()                  # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)                  # 4. Create the Multilayer perceptron (MLP) layer(s)         self.mlp = nn.Sequential(             nn.Linear(in_features=embedding_dim,                       out_features=mlp_size),             nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"             nn.Dropout(p=dropout),             nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above                       out_features=embedding_dim), # take back to embedding_dim             nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"         )          # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         x = self.mlp(x)         return x <p>Note: Unlike Figure 1, our <code>MLPBlock()</code> doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell}^{\\prime}$\" in equation 3), we'll include this when we create the entire Transformer encoder later on.</p> <p>MLPBlock class created!</p> <p>Let's try it out by create an instance of our <code>MLPBlock</code> and passing through the <code>patched_image_through_msa_block</code> variable we created in section 5.3.</p> In\u00a0[40]: Copied! <pre># Create an instance of MLPBlock\nmlp_block = MLPBlock(embedding_dim=768, # from Table 1 \n                     mlp_size=3072, # from Table 1\n                     dropout=0.1) # from Table 3\n\n# Pass output of MSABlock through MLPBlock\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\nprint(f\"Input shape of MLP block: {patched_image_through_mlp_block.shape}\")\nprint(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")\n</pre> # Create an instance of MLPBlock mlp_block = MLPBlock(embedding_dim=768, # from Table 1                       mlp_size=3072, # from Table 1                      dropout=0.1) # from Table 3  # Pass output of MSABlock through MLPBlock patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block) print(f\"Input shape of MLP block: {patched_image_through_mlp_block.shape}\") print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\") <pre>Input shape of MLP block: torch.Size([1, 197, 768])\nOutput shape MLP block: torch.Size([1, 197, 768])\n</pre> <p>Notice how the input and output shape of our data again stays the same when it goes in and out of the MLP block.</p> <p>However, the shape does change when the data gets passed through the <code>nn.Linear()</code> layers within the MLP block (expanded to MLP size from Table 1 and then compressed back to Hidden size $D$ from Table 1).</p> <p>Left: Vision Transformer architecture from Figure 1 with MLP and Norm layers highlighted, these layers make up equation 3 from section 3.1 of the paper. Right: Replicating equation 3 (without the skip connection on the end) using PyTorch layers.</p> <p>Ho ho!</p> <p>Equation 3 replicated (except for the residual connection on the end but we'll get to this in section 7.1)!</p> <p>Now we've got equation's 2 and 3 in PyTorch code, let's now put them together to create the Transformer Encoder.</p> In\u00a0[41]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass TransformerEncoderBlock(nn.Module):\n\"\"\"Creates a Transformer Encoder block.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                 attn_dropout:float=0): # Amount of dropout for attention layers\n        super().__init__()\n\n        # 3. Create MSA block (equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n        \n        # 4. Create MLP block (equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n        \n    # 5. Create a forward() method  \n    def forward(self, x):\n        \n        # 6. Create residual connection for MSA block (add the input to the output)\n        x =  self.msa_block(x) + x \n        \n        # 7. Create residual connection for MLP block (add the input to the output)\n        x = self.mlp_block(x) + x \n        \n        return x\n</pre> # 1. Create a class that inherits from nn.Module class TransformerEncoderBlock(nn.Module):     \"\"\"Creates a Transformer Encoder block.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                  attn_dropout:float=0): # Amount of dropout for attention layers         super().__init__()          # 3. Create MSA block (equation 2)         self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,                                                      num_heads=num_heads,                                                      attn_dropout=attn_dropout)                  # 4. Create MLP block (equation 3)         self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,                                    mlp_size=mlp_size,                                    dropout=mlp_dropout)              # 5. Create a forward() method       def forward(self, x):                  # 6. Create residual connection for MSA block (add the input to the output)         x =  self.msa_block(x) + x                   # 7. Create residual connection for MLP block (add the input to the output)         x = self.mlp_block(x) + x                   return x <p>Beautiful!</p> <p>Transformer Encoder block created!</p> <p>*Left: Figure 1 from the ViT paper with the Transformer Encoder of the ViT architecture highlighted. Right: Transformer Encoder mapped to equation 2 and 3 of the ViT paper, the Transformer Encoder is comprised of alternating blocks of equation 2 (Multi-Head Attention) and equation 3 (Multilayer perceptron).*</p> <p>See how we're starting to piece together the overall architecture like legos, coding one brick (or equation) at a time.</p> <p>Mapping the ViT Transformer Encoder to code.</p> <p>You might've noticed that Table 1 from the ViT paper has a Layers column. This refers to the number of Transformer Encoder blocks in the specific ViT architecure.</p> <p>In our case, for ViT-Base, we'll be stacking together 12 of these Transformer Encoder blocks to form the backbone of our architecture (we'll get to this in section 8).</p> <p>Let's get a <code>torchinfo.summary()</code> of passing an input of shape <code>(1, 197, 768) -&gt; (batch_size, num_patches, embedding_dimension)</code> to our Transformer Encoder block.</p> In\u00a0[42]: Copied! <pre># Create an instance of TransformerEncoderBlock\ntransformer_encoder_block = TransformerEncoderBlock()\n\n# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n# summary(model=transformer_encoder_block,\n#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # Create an instance of TransformerEncoderBlock transformer_encoder_block = TransformerEncoderBlock()  # # Print an input and output summary of our Transformer Encoder (uncomment for full output) # summary(model=transformer_encoder_block, #         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension) #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>Woah! Check out all those parameters!</p> <p>You can see our input changing shape as it moves through all of the various layers in the MSA block and MLP block of the Transformer Encoder block before finally returning to its original shape at the very end.</p> <p>Note: Just because our input to the Transformer Encoder block has the same shape at the output of the block doesn't mean the values weren't manipulated, the whole goal of the Transformer Encoder block (and stacking them together) is to learn a deep representation of the input using the various layers in between.</p> In\u00a0[43]: Copied! <pre># Create the same as above with torch.nn.TransformerEncoderLayer()\ntorch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n                                                             nhead=12, # Heads from Table 1 for ViT-Base\n                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                                                             activation=\"gelu\", # GELU non-linear activation\n                                                             batch_first=True, # Do our batches come first?\n                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n\ntorch_transformer_encoder_layer\n</pre> # Create the same as above with torch.nn.TransformerEncoderLayer() torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base                                                              nhead=12, # Heads from Table 1 for ViT-Base                                                              dim_feedforward=3072, # MLP size from Table 1 for ViT-Base                                                              dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                                                              activation=\"gelu\", # GELU non-linear activation                                                              batch_first=True, # Do our batches come first?                                                              norm_first=True) # Normalize first or after MSA/MLP layers?  torch_transformer_encoder_layer Out[43]: <pre>TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n)</pre> <p>To inspect it further, let's get a summary with <code>torchinfo.summary()</code>.</p> In\u00a0[44]: Copied! <pre># # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n# summary(model=torch_transformer_encoder_layer,\n#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output) # summary(model=torch_transformer_encoder_layer, #         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension) #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>The output of the summary is slightly different to ours due to how <code>torch.nn.TransformerEncoderLayer()</code> constructs its layer.</p> <p>But the layers it uses, number of parameters and input and output shapes are the same.</p> <p>You might be thinking, \"if we could create the Transformer Encoder so quickly with PyTorch layers, why did we bother reproducing equation 2 and 3?\"</p> <p>The answer is: practice.</p> <p>Now we've replicated a series of equations and layers from a paper, if you need to change the layers and try something different you can.</p> <p>But there are benefits of using the PyTorch pre-built layers, such as:</p> <ul> <li>Less prone to errors - Generally, if a layer makes it into the PyTorch standard library, its been tested and tried to work.</li> <li>Potentially better performance - As of July 2022 and PyTorch 1.12, the PyTorch implemented version of <code>torch.nn.TransformerEncoderLayer()</code> can see a speedup of more than 2x on many common workloads.</li> </ul> <p>Finally, since the ViT architecture uses several Transformer Layers stacked on top of each for the full architecture (Table 1 shows 12 Layers in the case of ViT-Base), you can do this with <code>torch.nn.TransformerEncoder(encoder_layer, num_layers)</code> where:</p> <ul> <li><code>encoder_layer</code> - The target Transformer Encoder layer created with <code>torch.nn.TransformerEncoderLayer()</code>.</li> <li><code>num_layers</code> - The number of Transformer Encoder layers to stack together.</li> </ul> In\u00a0[45]: Copied! <pre># 1. Create a ViT class that inherits from nn.Module\nclass ViT(nn.Module):\n\"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n                 in_channels:int=3, # Number of channels in input image\n                 patch_size:int=16, # Patch size\n                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0, # Dropout for attention projection\n                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers \n                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n                 num_classes:int=1000): # Default for ImageNet but can customize this\n        super().__init__() # don't forget the super().__init__()!\n        \n        # 3. Make the image size is divisble by the patch size \n        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n        \n        # 4. Calculate number of patches (height * width/patch^2)\n        self.num_patches = (img_size * img_size) // patch_size**2\n                 \n        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n        \n        # 6. Create learnable position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n                \n        # 7. Create embedding dropout value\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n        \n        # 8. Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n        \n        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n        # Note: The \"*\" means \"all\"\n        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n       \n        # 10. Create classifier head\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim, \n                      out_features=num_classes)\n        )\n    \n    # 11. Create a forward() method\n    def forward(self, x):\n        \n        # 12. Get batch size\n        batch_size = x.shape[0]\n        \n        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n\n        # 14. Create patch embedding (equation 1)\n        x = self.patch_embedding(x)\n\n        # 15. Concat class embedding and patch embedding (equation 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # 16. Add position embedding to patch embedding (equation 1) \n        x = self.position_embedding + x\n\n        # 17. Run embedding dropout (Appendix B.1)\n        x = self.embedding_dropout(x)\n\n        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)\n        x = self.transformer_encoder(x)\n\n        # 19. Put 0 index logit through classifier (equation 4)\n        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n\n        return x\n</pre> # 1. Create a ViT class that inherits from nn.Module class ViT(nn.Module):     \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  img_size:int=224, # Training resolution from Table 3 in ViT paper                  in_channels:int=3, # Number of channels in input image                  patch_size:int=16, # Patch size                  num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0, # Dropout for attention projection                  mlp_dropout:float=0.1, # Dropout for dense/MLP layers                   embedding_dropout:float=0.1, # Dropout for patch and position embeddings                  num_classes:int=1000): # Default for ImageNet but can customize this         super().__init__() # don't forget the super().__init__()!                  # 3. Make the image size is divisble by the patch size          assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"                  # 4. Calculate number of patches (height * width/patch^2)         self.num_patches = (img_size * img_size) // patch_size**2                           # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)         self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),                                             requires_grad=True)                  # 6. Create learnable position embedding         self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),                                                requires_grad=True)                          # 7. Create embedding dropout value         self.embedding_dropout = nn.Dropout(p=embedding_dropout)                  # 8. Create patch embedding layer         self.patch_embedding = PatchEmbedding(in_channels=in_channels,                                               patch_size=patch_size,                                               embedding_dim=embedding_dim)                  # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())          # Note: The \"*\" means \"all\"         self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,                                                                             num_heads=num_heads,                                                                             mlp_size=mlp_size,                                                                             mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])                 # 10. Create classifier head         self.classifier = nn.Sequential(             nn.LayerNorm(normalized_shape=embedding_dim),             nn.Linear(in_features=embedding_dim,                        out_features=num_classes)         )          # 11. Create a forward() method     def forward(self, x):                  # 12. Get batch size         batch_size = x.shape[0]                  # 13. Create class token embedding and expand it to match the batch size (equation 1)         class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)          # 14. Create patch embedding (equation 1)         x = self.patch_embedding(x)          # 15. Concat class embedding and patch embedding (equation 1)         x = torch.cat((class_token, x), dim=1)          # 16. Add position embedding to patch embedding (equation 1)          x = self.position_embedding + x          # 17. Run embedding dropout (Appendix B.1)         x = self.embedding_dropout(x)          # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)         x = self.transformer_encoder(x)          # 19. Put 0 index logit through classifier (equation 4)         x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index          return x        <ol> <li>\ud83d\udd7a\ud83d\udc83\ud83e\udd73 Woohoo!!! We just built a vision transformer!</li> </ol> <p>What an effort!</p> <p>Slowly but surely we created layers and blocks, inputs and outputs and put them all together to build our own ViT!</p> <p>Let's create a quick demo to showcase what's happening with the class token embedding being expanded over the batch dimensions.</p> In\u00a0[46]: Copied! <pre># Example of creating the class embedding and expanding over a batch dimension\nbatch_size = 32\nclass_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\nclass_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n\n# Print out the change in shapes\nprint(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\") \nprint(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")\n</pre> # Example of creating the class embedding and expanding over a batch dimension batch_size = 32 class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"  # Print out the change in shapes print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")  print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\") <pre>Shape of class token embedding single: torch.Size([1, 1, 768])\nShape of class token embedding expanded: torch.Size([32, 1, 768])\n</pre> <p>Notice how the first dimension gets expanded to the batch size and the other dimensions stay the same (because they're inferred by the \"<code>-1</code>\" dimensions in <code>.expand(batch_size, -1, -1)</code>).</p> <p>Alright time to test out <code>ViT()</code> class.</p> <p>Let's create a random tensor in the same shape as a single image, pass to an instance of <code>ViT</code> and see what happens.</p> In\u00a0[47]: Copied! <pre>set_seeds()\n\n# Create a random tensor with same shape as a single image\nrandom_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n\n# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\nvit = ViT(num_classes=len(class_names))\n\n# Pass the random image tensor to our ViT instance\nvit(random_image_tensor)\n</pre> set_seeds()  # Create a random tensor with same shape as a single image random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)  # Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi) vit = ViT(num_classes=len(class_names))  # Pass the random image tensor to our ViT instance vit(random_image_tensor) Out[47]: <pre>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>Outstanding!</p> <p>It looks like our random image tensor made it all the way through our ViT architecture and it's outputting three logit values (one for each class).</p> <p>And because our <code>ViT</code> class has plenty of parameters we could customize the <code>img_size</code>, <code>patch_size</code> or <code>num_classes</code> if we wanted to.</p> In\u00a0[48]: Copied! <pre>from torchinfo import summary\n\n# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n# summary(model=vit, \n#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Print a summary of our custom ViT model using torchinfo (uncomment for actual output) # summary(model=vit,  #         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width) #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>Now those are some nice looking layers!</p> <p>Checkout the total number of parameters too, 85,800,963, our biggest model yet!</p> <p>The number is very close to PyTorch's pretrained ViT-Base with patch size 16 at <code>torch.vision.models.vit_b_16()</code> with 86,567,656 total parameters (though this number of parameters is for the 1000 classes in ImageNet).</p> <p>Exercise: Try changing the <code>num_classes</code> parameter of our <code>ViT()</code> model to 1000 and then creating another summary with <code>torchinfo.summary()</code> and see if the number of parameters lines up between our code and <code>torchvision.models.vit_b_16()</code>.</p> In\u00a0[49]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \noptimizer = torch.optim.Adam(params=vit.parameters(), \n                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)\n                             weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n\n# Setup the loss function for multi-class classification\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set the seeds\nset_seeds()\n\n# Train the model and save the training results to a dictionary\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)\n</pre> from going_modular.going_modular import engine  # Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper  optimizer = torch.optim.Adam(params=vit.parameters(),                               lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k                              betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)                              weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k  # Setup the loss function for multi-class classification loss_fn = torch.nn.CrossEntropyLoss()  # Set the seeds set_seeds()  # Train the model and save the training results to a dictionary results = engine.train(model=vit,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=10,                        device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 4.8759 | train_acc: 0.2891 | test_loss: 1.0465 | test_acc: 0.5417\nEpoch: 2 | train_loss: 1.5900 | train_acc: 0.2617 | test_loss: 1.5876 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.4644 | train_acc: 0.2617 | test_loss: 1.2738 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.3159 | train_acc: 0.2773 | test_loss: 1.7498 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.3114 | train_acc: 0.3008 | test_loss: 1.7444 | test_acc: 0.2604\nEpoch: 6 | train_loss: 1.2445 | train_acc: 0.3008 | test_loss: 1.9704 | test_acc: 0.1979\nEpoch: 7 | train_loss: 1.2050 | train_acc: 0.3984 | test_loss: 3.5480 | test_acc: 0.1979\nEpoch: 8 | train_loss: 1.4368 | train_acc: 0.4258 | test_loss: 1.8324 | test_acc: 0.2604\nEpoch: 9 | train_loss: 1.5757 | train_acc: 0.2344 | test_loss: 1.2848 | test_acc: 0.5417\nEpoch: 10 | train_loss: 1.4658 | train_acc: 0.4023 | test_loss: 1.2389 | test_acc: 0.2604\n</pre> <p>Wonderful!</p> <p>Our ViT model has come to life!</p> <p>Though the results on our pizza, steak and sushi dataset don't look too good.</p> <p>Perhaps it's because we're missing a few things?</p> In\u00a0[50]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Plot our ViT model's loss curves\nplot_loss_curves(results)\n</pre> from helper_functions import plot_loss_curves  # Plot our ViT model's loss curves plot_loss_curves(results) <p>Hmm, it looks like our model's loss curves are all over the place.</p> <p>At least the loss looks like it's heading the right direction but the accuracy curves don't really show much promise.</p> <p>These results are likely because of the difference in data resources and training regime of our ViT model versus the ViT paper.</p> <p>It seems our model is severly underfitting (not achieving the results we'd like it to).</p> <p>How about we see if we can fix that by bringing in a pretrained ViT model?</p> In\u00a0[51]: Copied! <pre># The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__) \nprint(torchvision.__version__)\n</pre> # The following requires torch v0.12+ and torchvision v0.13+ import torch import torchvision print(torch.__version__)  print(torchvision.__version__) <pre>1.12.0+cu102\n0.13.0+cu102\n</pre> <p>Then we'll setup device-agonistc code.</p> In\u00a0[52]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[52]: <pre>'cuda'</pre> <p>Finally, we'll get the pretrained ViT-Base with patch size 16 from <code>torchvision.models</code> and prepare it for our FoodVision Mini use case by turning it into a feature extractor transfer learning model.</p> <p>Specifically, we'll:</p> <ol> <li>Get the pretrained weights for ViT-Base trained on ImageNet-1k from <code>torchvision.models.ViT_B_16_Weights.DEFAULT</code> (<code>DEFAULT</code> stands for best available).</li> <li>Setup a ViT model instance via <code>torchvision.models.vit_b_16</code>, pass it the pretrained weights step 1 and send it to the target device.</li> <li>Freeze all of the parameters in the base ViT model created in step 2 by setting their <code>requires_grad</code> attribute to <code>False</code>.</li> <li>Update the classifier head of the ViT model created in step 2 to suit our own problem by changing the number of <code>out_features</code> to our number of classes (pizza, steak, sushi).</li> </ol> <p>We covered steps like this in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model and section 3.4: Freezing the base model and changing the output layer to suit our needs.</p> In\u00a0[53]: Copied! <pre># 1. Get pretrained weights for ViT-Base\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available\n\n# 2. Setup a ViT model instance with pretrained weights\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# 3. Freeze the base parameters\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n    \n# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n# pretrained_vit # uncomment for model output\n</pre> # 1. Get pretrained weights for ViT-Base pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available  # 2. Setup a ViT model instance with pretrained weights pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)  # 3. Freeze the base parameters for parameter in pretrained_vit.parameters():     parameter.requires_grad = False      # 4. Change the classifier head (set the seeds to ensure same initialization with linear head) set_seeds() pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device) # pretrained_vit # uncomment for model output  <p>Pretrained ViT feature extractor model created!</p> <p>Let's now check it out by printing a <code>torchinfo.summary()</code>.</p> In\u00a0[54]: Copied! <pre># # Print a summary using torchinfo (uncomment for actual output)\n# summary(model=pretrained_vit, \n#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> # # Print a summary using torchinfo (uncomment for actual output) # summary(model=pretrained_vit,  #         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width) #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>Woohoo!</p> <p>Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen).</p> <p>And the total number of parameters, 85,800,963, is the same as our custom made ViT model above.</p> <p>But the number of trainable parameters for <code>pretrained_vit</code> is much, much lower than our custom <code>vit</code> at only 2,307 compared to 85,800,963 (in our custom <code>vit</code>, since we're training from scratch, all parameters are trainable).</p> <p>This means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory.</p> In\u00a0[55]: Copied! <pre>from helper_functions import download_data\n\n# Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> from helper_functions import download_data  # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[55]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>And now we'll setup the training and test directory paths.</p> In\u00a0[56]: Copied! <pre># Setup train and test directory paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\" \ntrain_dir, test_dir\n</pre> # Setup train and test directory paths train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir Out[56]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> <p>Finally, we'll transform our images into tensors and turn the tensors into DataLoaders.</p> <p>Since we're using a pretrained model form <code>torchvision.models</code> we can call the <code>transforms()</code> method on it to get its required transforms.</p> <p>Remember, if you're going to use a pretrained model, it's generally important to ensure your own custom data is transformed/formatted in the same way the data the original model was trained on.</p> <p>We covered this method of \"automatic\" transform creation in 06. PyTorch Transfer Learning section 2.2.</p> In\u00a0[57]: Copied! <pre># Get automatic transforms from pretrained ViT weights\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n</pre> # Get automatic transforms from pretrained ViT weights pretrained_vit_transforms = pretrained_vit_weights.transforms() print(pretrained_vit_transforms) <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> <p>And now we've got transforms ready, we can turn our images into DataLoaders using the <code>data_setup.create_dataloaders()</code> method we created in 05. PyTorch Going Modular section 2.</p> <p>Since we're using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we'd be mimicing an improvement found in Better plain ViT baselines for ImageNet-1k, a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we'll stick with 32.</p> In\u00a0[58]: Copied! <pre># Setup dataloaders\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n</pre> # Setup dataloaders train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                      test_dir=test_dir,                                                                                                      transform=pretrained_vit_transforms,                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)  In\u00a0[59]: Copied! <pre>from going_modular.going_modular import engine\n\n# Create optimizer and loss function\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train the classifier head of the pretrained ViT feature extractor model\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n</pre> from going_modular.going_modular import engine  # Create optimizer and loss function optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),                               lr=1e-3) loss_fn = torch.nn.CrossEntropyLoss()  # Train the classifier head of the pretrained ViT feature extractor model set_seeds() pretrained_vit_results = engine.train(model=pretrained_vit,                                       train_dataloader=train_dataloader_pretrained,                                       test_dataloader=test_dataloader_pretrained,                                       optimizer=optimizer,                                       loss_fn=loss_fn,                                       epochs=10,                                       device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7665 | train_acc: 0.7227 | test_loss: 0.5432 | test_acc: 0.8665\nEpoch: 2 | train_loss: 0.3428 | train_acc: 0.9453 | test_loss: 0.3263 | test_acc: 0.8977\nEpoch: 3 | train_loss: 0.2064 | train_acc: 0.9531 | test_loss: 0.2707 | test_acc: 0.9081\nEpoch: 4 | train_loss: 0.1556 | train_acc: 0.9570 | test_loss: 0.2422 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.1246 | train_acc: 0.9727 | test_loss: 0.2279 | test_acc: 0.8977\nEpoch: 6 | train_loss: 0.1216 | train_acc: 0.9766 | test_loss: 0.2129 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.0938 | train_acc: 0.9766 | test_loss: 0.2352 | test_acc: 0.8883\nEpoch: 8 | train_loss: 0.0797 | train_acc: 0.9844 | test_loss: 0.2281 | test_acc: 0.8778\nEpoch: 9 | train_loss: 0.1098 | train_acc: 0.9883 | test_loss: 0.2074 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.0650 | train_acc: 0.9883 | test_loss: 0.1804 | test_acc: 0.9176\n</pre> <p>Holy cow!</p> <p>Looks like our pretrained ViT feature extractor performed far better than our custom ViT model trained from scratch (in the same amount of time).</p> <p>Let's get visual.</p> In\u00a0[60]: Copied! <pre># Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)\n</pre> # Plot the loss curves from helper_functions import plot_loss_curves  plot_loss_curves(pretrained_vit_results)  <p>Woah!</p> <p>Those are some close to textbook looking (really good) loss curves (check out 04. PyTorch Custom Datasets section 8 for what an ideal loss curve should look like).</p> <p>That's the power of transfer learning!</p> <p>We managed to get outstanding results with the same model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.</p> <p>What do you think?</p> <p>Would our feature extractor model improve more if you kept training it?</p> In\u00a0[61]: Copied! <pre># Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n</pre> # Save the model from going_modular.going_modular import utils  utils.save_model(model=pretrained_vit,                  target_dir=\"models\",                  model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\") <pre>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\n</pre> <p>And since we're thinking about deploying this model, it'd be good to know the size of it (in megabytes or MB).</p> <p>Since we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.</p> <p>We can check the size of our model in bytes using the <code>st_size</code> attribute of Python's <code>pathlib.Path().stat()</code> method whilst passing it our model's filepath name.</p> <p>We can then scale the size in bytes to megabytes.</p> In\u00a0[62]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <pre>Pretrained ViT feature extractor model size: 327 MB\n</pre> <p>Hmm, looks like our ViT feature extractor model for Food Vision Mini turned out to be about 327 MB in size.</p> <p>How does this compare to the EffNetB2 feature extractor model in 07. PyTorch Experiment Tracking section 9?</p> Model Model size (MB) Test loss Test accuracy EffNetB2 feature extractor^ 29 ~0.3906 ~0.9384 ViT feature extractor 327 ~0.1084 ~0.9384 <p>Note: ^ the EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.</p> <p>The EffNetB2 model is ~11x smaller than the ViT model with similiar results for test loss and accuracy.</p> <p>However, the ViT model's results may improve more when trained with the same data (20% pizza, steak and sushi data).</p> <p>But in terms of deployment, if we were comparing these two models, something we'd need to consider is whether the extra accuracy from the ViT model is worth the ~11x increase in model size?</p> <p>Perhaps such a large model would take longer to load/run and wouldn't provide as good an experience as EffNetB2 which performs similarly but at a much reduced size.</p> In\u00a0[63]: Copied! <pre>import requests\n\n# Import function to make predictions on images and plot them \nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Setup custom image path\ncustom_image_path = image_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=pretrained_vit,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> import requests  # Import function to make predictions on images and plot them  from going_modular.going_modular.predictions import pred_and_plot_image  # Setup custom image path custom_image_path = image_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=pretrained_vit,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/pizza_steak_sushi/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>Two thumbs up!</p> <p>Congratulations!</p> <p>We've gone all the way from research paper to usable model code on our own custom images!</p>"},{"location":"08_pytorch_paper_replicating/#08-pytorch-paper-replicating","title":"08. PyTorch Paper Replicating\u00b6","text":"<p>Welcome to Milestone Project 2: PyTorch Paper Replicating!</p> <p>In this project, we're going to be replicating a machine learning research paper and creating a Vision Transformer (ViT) from scratch using PyTorch.</p> <p>We'll then see how ViT, a state-of-the-art computer vision architecture, performs on our FoodVision Mini problem.</p> <p>For Milestone Project 2 we're going to focus on recreating the Vision Transformer (ViT) computer vision architecture and applying it to our FoodVision Mini problem to classify different images of pizza, steak and sushi.</p>"},{"location":"08_pytorch_paper_replicating/#what-is-paper-replicating","title":"What is paper replicating?\u00b6","text":"<p>It's no secret machine learning is advancing fast.</p> <p>Many of these advances get published in machine learning research papers.</p> <p>And the goal of paper replicating is to take replicate these advances with code so you can use the techniques for your own problem.</p> <p>For example, let's say a new model architecture gets released that performs better than any other architecture before on various benchmarks, wouldn't it be nice to try that architecture on your own problems?</p> <p>Machine learning paper replicating involves turning a machine learning paper comprised of images/diagrams, math and text into usable code and in our case, usable PyTorch code. Diagram, math equations and text from the ViT paper.</p>"},{"location":"08_pytorch_paper_replicating/#what-is-a-machine-learning-research-paper","title":"What is a machine learning research paper?\u00b6","text":"<p>A machine learning research paper is a scientific paper that details findings of a research group on a specific area.</p> <p>The contents of a machine learning research paper can vary from paper to paper but they generally follow the structure:</p> Section Contents Abstract An overview/summary of the paper's main findings/contributions. Introduction What's the paper's main problem and details of previous methods used to try and solve it. Method How did the researchers go about conducting their research? For example, what model(s), data sources, training setups were used? Results What are the outcomes of the paper? If a new type of model or training setup was used, how did the results of findings compare to previous works? (this is where experiment tracking comes in handy) Conclusion What are the limitations of the suggested methods? What are some next steps for the research community? References What resources/other papers did the researchers look at to build their own body of work? Appendix Are there any extra resources/findings to look at that weren't included in any of the above sections?"},{"location":"08_pytorch_paper_replicating/#why-replicate-a-machine-learning-research-paper","title":"Why replicate a machine learning research paper?\u00b6","text":"<p>A machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.</p> <p>And if these experiments lead to better results in an area related to the problem you're working on, it'd be nice to them out.</p> <p>Also, replicating the work of others is a fantastic way to practice your skills.</p> <p>George Hotz is founder of comma.ai, a self-driving car company and livestreams machine learning coding on Twitch and those videos get posted in full to YouTube. I pulled this quote from one of his livestreams. The \"\u066d\" is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).</p> <p>When you first start trying to replicate research papers, you'll likely be overwhelmed.</p> <p>That's normal.</p> <p>Research teams spend weeks, months and sometimes years creating these works so it makes sense if it takes you sometime to even read let alone reproduce the works.</p> <p>Replicating research is such a tough problem, phenomenal machine learning libraries and tools such as, HuggingFace, PyTorch Image Models (<code>timm</code> library) and fast.ai have been born out of making machine learning research more accessible.</p>"},{"location":"08_pytorch_paper_replicating/#where-can-you-find-code-examples-for-machine-learning-research-papers","title":"Where can you find code examples for machine learning research papers?\u00b6","text":"<p>One of the first things you'll notice when it comes to machine learning research is: there's a lot of it.</p> <p>So beware, trying to stay on top of it is like trying to outrun a hamster wheel.</p> <p>Follow your interest, pick a few things that stand out to you.</p> <p>In saying this, there are several places to find and read machine learning research papers (and code):</p> Resource What is it? arXiv Pronounced \"archive\", arXiv is a free and open resource for reading technical articles on everything from physics to computer science (inlcuding machine learning). AK Twitter The AK Twitter account publishes machine learning research highlights, often with live demos almost every day. I don't understand 9/10 posts but I find it fun to explore every so often. Papers with Code A curated collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models. lucidrains' <code>vit-pytorch</code> GitHub repository Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale and with a specific focus looks like. The <code>vit-pytorch</code> repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code (much of the inspiration for this notebook was gathered from this repository). <p>Note: This list is far from exhaustive. I only list a few places, the ones I use most frequently personally. So beware the bias. However, I've noticed that even this short list often sully satisfies my needs for knowing what's going on in the field. Anymore and I might go crazy.</p>"},{"location":"08_pytorch_paper_replicating/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Rather than talk about a replicating a paper, we're going to get hands-on and actually replicate a paper.</p> <p>The process for replicating all papers will be slightly different but by seeing what it's like to do one, we'll get the momentum to do more.</p> <p>More specifically, we're going to be replicating the machine learning research paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale  (ViT paper) with PyTorch.</p> <p>The Transformer neural network architecture was originally introduced in the machine learning research paper Attention is all you need.</p> <p>And the original Transformer architecture was designed to work on one-dimensional (1D) sequences of text.</p> <p>A Transformer architecture is generally considered to be any neural network that uses the attention mechanism as its primary learning layer. Similar to a how a convolutional neural network (CNN) uses convolutions as its primary learning layer.</p> <p>Like the name suggests, the Vision Transformer (ViT) architecture was designed to adapt the original Transformer architecture to vision problem(s) (classification being the first and since many others have followed).</p> <p>The original Vision Transformer has been through several iterations over the past couple of years, however, we're going to focus on replicating the original, otherwise known as the \"vanilla Vision Transformer\". Because if you can recreate the original, you can adapt to the others.</p> <p>We're going to be focusing on building the ViT architecture as per the original ViT paper and applying it to FoodVision Mini.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's get the pizza, steak and sushi image classification dataset we've been using and build a Vision Transformer to try and improve FoodVision Mini model's results. 2. Create Datasets and DataLoaders We'll use the <code>data_setup.py</code> script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. 3. Replicating the ViT paper: an overview Replicating a machine learning research paper can be bit a fair challenge, so before we jump in, let's break the ViT paper down into smaller chunks, so we can replicate the paper chunk by chunk. 4. Equation 1: The Patch Embedding The ViT architecture is comprised of four main equations, the first being the patch and position embedding. Or turning an image into a sequence of learnable patches. 5. Equation 2: Multi-Head Attention (MSA) The self-attention/multi-head self-attention (MSA) mechanism is at the heart of every Transformer architecture, including the ViT architecture, let's create an MSA block using PyTorch's in-built layers. 6. Equation 3: Multilayer Perceptron (MLP) The ViT architecture uses a multilayer perceptron as part of its Transformer Encoder and for its output layer. Let's start by creating an MLP for the Transformer Encoder. 7. Creating the Transformer Encode A Transformer Encoder is typically comprised of alternating layers of MSA (equation 2) and MLP (equation 3) joined together via residual connections. Let's create one by stacking the layers we created in sections 5 &amp; 6 on top of each other. 8. Putting it all together to create ViT We've got all the pieces of the puzzle to create the ViT architecture, let's put them all together into a single class we can call as our model. 9. Setting up training code for our ViT model Training our custom ViT implementation is similar to all of the other model's we've trained previously. And thanks to our <code>train()</code> function in <code>engine.py</code> we can start training with a few lines of code. 10. Using a pretrained ViT from <code>torchvision.models</code> Training a large model like ViT usually takes a fair amount of data. Since we're only working with a small amount of pizza, steak and sushi images, let's see if we can leverage the power of transfer learning to improve our performance. 11. Make predictions on a custom image The magic of machine learning is seeing it work on your own data, so let's take our best performing model and put FoodVision Mini to the test on the infamous pizza-dad image (a photo of my dad eating pizza). <p>Note: Despite the fact we're going to be focused on replicating the ViT paper, avoid getting too bogged down on a particular paper as newer better methods will often come along, quickly, so the skill should be to remain curious whilst building the fundamental skills of turning math and words on a page into working code.</p>"},{"location":"08_pytorch_paper_replicating/#terminology","title":"Terminology\u00b6","text":"<p>There are going to be a fair few acronyms throughout this notebook.</p> <p>In light of this, here are some definitions:</p> <ul> <li>ViT - Stands for Vision Transformer (the main neural network architecture we're going to be focused on replicating).</li> <li>ViT paper - Short hand for the original machine learning research paper that introduced the ViT architecture, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, anytime ViT paper is mentioned, you can be assured it is referencing this paper.</li> </ul>"},{"location":"08_pytorch_paper_replicating/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"08_pytorch_paper_replicating/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>As we've done previously, let's make sure we've got all of the modules we'll need for this section.</p> <p>We'll import the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in 05. PyTorch Going Modular.</p> <p>To do so, we'll download <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>And since later on we'll be using <code>torchvision</code> v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.</p>"},{"location":"08_pytorch_paper_replicating/#1-get-data","title":"1. Get Data\u00b6","text":"<p>Since we're continuing on with FoodVision Mini, let's download the pizza, steak and sushi image dataset we've been using.</p> <p>To do so we can use the <code>download_data()</code> function from <code>helper_functions.py</code> that we created in 07. PyTorch Experiment Tracking section 1.</p> <p>We'll <code>source</code> to the raw GitHub link of the <code>pizza_steak_sushi.zip</code> data and the <code>destination</code> to <code>pizza_steak_sushi</code>.</p>"},{"location":"08_pytorch_paper_replicating/#2-create-datasets-and-dataloaders","title":"2. Create Datasets and DataLoaders\u00b6","text":"<p>Now we've got some data, let's now turn it into <code>DataLoader</code>'s.</p> <p>To do so we can use the <code>create_dataloaders()</code> function in <code>data_setup.py</code>.</p> <p>First, we'll create a transform to prepare our images.</p> <p>This where one of the first references to the ViT paper will come in.</p> <p>In Table 3, the training resolution is mentioned as being 224 (height=224, width=224).</p> <p>You can often find various hyperparameter settings listed in a table. In this case we're still preparing our data, so we're mainly concerned with things like image size and batch size. Source: Table 3 in ViT paper.</p> <p>So we'll make sure our transform resizes our images appropriately.</p> <p>And since we'll be training our model from scratch (no transfer learning to begin with), we won't provide a <code>normalize</code> transform like we did in 06. PyTorch Transfer Learning section 2.1.</p>"},{"location":"08_pytorch_paper_replicating/#21-prepare-transforms-for-images","title":"2.1 Prepare transforms for images\u00b6","text":""},{"location":"08_pytorch_paper_replicating/#22-turn-images-into-dataloaders","title":"2.2 Turn images into <code>DataLoader</code>'s\u00b6","text":"<p>Transforms created!</p> <p>Let's now create our <code>DataLoader</code>'s.</p> <p>The ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size we've been using (32).</p> <p>However, we're going to stick with a batch size of 32.</p> <p>Why?</p> <p>Because some hardware (including the free tier of Google Colab) may not be able to handle a batch size of 4096.</p> <p>Having a batch size of 4096 means that 4096 images need to fit into the GPU memory at a time.</p> <p>This works when you've got the hardware to handle it like a research team from Google often does but when you're running on a single GPU (such as using Google Colab), making sure things work with smaller batch size first is a good idea.</p> <p>An extension of this project could be to try a higher batch size value and see what happens.</p> <p>Note: We're using the <code>pin_memory=True</code> parameter in the <code>create_dataloaders()</code> function to speed up computation. <code>pin_memory=True</code> avoids unnecessary copying of memory between the CPU and GPU memory by \"pinning\" examples that have been seen before. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). However, setting <code>pin_memory=True</code> doesn't always improve performance (this is another one of those we're scenarios in machine learning where some things work sometimes and don't other times), so best to experiment, experiment, experiment. See the PyTorch <code>torch.utils.data.DataLoader</code> documentation or Making Deep Learning Go Brrrr from First Principles by Horace He for more.</p>"},{"location":"08_pytorch_paper_replicating/#23-visualize-a-single-image","title":"2.3 Visualize a single image\u00b6","text":"<p>Now we've loaded our data, let's visualize, visualize, visualize!</p> <p>An important step in the ViT paper is preparing the images into patches.</p> <p>We'll get to what this means in section 4 but for now, let's view a single image and its label.</p> <p>To do so, let's get a single image and label from a batch of data and inspect their shapes.</p>"},{"location":"08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview","title":"3. Replicating the ViT paper: an overview\u00b6","text":"<p>Before we write anymore code, let's discuss what we're doing.</p> <p>We'd like to replicate the ViT paper for our own problem, FoodVision Mini.</p> <p>So our model inputs are: images of pizza, steak and sushi.</p> <p>And our ideal model outputs are: predicted labels of pizza, steak or sushi.</p> <p>No different to what we've been doing throughout the previous sections.</p> <p>The question is: how do we go from our inputs to the desired outputs?</p>"},{"location":"08_pytorch_paper_replicating/#31-inputs-and-outputs-layers-and-blocks","title":"3.1 Inputs and outputs, layers and blocks\u00b6","text":"<p>ViT is a deep learning neural network architecture.</p> <p>And any neural network architecture is generally comprised of layers.</p> <p>And a collection of layers is often referred to as a block.</p> <p>And stacking many blocks together is what gives us the whole architecture.</p> <p>A layer takes an input (say an image tensor), performs some kind of function on it (for example what's in the layer's <code>forward()</code> method) and then returns an output.</p> <p>So if a single layer takes an input and gives an output, then a collection of layers or a block also takes an input and gives an output.</p> <p>Let's make this concrete:</p> <ul> <li>Layer - takes an input, performs a function on it, returns an output.</li> <li>Block - a collection of layers, takes an input, performs a series of functions on it, returns an output.</li> <li>Architecture (or model) - a collection of blocks, takes an input, performs a series of functions on it, returns an output.</li> </ul> <p>This ideology is what we're going to be using to replicate the ViT paper.</p> <p>We're going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.</p> <p>The reason we do this is because looking at a whole research paper can be intimidating.</p> <p>So for a better understanding, we'll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.</p> <p>A modern deep learning architecture is usually collection of layers and blocks. Where layers take an input (data as a numerical representation) and manipulate it using some kind of function (for example, the self-attention formula pictured above, however, this function could be almost anything) and then output it. Blocks are generally stacks of layers on top of each other doing a similar thing to a single layer but multiple times.</p>"},{"location":"08_pytorch_paper_replicating/#32-getting-specific-whats-vit-made-of","title":"3.2 Getting specific: What's ViT made of?\u00b6","text":"<p>There are many little details about the ViT model sprinkled throughout the paper.</p> <p>Finding them all is like one big treasure hunt!</p> <p>Remember, a research paper is often months of work compressed into a few pages so it's understandable for it to take of practice to replicate.</p> <p>However, the main three resources we'll be looking at for the architecture design are:</p> <ol> <li>Figure 1 - This gives an overview of the model in a graphical sense, you could almost recreate the architecture with this figure alone.</li> <li>Four equations in section 3.1 - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.</li> <li>Table 1 - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We'll be focused on the smallest version, ViT-Base.</li> </ol>"},{"location":"08_pytorch_paper_replicating/#321-exploring-figure-1","title":"3.2.1 Exploring Figure 1\u00b6","text":"<p>Let's start by going through Figure 1 of the ViT Paper.</p> <p>The main things we'll be paying attention to are:</p> <ol> <li>Layers - takes an input, performs an operation or function on the input, produces an output.</li> <li>Blocks - a collection of layers, which in turn also takes an input and produces an output.</li> </ol> <p>Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.</p> <p>The ViT architecture is comprised of several stages:</p> <ul> <li>Patch + Position Embedding (inputs) - Turns the input image into a sequence of image patches and add a position number what order the patch comes in.</li> <li>Linear projection of flattened patches (Embedded Patches) - The image patches get turned into an embedding, the benefit of using an embedding rather than just the image values is that an embedding is a learnable representation (typically in the form of a vector) of the image that can improve with training.</li> <li>Norm - This is short for \"Layer Normalization\" or \"LayerNorm\", a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer <code>torch.nn.LayerNorm()</code>.</li> <li>Multi-Head Attention - This is a Multi-Headed Self-Attention layer or \"MSA\" for short. You can create an MSA layer via the PyTorch layer <code>torch.nn.MultiheadAttention()</code>.</li> <li>MLP (or Multilayer perceptron) - A MLP can often refer to any collection of feedforward layers (or in PyTorch's case, a collection of layers with a <code>forward()</code> method). In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two <code>torch.nn.Linear()</code> layers with a <code>torch.nn.GELU()</code> non-linearity activation in between them (section 3.1) and a <code>torch.nn.Dropout()</code> layer after each (Appendex B.1).</li> <li>Transformer Encoder - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the \"+\" symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.</li> <li>MLP Head - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block.</li> </ul> <p>You might notice that many of the pieces of the ViT architecture can be created with existing PyTorch layers.</p> <p>This is because of how PyTorch is designed, it's one of the main purposes of PyTorch to create reusable neural network layers for both researchers and machine learning practitioners.</p> <p>Question: Why not code everything from scratch?</p> <p>You could definitely do that by reproducing all of the math equations from the paper with custom PyTorch layers and that would certainly be an educative exercise, however, using pre-existing PyTorch layers is usually favoured as pre-existing layers have often been extensively tested and performance checked to make sure they run correctly and fast.</p> <p>Note: We're going to focused on write PyTorch code to create these layers, for the background on what each of these layers does, I'd suggest reading the ViT Paper in full or reading the linked resources for each layer.</p> <p>Let's take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.</p> <p>Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class \"pizza\" is returned.</p>"},{"location":"08_pytorch_paper_replicating/#322-exploring-the-four-equations","title":"3.2.2 Exploring the Four Equations\u00b6","text":"<p>The next main part(s) of the ViT paper we're going to look at are the four equations in section 3.1.</p> <p>These four equations represent the math behind the four major parts of the ViT architecture.</p> <p>Section 3.1 describes each of these (some of the text has been omitted for brevity, bolded text is mine):</p> Equation number Description from ViT paper section 3.1 1 ...The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings... Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings... 2 The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019). 3 Same as equation 2. 4 Similar to BERT's [ class ] token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4)... <p>Let's map these descriptions to the ViT architecture in Figure 1.</p> <p>Connecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks.</p> <p>There's a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.</p> <p>How about we break down each equation further (it will be our goal to recreate these with code)?</p> <p>In all equations (except equation 4), \"$\\mathbf{z}$\" is the raw output of a particular layer:</p> <ol> <li>$\\mathbf{z}_{0}$ is \"z zero\" (this is the output of the initial patch embedding layer).</li> <li>$\\mathbf{z}_{\\ell}^{\\prime}$ is \"z of a particular layer prime\" (or an intermediary value of z).</li> <li>$\\mathbf{z}_{\\ell}$ is \"z of a particular layer\".</li> </ol> <p>And $\\mathbf{y}$ is the overall output of the architecture.</p>"},{"location":"08_pytorch_paper_replicating/#323-equation-1-overview","title":"3.2.3 Equation 1 overview\u00b6","text":"$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$<p>This equation deals with the class token, patch embedding and position embedding ($\\mathbf{E}$ is for embedding) of the input image.</p> <p>In vector form, the embedding might look something like:</p> <pre>x_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n</pre> <p>Where each of the elements in the vector is learnable (their <code>requires_grad=True</code>).</p>"},{"location":"08_pytorch_paper_replicating/#324-equation-2-overview","title":"3.2.4 Equation 2 overview\u00b6","text":"$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$<p>This says that for every layer from $1$ through to $L$ (the total number of layers), there's a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).</p> <p>The addition on the end is the equivalent of adding the input to the output and forming a skip/residual connection.</p> <p>We'll call this layer the \"MSA block\".</p> <p>In pseudocode, this might look like:</p> <pre>x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n</pre> <p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>"},{"location":"08_pytorch_paper_replicating/#325-equation-3-overview","title":"3.2.5 Equation 3 overview\u00b6","text":"$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\\\ \\end{aligned} $$<p>This says that for every layer from $1$ through to $L$ (the total number of layers), there's also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).</p> <p>The addition on the end is showing the presence of a skip/residual connection.</p> <p>We'll call this layer the \"MLP block\".</p> <p>In pseudocode, this might look like:</p> <pre>x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n</pre> <p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>"},{"location":"08_pytorch_paper_replicating/#326-equation-4-overview","title":"3.2.6 Equation 4 overview\u00b6","text":"$$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$<p>This says for the last layer $L$, the output $y$ is the 0 index token of $z$ wrapped in a LayerNorm layer (LN).</p> <p>Or in our case, the 0 index of <code>x_output_MLP_block</code>:</p> <pre>y = Linear_layer(LN_layer(x_output_MLP_block[0]))\n</pre> <p>Of course there are some simplifications above but we'll take care of those when we start to write PyTorch code for each section.</p> <p>Note: The above section covers alot of information. But don't forget if something doesn't make sense, you can always research it further. By asking questions like \"what is a residual connection?\".</p>"},{"location":"08_pytorch_paper_replicating/#327-exploring-table-1","title":"3.2.7 Exploring Table 1\u00b6","text":"<p>The final piece of the ViT architecture puzzle we'll focus on (for now) is Table 1.</p> Model Layers Hidden size $D$ MLP size Heads Params ViT-Base 12 768 3072 12 $86M$ ViT-Large 24 1024 4096 16 $307M$ ViT-Huge 32 1280 5120 16 $632M$ Table 1: Details of Vision Transformer model variants. Source: ViT paper. <p>This table showcasing the various hyperparameters of each of the ViT architectures.</p> <p>You can see the numbers gradually increase from ViT-Base to ViT-Huge.</p> <p>We're going to focus on replicating ViT-Base (start small and scale up when necessary) but we'll be writing code that could easily scale up to the larger variants.</p> <p>Breaking the hyperparameters down:</p> <ul> <li>Layers - How many Transformer Encoder blocks are there? (each of these will contain a MSA block and MLP block)</li> <li>Hidden size $D$ - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more compute.</li> <li>MLP size - What are the number of hidden units in the MLP layers?</li> <li>Heads - How many heads are there in the Multi-Head Attention layers?</li> <li>Params - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more compute. You'll notice even ViT-Base has far more parameters than any other model we've used so far.</li> </ul> <p>We'll use these values as the hyperparameter settings for our ViT architecture.</p>"},{"location":"08_pytorch_paper_replicating/#33-my-workflow-for-replicating-papers","title":"3.3 My workflow for replicating papers\u00b6","text":"<p>When I start working on replicating a paper, I go through the following steps:</p> <ol> <li>Read the whole paper end-to-end once (to get an idea of the main concepts).</li> <li>Go back through each section and see how they line up with each other and start thinking about how they might be turned into code (just like above).</li> <li>Repeat step 2 until I've got a fairly good outline.</li> <li>Use mathpix.com (a very handy tool) to turn any sections of the paper into markdown/LaTeX to put into notebooks.</li> <li>Replicate the simplest version of the model possible.</li> <li>If I get stuck, look up other examples.</li> </ol> <p>Turning the four equations from the ViT paper into editable LaTeX/markdown using mathpix.com.</p> <p>We've already gone through the first few steps above (and if you haven't read the full paper yet, I'd encourage you to give it a go) but what we'll be focusing on next is step 5: replicating the simplest version fo the model possible.</p> <p>This is why we're starting with ViT-Base.</p> <p>Replicating the smallest version of the architecture possible, get it working and then we can scale up if we wanted to.</p> <p>Note: If you've never read a research paper before, many of the above steps can be intimidating. But don't worry, like anything, your skills at reading and replicating papers will improve with practice. Don't forget, a research paper is often months of work by many people compressed into a few pages. So trying to replicate it on your own is no small feat.</p>"},{"location":"08_pytorch_paper_replicating/#4-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding","title":"4. Equation 1: Split data into patches and creating the class, position and patch embedding\u00b6","text":"<p>I remember one of my machine learning engineer friends used to say \"it's all about the embedding.\"</p> <p>As in, if you can represent your data in a good, learnable way (as embeddings are learnable representations), chances are, a learning algorithm will be able to perform well on them.</p> <p>With that being said, let's start by creating the class, position and patch embeddings for the ViT architecture.</p> <p>We'll start with the patch embedding.</p> <p>This means we'll be turning our input images in a sequence of patches and then embedding those patches.</p> <p>Recall that an embedding is a learnable representation of some form and is often a vector.</p> <p>The term learnable is important because this means the numerical representation of an input image (that the model sees) can be improved over time.</p> <p>We'll begin by following the opening paragraph of section 3.1 of the ViT paper (bold mine):</p> <p>The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.</p> <p>And size we're dealing with image shapes, let's keep in mind the line from Table 3 of the ViT paper:</p> <p>Training resolution is 224.</p> <p>Let's break down the text above.</p> <ul> <li>$D$ is the size of the patch embeddings, different values for $D$ for various sized ViT models can be found in Table 1.</li> <li>The image starts as 2D with size ${H \\times W \\times C}$.<ul> <li>$(H, W)$ is the resolution of the original image (height, width).</li> <li>$C$ is the number of channels.</li> </ul> </li> <li>The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.<ul> <li>$(P, P)$ is the resolution of each image patch (patch size).</li> <li>$N=H W / P^{2}$ is the resulting number of patches, which also serves as the input sequence length for the Transformer.</li> </ul> </li> </ul> <p>Mapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.</p>"},{"location":"08_pytorch_paper_replicating/#41-calculating-patch-embedding-input-and-output-shapes-by-hand","title":"4.1 Calculating patch embedding input and output shapes by hand\u00b6","text":"<p>How about we start by calculating these input and output shape values by hand?</p> <p>To do so, let's create some variables to mimic each of the terms (such as $H$, $W$ etc) above.</p> <p>We'll use a patch size ($P$) of 16 since it's the best performing version of ViT-Base uses (see column \"ViT-B/16\" of Table 5 in the ViT paper for more).</p>"},{"location":"08_pytorch_paper_replicating/#42-turning-a-single-image-into-patches","title":"4.2 Turning a single image into patches\u00b6","text":"<p>Now we know the ideal input and output shapes for our patch embedding layer, let's move towards making it.</p> <p>What we're doing is breaking down the overall architecture into smaller pieces, focusing on the inputs and outputs of individual layers.</p> <p>So how do we create the patch embedding layer?</p> <p>We'll get to that shortly, first, let's visualize, visualize, visualize! what it looks like to turn an image into patches.</p> <p>Let's start with our single image.</p>"},{"location":"08_pytorch_paper_replicating/#43-creating-image-patches-with-torchnnconv2d","title":"4.3 Creating image patches with <code>torch.nn.Conv2d()</code>\u00b6","text":"<p>We've seen what an image looks like when it gets turned into patches, now let's start moving towards replicating the patch embedding layers with PyTorch.</p> <p>To visualize our single image we wrote code to loop through the different height and width dimensions of a single image and plot individual patches.</p> <p>This operation is very similar to the convolutional operation we saw in 03. PyTorch Computer Vision section 7.1: Stepping through <code>nn.Conv2d()</code>.</p> <p>In fact, the authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):</p> <p>Hybrid Architecture. As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection $\\mathbf{E}$ (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size $1 \\times 1$, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.</p> <p>The \"feature map\" they're refering to are the weights/activations produced by a convolutional layer passing over a given image.</p> <p>By setting the <code>kernel_size</code> and <code>stride</code> parameters of a <code>torch.nn.Conv2d()</code> layer equal to the <code>patch_size</code>, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a \"Linear Projection\" in the ViT paper) of each patch.</p> <p>Remember our ideal input and output shapes for the patch embedding layer?</p> <ul> <li>Input: The image starts as 2D with size ${H \\times W \\times C}$.</li> <li>Output: The image gets converted to a 1D sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.</li> </ul> <p>Or for an image size of 224 and patch size of 16:</p> <ul> <li>Input (2D image): (224, 224, 3) -&gt; (height, width, color channels)</li> <li>Output (flattened 2D patches): (196, 768) -&gt; (number of patches, embedding dimension)</li> </ul> <p>We can recreate these with:</p> <ul> <li><code>torch.nn.Conv2d()</code> for turning our image into patches of CNN feature maps.</li> <li><code>torch.nn.Flatten()</code> for flattening the spatial dimensions of the feature map.</li> </ul> <p>Let's start with the <code>torch.nn.Conv2d()</code> layer.</p> <p>We can replicate the creation of patches by setting the <code>kernel_size</code> and <code>stride</code> equal to <code>patch_size</code>.</p> <p>This means each convolutional kernel will be of size <code>(patch_size x patch_size)</code> or if <code>patch_size=16</code>, <code>(16 x 16)</code> (the equivalent of one whole patch).</p> <p>And each step or <code>stride</code> of the convolutional kernel will be <code>patch_size</code> pixels long or <code>16</code> pixels long (equivalent of stepping to the next patch).</p> <p>We'll set <code>in_channels=3</code> for the number of color channels in our image and we'll set <code>out_channels=768</code>, the same as the $D$ value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a learnable vector of size 768).</p>"},{"location":"08_pytorch_paper_replicating/#44-flattening-the-patch-embedding-with-torchnnflatten","title":"4.4 Flattening the patch embedding with <code>torch.nn.Flatten()</code>\u00b6","text":"<p>We've turned our image into patch embeddings but they're still in 2D format.</p> <p>How do we get them into the desired output shape of the patch embedding layer of the ViT model?</p> <ul> <li>Desried output (1D sequence of flattened 2D patches): (196, 768) -&gt; (number of patches, embedding dimension) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> </ul> <p>Let's check the current shape.</p>"},{"location":"08_pytorch_paper_replicating/#45-turning-the-vit-patch-embedding-layer-into-a-pytorch-module","title":"4.5 Turning the ViT patch embedding layer into a PyTorch module\u00b6","text":"<p>Time to put everything we've done for creating the patch embedding into a single PyTorch layer.</p> <p>We can do so by subclassing <code>nn.Module</code> and creating a small PyTorch \"model\" to do all of the steps above.</p> <p>Specifically we'll:</p> <ol> <li>Create a class called <code>PatchEmbedding</code> which subclasses <code>nn.Module</code> (so it can be used a PyTorch layer).</li> <li>Initialize the class with the parameters <code>in_channels=3</code>, <code>patch_size=16</code> (for ViT-Base) and <code>embedding_dim=768</code> (this is $D$ for ViT-Base from Table 1).</li> <li>Create a layer to turn an image into patches using <code>nn.Conv2d()</code> (just like in 4.3 above).</li> <li>Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above).</li> <li>Define a <code>forward()</code> method to take an input and pass it through the layers created in 3 and 4.</li> <li>Make sure the output shape reflects the required output shape of the ViT architecture (${N \\times\\left(P^{2} \\cdot C\\right)}$).</li> </ol> <p>Let's do it!</p>"},{"location":"08_pytorch_paper_replicating/#46-creating-the-class-token-embedding","title":"4.6 Creating the class token embedding\u00b6","text":"<p>Okay we've made the image patch embedding, time to get to work on the class token embedding.</p> <p>Or $\\mathbf{x}_\\text {class }$ from equation 1.</p> <p>Left: Figure 1 from the ViT paper with the \"classification token\" or <code>[class]</code> embedding token we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.</p> <p>Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:</p> <p>Similar to BERT's <code>[ class ]</code> token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4).</p> <p>Note: BERT (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a <code>[ class ]</code> token at the start of a sequence originated, class being a description for the \"classification\" class the sequence belonged to.</p> <p>So we need to \"preprend a learnable embedding to the sequence of embedded patches\".</p> <p>Let's start by viewing our sequence of embedded patches tensor (created in section 4.5) and its shape.</p>"},{"location":"08_pytorch_paper_replicating/#47-creating-the-position-embedding","title":"4.7 Creating the position embedding\u00b6","text":"<p>Well, we've got the class token embedding and the patch embedding, now how might we create the position embedding?</p> <p>Or $\\mathbf{E}_{\\text {pos }}$ from equation 1 where $E$ stands for \"embedding\".</p> <p>Left: Figure 1 from the ViT paper with the position embedding we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.</p> <p>Let's find out more by reading section 3.1 of the ViT paper (bold mine):</p> <p>Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p> <p>By \"retain positional information\" the authors mean they want the architecture to know what \"order\" the patches come in. As in, patch two comes after patch one and patch three comes after patch two and on and on.</p> <p>This positional information can be important when considering what's in an image (without positional information an a flattened sequence could be seen as having no order and thus no patch relates to any other patch).</p> <p>To start creating the position embeddings, let's view our current embeddings.</p>"},{"location":"08_pytorch_paper_replicating/#48-putting-it-all-together-from-image-to-embedding","title":"4.8 Putting it all together: from image to embedding\u00b6","text":"<p>Alright, we've come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:</p> $$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$<p>Let's now put everything together in a single code cell and go from input image ($\\mathbf{x}$) to output embedding ($\\mathbf{z}_0$).</p> <p>We can do so by:</p> <ol> <li>Setting the patch size (we'll use <code>16</code> as it's widely used throughout the paper and for ViT-Base).</li> <li>Getting a single image, printing its shape and storing its height and width.</li> <li>Adding a batch dimension to the single image so it's compatible with our <code>PatchEmbedding</code> layer.</li> <li>Creating a <code>PatchEmbedding</code> layer (the one we made in section 4.5) with a <code>patch_size=16</code> and <code>embedding_dim=768</code> (from Table 1 for ViT-Base).</li> <li>Passing the single image through the <code>PatchEmbedding</code> layer in 4 to create a sequence of patch embeddings.</li> <li>Creating a class token embedding like in section 4.6.</li> <li>Prepending the class token emebdding to the patch embeddings created in step 5.</li> <li>Creating a position embedding like in section 4.7.</li> <li>Adding the position embedding to the class token and patch embeddings created in step 7.</li> </ol> <p>We'll also make sure to set the random seeds with <code>set_seeds()</code> and print out the shapes of different tensors along the way.</p>"},{"location":"08_pytorch_paper_replicating/#5-equation-2-multi-head-attention-msa","title":"5. Equation 2: Multi-Head Attention (MSA)\u00b6","text":"<p>We've got our input data patchified and embedded, now let's move onto the next part of the ViT architecture.</p> <p>To start, we'll break down the Transformer Encoder section into two parts (start small and increase when necessary).</p> <p>The first being equation 2 and the second being equation 3.</p> <p>Recall equation 2 states:</p> $$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$<p>This indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output of the layer).</p> <p>We'll refer to equation 2 as the \"MSA block\".</p> <p>*Left: Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.*</p> <p>Many layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch.</p> <p>In saying this, to replicate these layers and residual connection with PyTorch code we can use:</p> <ul> <li>Multi-Head Self Attention (MSA) - <code>torch.nn.MultiheadAttention()</code>.</li> <li>Norm (LN or LayerNorm) - <code>torch.nn.LayerNorm()</code>.</li> <li>Residual connection - add the input to output (we'll see this later on when we create the full Transformer Encoder block in section 7.1).</li> </ul>"},{"location":"08_pytorch_paper_replicating/#51-the-layernorm-ln-layer","title":"5.1 The LayerNorm (LN) layer\u00b6","text":"<p>Layer Normalization (<code>torch.nn.LayerNorm()</code> or Norm or LayerNorm or LN) normalizes an input over the last dimension.</p> <p>You can find the formal definition of <code>torch.nn.LayerNorm()</code> in the PyTorch documentation.</p> <p>PyTorch's <code>torch.nn.LayerNorm()</code>'s main parameter is <code>normalized_shape</code> which we can set to be equal to the dimension size we'd like to noramlize over (in our case it'll be $D$ or <code>768</code> for ViT-Base).</p> <p>What does it do?</p> <p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).</p> <p>I like to think of any kind of normalization as \"getting the data into a similar format\" or \"getting data samples into a similar distribution\".</p> <p>Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p> <p>It'd take some adjustment on each step right?</p> <p>And what you learn for each step wouldn't necessary help with the next one since they all differ, increasing the time it takes you to navigate the stairs.</p> <p>Normalization (including Layer Normalization) is the equivalent of making all the stairs the same height and length except the stairs are your data samples.</p> <p>So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard-deviations) easier than those with varying distributions.</p>"},{"location":"08_pytorch_paper_replicating/#52-the-multi-head-self-attention-msa-layer","title":"5.2 The Multi-Head Self Attention (MSA) layer\u00b6","text":"<p>The power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the Attention is all you need research paper.</p> <p>Originally designed for text inputs, the original self-attention mechanism takes a sequence of words and then calculates which word should pay more \"attention\" to another word.</p> <p>In other words, in the sentence \"the dog jumped over the fence\", perhaps the word \"dog\" relates strongly to \"jumped\" and \"fence\".</p> <p>This is simplified but the premise remains for images.</p> <p>Since our input is a sequence of image patches rather than words, self-attention and in turn multi-head attention will calculate which patch of an image is most related to another patch, eventually forming a learned representation of an image.</p> <p>But what's most important is that the layer does this on it's own given the data (we don't tell it what patterns to learn).</p> <p>And if the learned representation the layers form using MSA are good, we'll see the results in our model's performance.</p> <p>There are many resources online to learn more about the Transformer architeture and attention mechanism online such as Jay Alammar's wonderful Illustrated Transformer post and Illustrated Attention post.</p> <p>We're going to focus more on coding an existing PyTorch MSA implementation than creating our own.</p> <p>However, you can find the formal defintion of the ViT paper's MSA implementation is defined in Appendix A:</p> <p>*Left: Vision Transformer architecture overview from Figure 1 of the ViT paper. Right: Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.*</p> <p>The image above highlights the triple embedding input to the MSA layer.</p> <p>This is known as query, key, value input or qkv for short which is fundamental to the self-attention mechanism.</p> <p>In our case, the triple embedding input will be three versions of the output of the Norm layer, one for query, key and value.</p> <p>Or three versions of our layer-normalized image patch and position embeddings created in section 4.8.</p> <p>We can implement the MSA layer in PyTorch with <code>torch.nn.MultiheadAttention()</code> with the parameters:</p> <ul> <li><code>embed_dim</code> - the embedding dimension from Table 1 (Hidden size $D$).</li> <li><code>num_heads</code> - how many attention heads to use (this is where the term \"multihead\" comes from), this value is also in Table 1 (Heads).</li> <li><code>dropout</code> - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isn't used after the qkv-projections).</li> <li><code>batch_first</code> - does our batch dimension come first? (yes it does)</li> </ul>"},{"location":"08_pytorch_paper_replicating/#53-replicating-equation-2-with-pytorch-layers","title":"5.3 Replicating Equation 2 with PyTorch layers\u00b6","text":"<p>Let's put everything we've discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>MultiheadSelfAttentionBlock</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.</li> <li>Create a layer normalization (LN) layer with <code>torch.nn.LayerNorm()</code> with the <code>normalized_shape</code> parameter the same as our embedding dimension ($D$ from Table 1).</li> <li>Create a multi-head attention (MSA) layer with the appropriate <code>embed_dim</code>, <code>num_heads</code>, <code>dropout</code> and <code>batch_first</code> parameters.</li> <li>Create a <code>forward()</code> method for our class passing the in the inputs through the LN layer and MSA layer.</li> </ol>"},{"location":"08_pytorch_paper_replicating/#6-equation-3-multilayer-perceptron-mlp","title":"6. Equation 3: Multilayer Perceptron (MLP)\u00b6","text":"<p>We're on a roll here!</p> <p>Let's keep it going and replicate equation 3:</p> $$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$<p>Here MLP stands for \"multilayer perceptron\" and LN stands for \"layer normalization\" (as discussed above).</p> <p>And the addition on the end is the skip/residual connection.</p> <p>We'll refer to equation 3 as the \"MLP block\" of the Transformer encoder (notice how we're continuing the trend of breaking down the architecture into smaller chunks).</p> <p>*Left: Figure 1 from the ViT paper with MLP and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the multilayer perceptron (MLP) layer, Norm layer (LN) and residual connection to their respective parts of equation 3 in the ViT paper.*</p>"},{"location":"08_pytorch_paper_replicating/#61-the-mlp-layers","title":"6.1 The MLP layer(s)\u00b6","text":"<p>The term MLP is quite broad as it can refer to almost any combination of multiple layers (hence the \"multi\" in multilayer perceptron).</p> <p>But it generally follows the pattern of:</p> <p><code>linear layer -&gt; non-linear layer -&gt; linear layer -&gt; non-linear layer</code></p> <p>In the the case of the ViT paper, the MLP structure is defined in section 3.1:</p> <p>The MLP contains two layers with a GELU non-linearity.</p> <p>Where \"two layers\" refers to linear layers (<code>torch.nn.Linear()</code> in PyTorch) and \"GELU non-linearity\" is the GELU  (Gaussian Error Linear Units) non-linear activation function (<code>torch.nn.GELU()</code> in PyTorch).</p> <p>Note: A linear layer (<code>torch.nn.Linear()</code>) can sometimes also be referred to as a \"dense layer\" or \"feedforward layer\". Some papers even use all three terms to describe the same thing (as in the ViT paper).</p> <p>Another sneaky detail about the MLP block doesn't appear until Appendix B.1 (Training):</p> <p>Table 3 summarizes our training setups for our different models. ...Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.</p> <p>This means that every linear layer (or dense layer) in the MLP block has a dropout layer (<code>torch.nn.Dropout()</code> in PyTorch).</p> <p>The value of which can be found in Table 3 of the ViT paper (for ViT-Base, <code>dropout=0.1</code>).</p> <p>Knowing this, the structure of our MLP block will be:</p> <p><code>layer norm -&gt; linear layer -&gt; non-linear layer -&gt; dropout -&gt; linear layer -&gt; dropout</code></p> <p>With hyperparameter values for the linear layers available from Table 1 (MLP size is the number of hidden units between the linear layers and hidden size $D$ is the output size of the MLP block).</p>"},{"location":"08_pytorch_paper_replicating/#62-replicating-equation-3-with-pytorch-layers","title":"6.2 Replicating Equation 3 with PyTorch layers\u00b6","text":"<p>Let's put everything we've discussed about the LayerNorm (LN) and MLP (MSA) layers in equation 3 into practice.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>MLPBlock</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</li> <li>Create a layer normalization (LN) layer with <code>torch.nn.LayerNorm()</code> with the <code>normalized_shape</code> parameter the same as our embedding dimension ($D$ from Table 1).</li> <li>Create a sequential series of MLP layers(s) using <code>torch.nn.Linear()</code>, <code>torch.nn.Dropout()</code> and <code>torch.nn.GELU()</code> with appropriate hyperparameter values from Table 1 and Table 3.</li> <li>Create a <code>forward()</code> method for our class passing the in the inputs through the LN layer and MLP layer(s).</li> </ol>"},{"location":"08_pytorch_paper_replicating/#7-create-the-transformer-encoder","title":"7. Create the Transformer Encoder\u00b6","text":"<p>Time to stack together our <code>MultiheadSelfAttentionBlock</code> (equation 2) and <code>MLPBlock</code> (equation 3) and create the Transformer Encoder of the ViT architecture.</p> <p>In deep learning, an \"encoder\" or \"auto encoder\" generally refers to a stack of layers that \"encodes\" an input (turns it into some form of numerical representation).</p> <p>In our case, the Transformer Encoder will encode our patched image embedding into a learned representation using a series of alternating layers of MSA blocks and MLP blocks, as per section 3.1 of the ViT Paper:</p> <p>The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski &amp; Auli, 2019).</p> <p>We've created MSA and MLP blocks but what about the residual connections?</p> <p>Residual connections (also called skip connections), were first introduced in the paper Deep Residual Learning for Image Recognition and are achieved by adding a layer(s) input to its subsequent output.</p> <p>Where the subsequence output might be one or more layers later.</p> <p>In the case of the ViT architecture, the residual connection means the input of the MSA block is added back to the output of the MSA block before it passes to the MLP block.</p> <p>And the same thing happens with the MLP block before it goes onto the next Transformer Encoder block.</p> <p>Or in pseudocode:</p> <p><code>x_input -&gt; MSA_block -&gt; [MSA_block_output + x_input] -&gt; MLP_block -&gt; [MLP_block_output + MSA_block_output + x_input] -&gt; ...</code></p> <p>What does this do?</p> <p>One of the main ideas behind residual connections is that they prevent weight values and gradient updates from getting too small and thus allow deeper networks and in turn allow deeper representations to be learned.</p> <p>Note: The iconic computer vision architecture \"ResNet\" is named so because of the introduction of residual connections. You can find many pretrained versions of ResNet architectures in <code>torchvision.models</code>.</p>"},{"location":"08_pytorch_paper_replicating/#71-creating-a-transformer-encoder-by-combining-our-custom-made-layers","title":"7.1 Creating a Transformer Encoder by combining our custom made layers\u00b6","text":"<p>Enough talk, let's see this in action and make a ViT Transformer Encoder with PyTorch by combining our previously created layers.</p> <p>To do so, we'll:</p> <ol> <li>Create a class called <code>TransformerEncoderBlock</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</li> <li>Instantiate a MSA block for equation 2 using our <code>MultiheadSelfAttentionBlock</code> from section 5.2 with the appropriate parameters.</li> <li>Instantiate a MLP block for equation 3 using our <code>MLPBlock</code> from section 6.2 with the appropriate parameters.</li> <li>Create a <code>forward()</code> method for our <code>TransformerEncoderBlock</code> class.</li> <li>Create a residual connection for the MSA block (for equation 2).</li> <li>Create a residual connection for the MLP block (for equation 3).</li> </ol>"},{"location":"08_pytorch_paper_replicating/#72-creating-a-transformer-encoder-with-pytorchs-transformer-layers","title":"7.2 Creating a Transformer Encoder with PyTorch's Transformer layers\u00b6","text":"<p>So far we've built the components of and the Transformer Encoder layer itself ourselves.</p> <p>But because of their rise in popularity and effectiveness, PyTorch now has in-built Transformer layers as part of <code>torch.nn</code>.</p> <p>For example, we can recreate the <code>TransformerEncoderBlock</code> we just created using <code>torch.nn.TransformerEncoderLayer()</code> and setting the same hyperparameters as above.</p>"},{"location":"08_pytorch_paper_replicating/#8-putting-it-all-together-to-create-vit","title":"8. Putting it all together to create ViT\u00b6","text":"<p>Alright, alright, alright, we've come a long way!</p> <p>But now it's time to do the exciting thing of putting together all of the pieces of the puzzle.</p> <p>We're going to combine all of the blocks we've created to replicate the full ViT architecture.</p> <p>From the patch and positional embedding to the Transformer Encoder(s) to the MLP Head.</p> <p>But wait, we haven't created equation 4 yet...</p> $$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$<p>Don't worry, we can put equation 4 into our overall ViT architecture class.</p> <p>All we need is a <code>torch.nn.LayerNorm()</code> layer and a <code>torch.nn.Linear()</code> layer to convert the 0th index ($\\mathbf{z}_{L}^{0}$) of the Transformer Encoder logit outputs to the target number of classes we have.</p> <p>To create the full architecture, we'll also need to stack a number of our <code>TransformerEncoderBlock</code>s on top of each other, we can do this by passing a list of them to <code>torch.nn.Sequential()</code> (this will make a sequential range of <code>TransformerEncoderBlock</code>s).</p> <p>We'll focus on the ViT-Base hyperparameters from Table 1 but our code should be adaptable to other ViT variants.</p> <p>Creating ViT will be our biggest code block yet but we can do it!</p> <p>Finally, to bring our own implementation of ViT to life, let's:</p> <ol> <li>Create a class called <code>ViT</code> that inherits from <code>torch.nn.Module</code>.</li> <li>Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.</li> <li>Make sure the image size is divisible by the patch size (the image should be split into even patches).</li> <li>Calculate the number of patches using the formula $N=H W / P^{2}$, where $H$ is the image height, $W$ is the image width and $P$ is the patch size.</li> <li>Create a learnable class embedding token (equation 1) as done above in section 4.6.</li> <li>Create a learnable position embedding vector (equation 1) as done above in section 4.7.</li> <li>Setup the embedding dropout layer as discussed in Appendix B.1 of the ViT paper.</li> <li>Create the patch embedding layer using the <code>PatchEmbedding</code> class as above in section 4.5.</li> <li>Create a series of Transformer Encoder blocks by passing a list of <code>TransformerEncoderBlock</code>s created in section 7.1 to <code>torch.nn.Sequential()</code> (equations 2 &amp; 3).</li> <li>Create the MLP head (also called classifier head or equation 4) by passing a <code>torch.nn.LayerNorm()</code> (LN) layer and a <code>torch.nn.Linear(out_features=num_classes)</code> layer (where <code>num_classes</code> is the target number of classes) linear layer to <code>torch.nn.Sequential()</code>.</li> <li>Create a <code>forward()</code> method that accepts an input.</li> <li>Get the batch size of the input (the first dimension of the shape).</li> <li>Create the patching embedding using the layer created in step 8 (equation 1).</li> <li>Create the class token embedding using the layer created in step 5 and expand it across the number of batches found in step 11 using <code>torch.Tensor.expand()</code> (equation 1).</li> <li>Concatenate the class token embedding created in step 13 to the first dimension of the patch embedding created in step 12 using <code>torch.cat()</code> (equation 1).</li> <li>Add the position embedding created in step 6 to the patch and class token embedding created in step 14 (equation 1).</li> <li>Pass the patch and position embedding through the dropout layer created in step 7.</li> <li>Pass the patch and position embedding from step 16 through the stack of Transformer Encoder layers created in step 9 (equations 2 &amp; 3).</li> <li>Pass index 0 of the output of the stack of Transformer Encoder layers from step 17 through the classifier head created in step 10 (equation 4).</li> <li>Dance and shout woohoo!!! We just built a Vision Transformer!</li> </ol> <p>You ready?</p> <p>Let's go.</p>"},{"location":"08_pytorch_paper_replicating/#81-getting-a-visual-summary-of-our-vit-model","title":"8.1 Getting a visual summary of our ViT model\u00b6","text":"<p>We handcrafted our own version of the ViT architecture and seen that a random image tensor can flow all the way through it.</p> <p>How about we use <code>torchinfo.summary()</code> to get a visual overview of the input and output shapes of all the layers in our model?</p> <p>Note: The ViT paper states the use of a batch size of 4096 for training, however, this requires a far bit of CPU/GPU compute memory to handle (the larger the batch size the more memory required). So to make sure we don't get memory errors, we'll stick with a batch size of 32. You could always increase this later if you have access to hardware with more memory.</p>"},{"location":"08_pytorch_paper_replicating/#9-setting-up-training-code-for-our-vit-model","title":"9. Setting up training code for our ViT model\u00b6","text":"<p>Ok time for the easy part.</p> <p>Training!</p> <p>Why easy?</p> <p>Because we've got most of what we need ready to go, from our model (<code>vit</code>) to our DataLoaders (<code>train_dataloader</code>, <code>test_dataloader</code>) to the training functions we created in 05. PyTorch Going Modular section 4.</p> <p>To train our model we can import the <code>train()</code> function from <code>going_modular.going_modular.engine</code>.</p> <p>All we need is a loss function and an optimizer.</p>"},{"location":"08_pytorch_paper_replicating/#91-creating-an-optimizer","title":"9.1 Creating an optimizer\u00b6","text":"<p>Searching the ViT paper for \"optimizer\", section 4.1 on Training &amp; Fine-tuning states:</p> <p>Training &amp; Fine-tuning. We train all models, including ResNets, using Adam (Kingma &amp; Ba, 2015 ) with $\\beta_{1}=0.9, \\beta_{2}=0.999$, a batch size of 4096 and apply a high weight decay of $0.1$, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting).</p> <p>So we can see they chose to use the \"Adam\" optimizer (<code>torch.optim.Adam()</code>) rather than SGD (stochastic gradient descent, <code>torch.optim.SGD()</code>).</p> <p>The authors set Adam's $\\beta$ (beta) values to $\\beta_{1}=0.9, \\beta_{2}=0.999$, these are the default values for the <code>betas</code> parameter in <code>torch.optim.Adam(betas=(0.9, 0.999))</code>.</p> <p>They also state the use of weight decay (slowly reducing the values of the weights during optimization to prevent overfitting), we can set this with the <code>weight_decay</code> parameter in <code>torch.optim.Adam(weight_decay=0.3)</code> (according to the setting of ViT-* trained on ImageNet-1k).</p> <p>We'll set the learning rate of the optimizer to 0.003 as per Table 3 (according to the setting of ViT-* trained on ImageNet-1k).</p> <p>And as discussed previously, we're going to use a lower batch size than 4096 due to hardware limitations (if you have a large GPU, feel free to increase this).</p>"},{"location":"08_pytorch_paper_replicating/#92-creating-a-loss-function","title":"9.2 Creating a loss function\u00b6","text":"<p>Strangely, searching the ViT paper for \"loss\" or \"loss function\" or \"criterion\" returns no results.</p> <p>However, since the target problem we're working with is multi-class classification (the same for the ViT paper), we'll use <code>torch.nn.CrossEntropyLoss()</code>.</p>"},{"location":"08_pytorch_paper_replicating/#93-training-our-vit-model","title":"9.3 Training our ViT model\u00b6","text":"<p>Okay, now we know what optimizer and loss function we're going to use, let's setup the training code for training our ViT.</p> <p>We'll start by importing the <code>engine.py</code> script from <code>going_modular.going_modular</code> then we'll setup the optimizer and loss function and finally we'll use the <code>train()</code> function from <code>engine.py</code> to train our ViT model for 10 epochs (we're using a smaller number of epochs than the ViT paper to make sure everything works).</p>"},{"location":"08_pytorch_paper_replicating/#94-what-our-training-setup-is-missing","title":"9.4 What our training setup is missing\u00b6","text":"<p>The original ViT architecture achieves good results on several image classification benchmarks (on par or better than many state-of-the-art results when it was released).</p> <p>However, our results (so far) aren't as good.</p> <p>There's a few reasons this could be but the main one is scale.</p> <p>The original ViT paper uses a far larger amount of data than ours (in deep learning, more data is generally always a good thing) and a longer training schedule (see Table 3).</p> Hyperparameter value ViT Paper Our implementation Number of training images 1.3M (ImageNet-1k), 14M (ImageNet-21k), 303M (JFT) 225 Epochs 7 (for largest dataset), 90, 300 (for ImageNet) 10 Batch size 4096 32 Learning rate warmup 10k steps (Table 3) None Learning rate decay Linear/Cosine (Table 3) None Gradient clipping Global norm 1 (Table 3) None <p>Even though our ViT architecture is the same as the paper, the results from the ViT paper were achieved using far more data and a more elaborate training scheme than ours.</p> <p>Because of the size of the ViT architecture and its high number of parameters (increased learning capabilities), and amount of data it uses (increased learning opportunities), many of the techniques used in the ViT paper training scheme such as learning rate warmup, learning rate decay and gradient clipping are specifically designed to prevent overfitting (regularization).</p> <p>Note: For any technique you're unsure of, you can often quickly find an example by searching \"pytorch TECHNIQUE NAME\", for exmaple, say you wanted to learn about learning rate warmup and what it does, you could search \"pytorch learning rate warmup\".</p> <p>Good news is, there are many pretrained ViT models (using vast amounts of data) available online, we'll see one in action in section 10.</p>"},{"location":"08_pytorch_paper_replicating/#95-plot-the-loss-curves-of-our-vit-model","title":"9.5 Plot the loss curves of our ViT model\u00b6","text":"<p>We've trained our ViT model and seen the results as numbers on a page.</p> <p>But let's now follow the data explorer's motto of visualize, visualize, visualize!</p> <p>And one of the best things to visualize for a model is its loss curves.</p> <p>To check out our ViT model's loss curves, we can use the <code>plot_loss_curves</code> function from <code>helper_functions.py</code> we created in 04. PyTorch Custom Datasets section 7.8.</p>"},{"location":"08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset","title":"10. Using a pretrained ViT from <code>torchvision.models</code> on the same dataset\u00b6","text":"<p>We've discussed the benefits of using pretrained models in 06. PyTorch Transfer Learning.</p> <p>But since we've now trained our own ViT from scratch and achieved less than optimal results, the benefits of transfer learning (using a pretrained model) really shine.</p>"},{"location":"08_pytorch_paper_replicating/#101-why-use-a-pretrained-model","title":"10.1 Why use a pretrained model?\u00b6","text":"<p>An important note on many modern machine learning research papers is that much of the results are obtained with large datasets and vast compute resources.</p> <p>And in modern day machine learning, the original fully trained ViT would likely not be considered a \"super large\" training setup (models are continually getting bigger and bigger).</p> <p>Reading the ViT paper section 4.2:</p> <p>Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days.</p> <p>As of July 2022, the price for renting a TPUv3 (Tensor Processing Unit version 3) with 8 cores on Google Cloud is $8 USD per hour.</p> <p>To rent one for 30 straight days would cost $5,760 USD.</p> <p>This cost (monetary and time) may be viable for some larger research teams or enterprises but for many people it's not.</p> <p>So having a pretrained model available through resources like <code>torchvision.models</code>, the <code>timm</code> (Torch Image Models) library, the HuggingFace Hub or even from the authors of the papers themselves (there's a growing trend for machine learning researchers to release the code and pretrained models from their research papers, I'm a big fan of this trend, many of these resources can be found on Paperswithcode.com).</p> <p>If you're focused on leveraging the benefits of a specific model architecture rather than creating your custom architecture, I'd highly recommend using a pretrained model.</p>"},{"location":"08_pytorch_paper_replicating/#102-getting-a-pretrained-vit-model-and-creating-a-feature-extractor","title":"10.2 Getting a pretrained ViT model and creating a feature extractor\u00b6","text":"<p>We can get a pretrained ViT model from <code>torchvision.models</code>.</p> <p>We'll go from the top by first making sure we've got the right versions of <code>torch</code> and <code>torchvision</code>.</p> <p>Note: The following code requires <code>torch</code> v0.12+ and <code>torchvision</code> v0.13+ to use the latest <code>torchvision</code> model weights API.</p>"},{"location":"08_pytorch_paper_replicating/#103-preparing-data-for-the-pretrained-vit-model","title":"10.3 Preparing data for the pretrained ViT model\u00b6","text":"<p>We downloaded and created DataLoaders for our own ViT model back in section 2.</p> <p>So we don't necessarily need to do it again.</p> <p>But in the name of practice, let's download some image data (pizza, steak and sushi images for Food Vision Mini), setup train and test directories and then transform the images into tensors and DataLoaders.</p> <p>We can download pizza, steak and sushi images from the course GitHub and the <code>download_data()</code> function we creating in 07. PyTorch Experiment Tracking section 1.</p>"},{"location":"08_pytorch_paper_replicating/#104-train-feature-extractor-vit-model","title":"10.4 Train feature extractor ViT model\u00b6","text":"<p>Feature extractor model ready, DataLoaders ready, time to train!</p> <p>As before we'll use the Adam optimizer (<code>torch.optim.Adam()</code>) with a learning rate of <code>1e-3</code> and <code>torch.nn.CrossEntropyLoss()</code> as the loss function.</p> <p>Our <code>engine.train()</code> function we created in 05. PyTorch Going Modular section 4 will take care of the rest.</p>"},{"location":"08_pytorch_paper_replicating/#105-plot-feature-extractor-vit-model-loss-curves","title":"10.5 Plot feature extractor ViT model loss curves\u00b6","text":"<p>Our pretrained ViT feature model numbers look good on the training and test sets.</p> <p>How do the loss curves look?</p>"},{"location":"08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size","title":"10.6 Save feature extractor ViT model and check file size\u00b6","text":"<p>It looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.</p> <p>Perhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks its pizza, steak or sushi).</p> <p>To do so we can first save our model with the <code>utils.save_model()</code> function we created in 05. PyTorch Going Modular section 5.</p>"},{"location":"08_pytorch_paper_replicating/#11-make-predictions-on-a-custom-image","title":"11. Make predictions on a custom image\u00b6","text":"<p>And finally, we'll finish with the ultimate test, predicting on our own custom data.</p> <p>Let's download the pizza dad image (a photo of my dad eating pizza) and use our ViT feature extractor to predict on it.</p> <p>To do we, let's can use the <code>pred_and_plot()</code> function we created in 06. PyTorch Transfer Learning section 6, for convenience, I saved this function to <code>going_modular.going_modular.predictions.py</code> on the course GitHub.</p>"},{"location":"08_pytorch_paper_replicating/#main-takeaways","title":"Main takeaways\u00b6","text":"<ul> <li>With the explosion of machine learning, new research papers detailing advancements come out every day. And it's impossible to keep up with it all but you can narrow things down to your own use case, such as what we did here, replicating a computer vision paper for FoodVision Mini.</li> <li>Machine learning research papers are often contain months of research by teams of smart people compressed into a few pages (so teasing out all the details and replicating the paper in full can be a bit of challenge).</li> <li>The goal of paper replicating is to turn machine learning research papers (text and math) into usable code.<ul> <li>With this being said, many machine learning research teams are starting to publish code with their papers and one of the best places to see this is at Paperswithcode.com</li> </ul> </li> <li>Breaking a machine learning research paper into inputs and outputs (what goes in and out of each layer/block/model?) and layers (how does each layer manipulate the input?) and blocks (a collection of layers) and replicating each part step by step (like we've done in this notebook) can be very helpful for understanding.</li> <li>Pretrained models are available for many state of the art model architectures and with the power of transfer learning, these often perform very well with little data.</li> <li>Larger models generally perform better but have a larger footprint too (they take up more storage space and can take longer to perform inference).<ul> <li>A big question is: deployment wise, is the extra performance of a larger model worth it/aligned with the use case?</li> </ul> </li> </ul>"},{"location":"08_pytorch_paper_replicating/#exercises","title":"Exercises\u00b6","text":"<p>Note: These exercises expect the use of <code>torchvision</code> v0.13+ (released July 2022), previous versions may work but will likely have errors.</p> <p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>All exercises should be completed using device-agnostic code.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 08.</li> <li>Example solutions notebook for 08 (try the exercises before looking at this).<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all).</li> </ul> </li> </ul> <ol> <li>Replicate the ViT architecture we created with in-built PyTorch transformer layers.<ul> <li>You'll want to look into replacing our <code>TransformerEncoderBlock()</code> class with <code>torch.nn.TransformerEncoderLayer()</code> (these contain the same layers as our custom blocks).</li> <li>You can stack <code>torch.nn.TransformerEncoderLayer()</code>'s on top of each other with <code>torch.nn.TransformerEncoder()</code>.</li> </ul> </li> <li>Turn the custom ViT architecture we created into a Python script, for example, <code>vit.py</code>.<ul> <li>You should be able to import an entire ViT model using something like<code>from vit import ViT</code>.</li> </ul> </li> <li>Train a pretrained ViT feature extractor model (like the one we made in 08. PyTorch Paper Replicating section 10) on 20% of the pizza, steak and sushi data like the dataset we used in 07. PyTorch Experiment Tracking section 7.3.<ul> <li>See how it performs compared to the EffNetB2 model we compared it to in 08. PyTorch Paper Replicating section 10.6.</li> </ul> </li> <li>Try repeating the steps from excercise 3 but this time use the \"<code>ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</code>\" pretrained weights from <code>torchvision.models.vit_b_16()</code>.<ul> <li>Note: ViT pretrained with SWAG weights has a minimum input image size of <code>(384, 384)</code> (the pretrained ViT in exercise 3 has a minimum input size of <code>(224, 224)</code>), though this is accessible in the weights <code>.transforms()</code> method.</li> </ul> </li> <li>Our custom ViT model architecture closely mimics that of the ViT paper, however, our training recipe misses a few things. Research some of the following topics from Table 3 in the ViT paper that we miss and write a sentence about each and how it might help with training:<ul> <li>ImageNet-22k pretraining (more data).</li> <li>Learning rate warmup.</li> <li>Learning rate decay.</li> <li>Gradient clipping.</li> </ul> </li> </ol>"},{"location":"08_pytorch_paper_replicating/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>There have been several iterations and tweaks to the Vision Transformer since its original release and the most concise and best performing (as of July 2022) can be viewed in Better plain ViT baselines for ImageNet-1k. Depsite of the upgrades, we stuck with replicating a \"vanilla Vision Transformer\" in this notebook because if you understand the structure of the original, you can bridge to different iterations.</li> <li>The <code>vit-pytorch</code> repository on GitHub by lucidrains is one of the most extensive resources of different ViT architectures implemented in PyTorch. It's a phenomenal reference and one I used often to create the materials we've been through in this chapter.</li> <li>PyTorch have their own implementation of the ViT architecture on GitHub, it's used as the basis of the pretrained ViT models in <code>torchvision.models</code>.</li> <li>Jay Alammar has fantastic illustrations and explanations on his blog of the attention mechanism (the foundation of Transformer models) and Transformer models.</li> <li>Adrish Dey has a fantastic write up of Layer Normalization (a main component of the ViT architecture) can help neural network training.</li> <li>The self-attention (and multi-head self-attention) mechanism is at the heart of the ViT architecture as well as many other Transformer architectures, it was originally introduced in the Attention is all you need paper.</li> <li>Yannic Kilcher's YouTube channel is a sensational resource for visual paper walkthroughs, you can see his videos for the following papers:<ul> <li>Attention is all you need (the paper that introduced the Transformer architecture).</li> <li>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (the paper that introduced the ViT architecture).</li> </ul> </li> </ul>"},{"location":"08_pytorch_profiling/","title":"08: PyTorch Profiling","text":"In\u00a0[10]: Copied! <pre>import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom going_modular import data_setup, engine\n</pre> import torch import torchvision from torch import nn from torchvision import transforms, datasets from torchinfo import summary  import numpy as np import matplotlib.pyplot as plt  from going_modular import data_setup, engine In\u00a0[11]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[11]: <pre>'cuda'</pre> In\u00a0[12]: Copied! <pre>import os\nimport requests\nfrom zipfile import ZipFile\n\ndef get_food_image_data():\n    if not os.path.exists(\"data/10_whole_foods\"):\n        os.makedirs(\"data/\", exist_ok=True)\n        # Download data\n        data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"\n        print(f\"Downloading data from {data_url}...\")\n        requests.get(data_url)\n        # Unzip data\n        targ_dir = \"data/10_whole_foods\"\n        print(f\"Extracting data to {targ_dir}...\")\n        with ZipFile(\"10_whole_foods.zip\") as zip_ref:\n            zip_ref.extractall(targ_dir)\n    else:\n        print(\"data/10_whole_foods dir exists, skipping download\")\n\nget_food_image_data()\n</pre> import os import requests from zipfile import ZipFile  def get_food_image_data():     if not os.path.exists(\"data/10_whole_foods\"):         os.makedirs(\"data/\", exist_ok=True)         # Download data         data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"         print(f\"Downloading data from {data_url}...\")         requests.get(data_url)         # Unzip data         targ_dir = \"data/10_whole_foods\"         print(f\"Extracting data to {targ_dir}...\")         with ZipFile(\"10_whole_foods.zip\") as zip_ref:             zip_ref.extractall(targ_dir)     else:         print(\"data/10_whole_foods dir exists, skipping download\")  get_food_image_data() <pre>data/10_whole_foods dir exists, skipping download\n</pre> In\u00a0[38]: Copied! <pre># Setup dirs\ntrain_dir = \"data/10_whole_foods/train\"\ntest_dir = \"data/10_whole_foods/test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create starter transform\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=32,\n    num_workers=8\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = \"data/10_whole_foods/train\" test_dir = \"data/10_whole_foods/test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create starter transform simple_transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])             # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=simple_transform,     batch_size=32,     num_workers=8 )  train_dataloader, test_dataloader, class_names Out[38]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f052ab26e20&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f05299eed00&gt;,\n ['apple',\n  'banana',\n  'beef',\n  'blueberries',\n  'carrots',\n  'chicken_wings',\n  'egg',\n  'honey',\n  'mushrooms',\n  'strawberries'])</pre> In\u00a0[66]: Copied! <pre>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n# model\n</pre> model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # model In\u00a0[67]: Copied! <pre># Update the classifier\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2),\n    nn.Linear(1280, len(class_names)).to(device))\n\n# Freeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Update the classifier model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2),     nn.Linear(1280, len(class_names)).to(device))  # Freeze all base layers  for param in model.features.parameters():     param.requires_grad = False In\u00a0[68]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[69]: Copied! <pre>model.name = \"EfficietNetB0\"\nmodel.name\n</pre> model.name = \"EfficietNetB0\" model.name Out[69]: <pre>'EfficietNetB0'</pre> In\u00a0[70]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\nfrom going_modular.engine import train_step, test_step\nfrom tqdm import tqdm\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter from going_modular.engine import train_step, test_step from tqdm import tqdm writer = SummaryWriter() <p>Update the <code>train_step()</code> function to include the PyTorch profiler.</p> In\u00a0[71]: Copied! <pre>def train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    train_loss, train_acc = 0, 0\n    ## NEW: Add PyTorch profiler\n\n    dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))\n    with torch.profiler.profile(\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),\n        # with_stack=True # this adds a lot of overhead to training (tracing all the stack)\n    ):\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to GPU\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            \n            # Turn on mixed precision if available\n            with torch.autocast(device_type=device, enabled=True):\n                # 1. Forward pass\n                y_pred = model(X)\n\n                # 2. Calculate loss\n                loss = loss_fn(y_pred, y)\n\n            # 3. Optimizer zero grad\n            optimizer.zero_grad()\n\n            # 4. Loss backward\n            loss.backward()\n\n            # 5. Optimizer step\n            optimizer.step()\n\n            # 6. Calculate metrics\n            train_loss += loss.item()\n            y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)\n            # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")\n            # print(f\"y argmax: {y_pred.argmax(dim=1)}\")\n            # print(f\"Equal: {(y_pred_class == y)}\")\n            train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n            # print(f\"batch: {batch} train_acc: {train_acc}\")\n\n    # Adjust returned metrics\n    return train_loss / len(dataloader), train_acc / len(dataloader)\n</pre> def train_step(model, dataloader, loss_fn, optimizer):     model.train()     train_loss, train_acc = 0, 0     ## NEW: Add PyTorch profiler      dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))     with torch.profiler.profile(         on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),         # with_stack=True # this adds a lot of overhead to training (tracing all the stack)     ):         for batch, (X, y) in enumerate(dataloader):             # Send data to GPU             X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)                          # Turn on mixed precision if available             with torch.autocast(device_type=device, enabled=True):                 # 1. Forward pass                 y_pred = model(X)                  # 2. Calculate loss                 loss = loss_fn(y_pred, y)              # 3. Optimizer zero grad             optimizer.zero_grad()              # 4. Loss backward             loss.backward()              # 5. Optimizer step             optimizer.step()              # 6. Calculate metrics             train_loss += loss.item()             y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)             # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")             # print(f\"y argmax: {y_pred.argmax(dim=1)}\")             # print(f\"Equal: {(y_pred_class == y)}\")             train_acc += (y_pred_class == y).sum().item() / len(y_pred)             # print(f\"batch: {batch} train_acc: {train_acc}\")      # Adjust returned metrics     return train_loss / len(dataloader), train_acc / len(dataloader) <p>TK - Now to use the writer, we've got to adjust the <code>train()</code> function...</p> In\u00a0[72]: Copied! <pre>def train(\n    model,\n    train_dataloader,\n    test_dataloader,\n    optimizer,\n    loss_fn=nn.CrossEntropyLoss(),\n    epochs=5,\n):\n\n    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n        )\n        test_loss, test_acc = test_step(\n            model=model, dataloader=test_dataloader, loss_fn=loss_fn\n        )\n\n        # Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        # Add results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n    \n    # Close the writer\n    writer.close()\n\n    return results\n</pre> def train(     model,     train_dataloader,     test_dataloader,     optimizer,     loss_fn=nn.CrossEntropyLoss(),     epochs=5, ):      results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}      for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(             model=model,             dataloader=train_dataloader,             loss_fn=loss_fn,             optimizer=optimizer,         )         test_loss, test_acc = test_step(             model=model, dataloader=test_dataloader, loss_fn=loss_fn         )          # Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # Update results         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          # Add results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)          # Close the writer     writer.close()      return results In\u00a0[73]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:05&lt;00:21,  5.27s/it]</pre> <pre>Epoch: 1 | train_loss: 1.9644 | train_acc: 0.4386 | test_loss: 1.5205 | test_acc: 0.7865\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:09&lt;00:14,  4.94s/it]</pre> <pre>Epoch: 2 | train_loss: 1.2589 | train_acc: 0.7878 | test_loss: 1.1589 | test_acc: 0.7604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:14&lt;00:09,  4.72s/it]</pre> <pre>Epoch: 3 | train_loss: 0.8642 | train_acc: 0.8776 | test_loss: 0.9347 | test_acc: 0.7917\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.6827 | train_acc: 0.8856 | test_loss: 0.6637 | test_acc: 0.8750\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:23&lt;00:00,  4.65s/it]</pre> <pre>Epoch: 5 | train_loss: 0.5688 | train_acc: 0.9069 | test_loss: 0.6175 | test_acc: 0.8854\n</pre> <pre>\n</pre> <p>Looks like mixed precision doesn't offer much benefit for smaller feature extraction models...</p> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750\n\n# # With mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906\n</pre> # # Without mixed precision #  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750  # # With mixed precision #  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906 In\u00a0[74]: Copied! <pre># Unfreeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = True\n\n# for param in model.features.parameters():\n#     print(param.requires_grad)\n</pre> # Unfreeze all base layers  for param in model.features.parameters():     param.requires_grad = True  # for param in model.features.parameters(): #     print(param.requires_grad) In\u00a0[75]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]</pre> <pre>Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]</pre> <pre>Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]</pre> <pre>Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]</pre> <pre>Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385\n\n# # With mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> # # Without mixed precision... #  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385  # # With mixed precision... #  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125 <p>Checking the PyTorch profiler, it seems that mixed precision utilises some Tensor Cores, however, these aren't large numbers.</p> <p>E.g. it uses 9-12% Tensor Cores. Perhaps the slow down when using mixed precision is because the tensors have to get altered and converted when there isn't very many of them. For example only 9-12% of tensors get converted so the speed up gains aren't realised on these tensors because they get cancelled out by the conversion time.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"08_pytorch_profiling/#08-pytorch-profiling","title":"08: PyTorch Profiling\u00b6","text":"<p>This notebook is an experiment to try out the PyTorch profiler.</p> <p>See here for more:</p> <ul> <li>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</li> <li>https://pytorch.org/docs/stable/profiler.html</li> </ul>"},{"location":"08_pytorch_profiling/#setup-device","title":"Setup device\u00b6","text":""},{"location":"08_pytorch_profiling/#get-and-load-data","title":"Get and load data\u00b6","text":""},{"location":"08_pytorch_profiling/#load-model","title":"Load model\u00b6","text":""},{"location":"08_pytorch_profiling/#train-model-and-track-results","title":"Train model and track results\u00b6","text":""},{"location":"08_pytorch_profiling/#adjust-training-function-to-track-results-with-summarywriter","title":"Adjust training function to track results with <code>SummaryWriter</code>\u00b6","text":""},{"location":"08_pytorch_profiling/#try-mixed-precision-with-larger-model","title":"Try mixed precision with larger model\u00b6","text":"<p>Now we'll try turn on mixed precision with a larger model (e.g. EffifientNetB0 with all layers tuneable).</p>"},{"location":"08_pytorch_profiling/#extensions","title":"Extensions\u00b6","text":"<ul> <li>Does changing the data input size to EfficientNetB4 change its results? E.g. input image size of (380, 380) instead of (224, 224)?</li> </ul>"},{"location":"09_pytorch_model_deployment/","title":"09. PyTorch Model Deployment","text":"<p>View Source Code | View Slides</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220824+cu113\ntorchvision version: 0.14.0.dev20220824+cu113\n</pre> <p>Note: If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of <code>torch</code> and <code>torchvision</code>.</p> <p>Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the <code>helper_functions.py</code> script from GitHub.</p> <p>The <code>helper_functions.py</code> script contains several functions we created in previous sections:</p> <ul> <li><code>set_seeds()</code> to set the random seeds (created in 07. PyTorch Experiment Tracking section 0).</li> <li><code>download_data()</code> to download a data source given a link (created in 07. PyTorch Experiment Tracking section 1).</li> <li><code>plot_loss_curves()</code> to inspect our model's training results (created in 04. PyTorch Custom Datasets section 7.8)</li> </ul> <p>Note: It may be a better idea for many of the functions in the <code>helper_functions.py</code> script to be merged into <code>going_modular/going_modular/utils.py</code>, perhaps that's an extension you'd like to try.</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <p>Finally, we'll setup device-agnostic code to make sure our models run on the GPU.</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Download pizza, steak, sushi images from GitHub\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\ndata_20_percent_path\n</pre> # Download pizza, steak, sushi images from GitHub data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\")  data_20_percent_path <pre>[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n</pre> Out[4]: <pre>PosixPath('data/pizza_steak_sushi_20_percent')</pre> <p>Wonderful!</p> <p>Now we've got a dataset, let's creat training and test paths.</p> In\u00a0[5]: Copied! <pre># Setup directory paths to train and test images\ntrain_dir = data_20_percent_path / \"train\"\ntest_dir = data_20_percent_path / \"test\"\n</pre> # Setup directory paths to train and test images train_dir = data_20_percent_path / \"train\" test_dir = data_20_percent_path / \"test\" In\u00a0[6]: Copied! <pre># 1. Setup pretrained EffNetB2 weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n\n# 2. Get EffNetB2 transforms\neffnetb2_transforms = effnetb2_weights.transforms()\n\n# 3. Setup pretrained model\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n\n# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\nfor param in effnetb2.parameters():\n    param.requires_grad = False\n</pre> # 1. Setup pretrained EffNetB2 weights effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # 2. Get EffNetB2 transforms effnetb2_transforms = effnetb2_weights.transforms()  # 3. Setup pretrained model effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"  # 4. Freeze the base layers in the model (this will freeze all layers to begin with) for param in effnetb2.parameters():     param.requires_grad = False <p>Now to change the classifier head, let's first inspect it using the <code>classifier</code> attribute of our model.</p> In\u00a0[7]: Copied! <pre># Check out EffNetB2 classifier head\neffnetb2.classifier\n</pre> # Check out EffNetB2 classifier head effnetb2.classifier Out[7]: <pre>Sequential(\n  (0): Dropout(p=0.3, inplace=True)\n  (1): Linear(in_features=1408, out_features=1000, bias=True)\n)</pre> <p>Excellent! To change the classifier head to suit our own problem, let's replace the <code>out_features</code> variable with the same number of classes we have (in our case, <code>out_features=3</code>, one for pizza, steak, sushi).</p> <p>Note: This process of changing the output layers/classifier head will be dependent on the problem you're working on. For example, if you wanted a different number of outputs or a different kind of ouput, you would have to change the output layers accordingly.</p> In\u00a0[8]: Copied! <pre># 5. Update the classifier head\neffnetb2.classifier = nn.Sequential(\n    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n    nn.Linear(in_features=1408, # keep in_features same \n              out_features=3)) # change out_features to suit our number of classes\n</pre> # 5. Update the classifier head effnetb2.classifier = nn.Sequential(     nn.Dropout(p=0.3, inplace=True), # keep dropout layer same     nn.Linear(in_features=1408, # keep in_features same                out_features=3)) # change out_features to suit our number of classes <p>Beautiful!</p> In\u00a0[9]: Copied! <pre>def create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n\"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 4. Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 5. Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # 4. Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # 5. Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <p>Woohoo! That's a nice looking function, let's try it out.</p> In\u00a0[10]: Copied! <pre>effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n                                                      seed=42)\n</pre> effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,                                                       seed=42) <p>No errors, nice, now to really try it out, let's get a summary with <code>torchinfo.summary()</code>.</p> In\u00a0[11]: Copied! <pre>from torchinfo import summary\n\n# # Print EffNetB2 model summary (uncomment for full output) \n# summary(effnetb2, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Print EffNetB2 model summary (uncomment for full output)  # summary(effnetb2,  #         input_size=(1, 3, 224, 224), #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>Base layers frozen, top layers trainable and customized!</p> In\u00a0[12]: Copied! <pre># Setup DataLoaders\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                 test_dir=test_dir,\n                                                                                                 transform=effnetb2_transforms,\n                                                                                                 batch_size=32)\n</pre> # Setup DataLoaders from going_modular.going_modular import data_setup train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                  test_dir=test_dir,                                                                                                  transform=effnetb2_transforms,                                                                                                  batch_size=32) In\u00a0[13]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=effnetb2.parameters(),\n                             lr=1e-3)\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set seeds for reproducibility and train the model\nset_seeds()\neffnetb2_results = engine.train(model=effnetb2,\n                                train_dataloader=train_dataloader_effnetb2,\n                                test_dataloader=test_dataloader_effnetb2,\n                                epochs=10,\n                                optimizer=optimizer,\n                                loss_fn=loss_fn,\n                                device=device)\n</pre> from going_modular.going_modular import engine  # Setup optimizer optimizer = torch.optim.Adam(params=effnetb2.parameters(),                              lr=1e-3) # Setup loss function loss_fn = torch.nn.CrossEntropyLoss()  # Set seeds for reproducibility and train the model set_seeds() effnetb2_results = engine.train(model=effnetb2,                                 train_dataloader=train_dataloader_effnetb2,                                 test_dataloader=test_dataloader_effnetb2,                                 epochs=10,                                 optimizer=optimizer,                                 loss_fn=loss_fn,                                 device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9856 | train_acc: 0.5604 | test_loss: 0.7408 | test_acc: 0.9347\nEpoch: 2 | train_loss: 0.7175 | train_acc: 0.8438 | test_loss: 0.5869 | test_acc: 0.9409\nEpoch: 3 | train_loss: 0.5876 | train_acc: 0.8917 | test_loss: 0.4909 | test_acc: 0.9500\nEpoch: 4 | train_loss: 0.4474 | train_acc: 0.9062 | test_loss: 0.4355 | test_acc: 0.9409\nEpoch: 5 | train_loss: 0.4290 | train_acc: 0.9104 | test_loss: 0.3915 | test_acc: 0.9443\nEpoch: 6 | train_loss: 0.4381 | train_acc: 0.8896 | test_loss: 0.3512 | test_acc: 0.9688\nEpoch: 7 | train_loss: 0.4245 | train_acc: 0.8771 | test_loss: 0.3268 | test_acc: 0.9563\nEpoch: 8 | train_loss: 0.3897 | train_acc: 0.8958 | test_loss: 0.3457 | test_acc: 0.9381\nEpoch: 9 | train_loss: 0.3749 | train_acc: 0.8812 | test_loss: 0.3129 | test_acc: 0.9131\nEpoch: 10 | train_loss: 0.3757 | train_acc: 0.8604 | test_loss: 0.2813 | test_acc: 0.9688\n</pre> In\u00a0[14]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(effnetb2_results) <p>Woah!</p> <p>Those are some nice looking loss curves.</p> <p>It looks like our model is performing quite well and perhaps would benefit from a little longer training and potentially some data augmentation (to help prevent potential overfitting occurring from longer training).</p> In\u00a0[15]: Copied! <pre>from going_modular.going_modular import utils\n\n# Save the model\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n</pre> from going_modular.going_modular import utils  # Save the model utils.save_model(model=effnetb2,                  target_dir=\"models\",                  model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\") <pre>[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n</pre> In\u00a0[16]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\") <pre>Pretrained EffNetB2 feature extractor model size: 29 MB\n</pre> In\u00a0[17]: Copied! <pre># Count number of parameters in EffNetB2\neffnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\neffnetb2_total_params\n</pre> # Count number of parameters in EffNetB2 effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters()) effnetb2_total_params Out[17]: <pre>7705221</pre> <p>Excellent!</p> <p>Now let's put everything in a dictionary so we can make comparisons later on.</p> In\u00a0[18]: Copied! <pre># Create a dictionary with EffNetB2 statistics\neffnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n                  \"number_of_parameters\": effnetb2_total_params,\n                  \"model_size (MB)\": pretrained_effnetb2_model_size}\neffnetb2_stats\n</pre> # Create a dictionary with EffNetB2 statistics effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],                   \"test_acc\": effnetb2_results[\"test_acc\"][-1],                   \"number_of_parameters\": effnetb2_total_params,                   \"model_size (MB)\": pretrained_effnetb2_model_size} effnetb2_stats Out[18]: <pre>{'test_loss': 0.28128674924373626,\n 'test_acc': 0.96875,\n 'number_of_parameters': 7705221,\n 'model_size (MB)': 29}</pre> <p>Epic!</p> <p>Looks like our EffNetB2 model is performing at over 95% accuracy!</p> <p>Criteria number 1: perform at 95%+ accuracy, tick!</p> In\u00a0[19]: Copied! <pre># Check out ViT heads layer\nvit = torchvision.models.vit_b_16()\nvit.heads\n</pre> # Check out ViT heads layer vit = torchvision.models.vit_b_16() vit.heads Out[19]: <pre>Sequential(\n  (head): Linear(in_features=768, out_features=1000, bias=True)\n)</pre> <p>Knowing this, we've got all the pieces of the puzzle we need.</p> In\u00a0[20]: Copied! <pre>def create_vit_model(num_classes:int=3, \n                     seed:int=42):\n\"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of target classes. Defaults to 3.\n        seed (int, optional): random seed value for output layer. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): ViT-B/16 feature extractor model. \n        transforms (torchvision.transforms): ViT-B/16 image transforms.\n    \"\"\"\n    # Create ViT_B_16 pretrained weights, transforms and model\n    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.vit_b_16(weights=weights)\n\n    # Freeze all layers in model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head to suit our needs (this will be trainable)\n    torch.manual_seed(seed)\n    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n                                          out_features=num_classes)) # update to reflect target number of classes\n    \n    return model, transforms\n</pre> def create_vit_model(num_classes:int=3,                       seed:int=42):     \"\"\"Creates a ViT-B/16 feature extractor model and transforms.      Args:         num_classes (int, optional): number of target classes. Defaults to 3.         seed (int, optional): random seed value for output layer. Defaults to 42.      Returns:         model (torch.nn.Module): ViT-B/16 feature extractor model.          transforms (torchvision.transforms): ViT-B/16 image transforms.     \"\"\"     # Create ViT_B_16 pretrained weights, transforms and model     weights = torchvision.models.ViT_B_16_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.vit_b_16(weights=weights)      # Freeze all layers in model     for param in model.parameters():         param.requires_grad = False      # Change classifier head to suit our needs (this will be trainable)     torch.manual_seed(seed)     model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model                                           out_features=num_classes)) # update to reflect target number of classes          return model, transforms <p>ViT feature extraction model creation function ready!</p> <p>Let's test it out.</p> In\u00a0[21]: Copied! <pre># Create ViT model and transforms\nvit, vit_transforms = create_vit_model(num_classes=3,\n                                       seed=42)\n</pre> # Create ViT model and transforms vit, vit_transforms = create_vit_model(num_classes=3,                                        seed=42) <p>No errors, lovely to see!</p> <p>Now let's get a nice-looking summary of our ViT model using <code>torchinfo.summary()</code>.</p> In\u00a0[22]: Copied! <pre>from torchinfo import summary\n\n# # Print ViT feature extractor model summary (uncomment for full output)\n# summary(vit, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Print ViT feature extractor model summary (uncomment for full output) # summary(vit,  #         input_size=(1, 3, 224, 224), #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>Just like our EffNetB2 feature extractor model, our ViT model's base layers are frozen and the output layer is customized to our needs!</p> <p>Do you notice the big difference though?</p> <p>Our ViT model has far more parameters than our EffNetB2 model. Perhaps this will come into play when we compare are our models across speed and performance later on.</p> In\u00a0[23]: Copied! <pre># Setup ViT DataLoaders\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                       test_dir=test_dir,\n                                                                                       transform=vit_transforms,\n                                                                                       batch_size=32)\n</pre> # Setup ViT DataLoaders from going_modular.going_modular import data_setup train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                        test_dir=test_dir,                                                                                        transform=vit_transforms,                                                                                        batch_size=32) In\u00a0[24]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=1e-3)\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train ViT model with seeds set for reproducibility\nset_seeds()\nvit_results = engine.train(model=vit,\n                           train_dataloader=train_dataloader_vit,\n                           test_dataloader=test_dataloader_vit,\n                           epochs=10,\n                           optimizer=optimizer,\n                           loss_fn=loss_fn,\n                           device=device)\n</pre> from going_modular.going_modular import engine  # Setup optimizer optimizer = torch.optim.Adam(params=vit.parameters(),                              lr=1e-3) # Setup loss function loss_fn = torch.nn.CrossEntropyLoss()  # Train ViT model with seeds set for reproducibility set_seeds() vit_results = engine.train(model=vit,                            train_dataloader=train_dataloader_vit,                            test_dataloader=test_dataloader_vit,                            epochs=10,                            optimizer=optimizer,                            loss_fn=loss_fn,                            device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7023 | train_acc: 0.7500 | test_loss: 0.2714 | test_acc: 0.9290\nEpoch: 2 | train_loss: 0.2531 | train_acc: 0.9104 | test_loss: 0.1669 | test_acc: 0.9602\nEpoch: 3 | train_loss: 0.1766 | train_acc: 0.9542 | test_loss: 0.1270 | test_acc: 0.9693\nEpoch: 4 | train_loss: 0.1277 | train_acc: 0.9625 | test_loss: 0.1072 | test_acc: 0.9722\nEpoch: 5 | train_loss: 0.1163 | train_acc: 0.9646 | test_loss: 0.0950 | test_acc: 0.9784\nEpoch: 6 | train_loss: 0.1270 | train_acc: 0.9375 | test_loss: 0.0830 | test_acc: 0.9722\nEpoch: 7 | train_loss: 0.0899 | train_acc: 0.9771 | test_loss: 0.0844 | test_acc: 0.9784\nEpoch: 8 | train_loss: 0.0928 | train_acc: 0.9812 | test_loss: 0.0759 | test_acc: 0.9722\nEpoch: 9 | train_loss: 0.0933 | train_acc: 0.9792 | test_loss: 0.0729 | test_acc: 0.9784\nEpoch: 10 | train_loss: 0.0662 | train_acc: 0.9833 | test_loss: 0.0642 | test_acc: 0.9847\n</pre> In\u00a0[25]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(vit_results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(vit_results) <p>Ohh yeah!</p> <p>Those are some nice looking loss curves. Just like our EffNetB2 feature extractor model, it looks our ViT model might benefit from a little longer training time and perhaps some data augmentation (to help prevent overfitting).</p> In\u00a0[26]: Copied! <pre># Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=vit,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n</pre> # Save the model from going_modular.going_modular import utils  utils.save_model(model=vit,                  target_dir=\"models\",                  model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\") <pre>[INFO] Saving model to: models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n</pre> In\u00a0[27]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <pre>Pretrained ViT feature extractor model size: 327 MB\n</pre> <p>Hmm, how does the ViT feature extractor model size compare to our EffNetB2 model size?</p> <p>We'll find this out shortly when we compare all of our model's characteristics.</p> In\u00a0[28]: Copied! <pre># Count number of parameters in ViT\nvit_total_params = sum(torch.numel(param) for param in vit.parameters())\nvit_total_params\n</pre> # Count number of parameters in ViT vit_total_params = sum(torch.numel(param) for param in vit.parameters()) vit_total_params Out[28]: <pre>85800963</pre> <p>Woah, that looks like a fair bit more than our EffNetB2!</p> <p>Note: A larger number of parameters (or weights/patterns) generally means a model has a higher capacity to learn, whether it actually uses this extra capacity is another story. In light of this, our EffNetB2 model has 7,705,221 parameters where as our ViT model has 85,800,963 (11.1x more) so we could assume that our ViT model has more of a capacity to learn, if given more data (more opportunities to learn). However, this larger capacity to learn ofen comes with an  increased model filesize and a longer time to perform inference.</p> <p>Now let's create a dictionary with some important characteristics of our ViT model.</p> In\u00a0[29]: Copied! <pre># Create ViT statistics dictionary\nvit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n             \"test_acc\": vit_results[\"test_acc\"][-1],\n             \"number_of_parameters\": vit_total_params,\n             \"model_size (MB)\": pretrained_vit_model_size}\n\nvit_stats\n</pre> # Create ViT statistics dictionary vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],              \"test_acc\": vit_results[\"test_acc\"][-1],              \"number_of_parameters\": vit_total_params,              \"model_size (MB)\": pretrained_vit_model_size}  vit_stats Out[29]: <pre>{'test_loss': 0.06418210905976593,\n 'test_acc': 0.984659090909091,\n 'number_of_parameters': 85800963,\n 'model_size (MB)': 327}</pre> <p>Nice! Looks like our ViT model achieves over 95% accuracy too.</p> In\u00a0[30]: Copied! <pre>from pathlib import Path\n\n# Get all test data paths\nprint(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\ntest_data_paths[:5]\n</pre> from pathlib import Path  # Get all test data paths print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\") test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\")) test_data_paths[:5] <pre>[INFO] Finding all filepaths ending with '.jpg' in directory: data/pizza_steak_sushi_20_percent/test\n</pre> Out[30]: <pre>[PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/2752603.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/39461.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/730464.jpg')]</pre> In\u00a0[31]: Copied! <pre>import pathlib\nimport torch\n\nfrom PIL import Image\nfrom timeit import default_timer as timer \nfrom tqdm.auto import tqdm\nfrom typing import List, Dict\n\n# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\ndef pred_and_store(paths: List[pathlib.Path], \n                   model: torch.nn.Module,\n                   transform: torchvision.transforms, \n                   class_names: List[str], \n                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:\n    \n    # 2. Create an empty list to store prediction dictionaires\n    pred_list = []\n    \n    # 3. Loop through target paths\n    for path in tqdm(paths):\n        \n        # 4. Create empty dictionary to store prediction information for each sample\n        pred_dict = {}\n\n        # 5. Get the sample path and ground truth class name\n        pred_dict[\"image_path\"] = path\n        class_name = path.parent.stem\n        pred_dict[\"class_name\"] = class_name\n        \n        # 6. Start the prediction timer\n        start_time = timer()\n        \n        # 7. Open image path\n        img = Image.open(path)\n        \n        # 8. Transform the image, add batch dimension and put image on target device\n        transformed_image = transform(img).unsqueeze(0).to(device) \n        \n        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n        model.to(device)\n        model.eval()\n        \n        # 10. Get prediction probability, predicition label and prediction class\n        with torch.inference_mode():\n            pred_logit = model(transformed_image) # perform inference on target sample \n            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n\n            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n            pred_dict[\"pred_class\"] = pred_class\n            \n            # 12. End the timer and calculate time per pred\n            end_time = timer()\n            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n\n        # 13. Does the pred match the true label?\n        pred_dict[\"correct\"] = class_name == pred_class\n\n        # 14. Add the dictionary to the list of preds\n        pred_list.append(pred_dict)\n    \n    # 15. Return list of prediction dictionaries\n    return pred_list\n</pre> import pathlib import torch  from PIL import Image from timeit import default_timer as timer  from tqdm.auto import tqdm from typing import List, Dict  # 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time def pred_and_store(paths: List[pathlib.Path],                     model: torch.nn.Module,                    transform: torchvision.transforms,                     class_names: List[str],                     device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:          # 2. Create an empty list to store prediction dictionaires     pred_list = []          # 3. Loop through target paths     for path in tqdm(paths):                  # 4. Create empty dictionary to store prediction information for each sample         pred_dict = {}          # 5. Get the sample path and ground truth class name         pred_dict[\"image_path\"] = path         class_name = path.parent.stem         pred_dict[\"class_name\"] = class_name                  # 6. Start the prediction timer         start_time = timer()                  # 7. Open image path         img = Image.open(path)                  # 8. Transform the image, add batch dimension and put image on target device         transformed_image = transform(img).unsqueeze(0).to(device)                   # 9. Prepare model for inference by sending it to target device and turning on eval() mode         model.to(device)         model.eval()                  # 10. Get prediction probability, predicition label and prediction class         with torch.inference_mode():             pred_logit = model(transformed_image) # perform inference on target sample              pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities             pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label             pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU              # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)              pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)             pred_dict[\"pred_class\"] = pred_class                          # 12. End the timer and calculate time per pred             end_time = timer()             pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)          # 13. Does the pred match the true label?         pred_dict[\"correct\"] = class_name == pred_class          # 14. Add the dictionary to the list of preds         pred_list.append(pred_dict)          # 15. Return list of prediction dictionaries     return pred_list <p>Ho, ho!</p> <p>What a good looking function!</p> <p>And you know what, since our <code>pred_and_store()</code> is a pretty good utility function for making and storing predictions, it could be stored to <code>going_modular.going_modular.predictions.py</code> for later use. That might be an extension you'd like to try, check out 05. PyTorch Going Modular for ideas.</p> In\u00a0[32]: Copied! <pre># Make predictions across test dataset with EffNetB2\neffnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                          model=effnetb2,\n                                          transform=effnetb2_transforms,\n                                          class_names=class_names,\n                                          device=\"cpu\") # make predictions on CPU\n</pre> # Make predictions across test dataset with EffNetB2 effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,                                           model=effnetb2,                                           transform=effnetb2_transforms,                                           class_names=class_names,                                           device=\"cpu\") # make predictions on CPU  <pre>  0%|          | 0/150 [00:00&lt;?, ?it/s]</pre> <p>Nice! Look at those predictions fly!</p> <p>Let's inspect the first couple and see what they look like.</p> In\u00a0[33]: Copied! <pre># Inspect the first 2 prediction dictionaries\neffnetb2_test_pred_dicts[:2]\n</pre> # Inspect the first 2 prediction dictionaries effnetb2_test_pred_dicts[:2] Out[33]: <pre>[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9293,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0494,\n  'correct': True},\n {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9534,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0264,\n  'correct': True}]</pre> <p>Woohoo!</p> <p>It looks like our <code>pred_and_store()</code> function worked nicely.</p> <p>Thanks to our list of dictionaries data structure, we've got plenty of useful information we can further inspect.</p> <p>To do so, let's turn our list of dictionaries into a pandas DataFrame.</p> In\u00a0[34]: Copied! <pre># Turn the test_pred_dicts into a DataFrame\nimport pandas as pd\neffnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\neffnetb2_test_pred_df.head()\n</pre> # Turn the test_pred_dicts into a DataFrame import pandas as pd effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts) effnetb2_test_pred_df.head() Out[34]: image_path class_name pred_prob pred_class time_for_pred correct 0 data/pizza_steak_sushi_20_percent/test/steak/8... steak 0.9293 steak 0.0494 True 1 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.9534 steak 0.0264 True 2 data/pizza_steak_sushi_20_percent/test/steak/2... steak 0.7532 steak 0.0256 True 3 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.5935 steak 0.0263 True 4 data/pizza_steak_sushi_20_percent/test/steak/7... steak 0.8959 steak 0.0269 True <p>Beautiful!</p> <p>Look how easily those prediction dictionaries turn into a structured format we can perform analysis on.</p> <p>Such as finding how many predictions our EffNetB2 model got wrong...</p> In\u00a0[35]: Copied! <pre># Check number of correct predictions\neffnetb2_test_pred_df.correct.value_counts()\n</pre> # Check number of correct predictions effnetb2_test_pred_df.correct.value_counts() Out[35]: <pre>True     145\nFalse      5\nName: correct, dtype: int64</pre> <p>Five wrong predictions out of 150 total, not bad!</p> <p>And how about the average prediction time?</p> In\u00a0[36]: Copied! <pre># Find the average time per prediction \neffnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")\n</pre> # Find the average time per prediction  effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4) print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\") <pre>EffNetB2 average time per prediction: 0.0269 seconds\n</pre> <p>Hmm, how does that average prediction time live up to our criteria of our model performing at real-time (~30FPS or 0.03 seconds per prediction)?</p> <p>Note: Prediction times will be different across different hardware types (e.g. a local Intel i9 vs Google Colab CPU). The better and faster the hardware, generally, the faster the prediction. For example, on my local deep learning PC with an Intel i9 chip, my average prediction time with EffNetB2 is around 0.031 seconds (just under real-time). However, on Google Colab (I'm not sure what CPU hardware Colab uses but it looks like it might be an Intel(R) Xeon(R)), my average prediction time with EffNetB2 is about 0.1396 seconds (3-4x slower).</p> <p>Let's add our EffNetB2 average time per prediction to our <code>effnetb2_stats</code> dictionary.</p> In\u00a0[37]: Copied! <pre># Add EffNetB2 average prediction time to stats dictionary \neffnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\neffnetb2_stats\n</pre> # Add EffNetB2 average prediction time to stats dictionary  effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred effnetb2_stats Out[37]: <pre>{'test_loss': 0.28128674924373626,\n 'test_acc': 0.96875,\n 'number_of_parameters': 7705221,\n 'model_size (MB)': 29,\n 'time_per_pred_cpu': 0.0269}</pre> In\u00a0[38]: Copied! <pre># Make list of prediction dictionaries with ViT feature extractor model on test images\nvit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                     model=vit,\n                                     transform=vit_transforms,\n                                     class_names=class_names,\n                                     device=\"cpu\")\n</pre> # Make list of prediction dictionaries with ViT feature extractor model on test images vit_test_pred_dicts = pred_and_store(paths=test_data_paths,                                      model=vit,                                      transform=vit_transforms,                                      class_names=class_names,                                      device=\"cpu\") <pre>  0%|          | 0/150 [00:00&lt;?, ?it/s]</pre> <p>Predictions made!</p> <p>Now let's check out the first couple.</p> In\u00a0[39]: Copied! <pre># Check the first couple of ViT predictions on the test dataset\nvit_test_pred_dicts[:2]\n</pre> # Check the first couple of ViT predictions on the test dataset vit_test_pred_dicts[:2] Out[39]: <pre>[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9933,\n  'pred_class': 'steak',\n  'time_for_pred': 0.1313,\n  'correct': True},\n {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9893,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0638,\n  'correct': True}]</pre> <p>Wonderful!</p> <p>And just like before, since our ViT model's predictions are in the form of a list of dictionaries, we can easily turn them into a pandas DataFrame for further inspection.</p> In\u00a0[40]: Copied! <pre># Turn vit_test_pred_dicts into a DataFrame\nimport pandas as pd\nvit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\nvit_test_pred_df.head()\n</pre> # Turn vit_test_pred_dicts into a DataFrame import pandas as pd vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts) vit_test_pred_df.head() Out[40]: image_path class_name pred_prob pred_class time_for_pred correct 0 data/pizza_steak_sushi_20_percent/test/steak/8... steak 0.9933 steak 0.1313 True 1 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.9893 steak 0.0638 True 2 data/pizza_steak_sushi_20_percent/test/steak/2... steak 0.9971 steak 0.0627 True 3 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.7685 steak 0.0632 True 4 data/pizza_steak_sushi_20_percent/test/steak/7... steak 0.9499 steak 0.0641 True <p>How many predictions did our ViT model get correct?</p> In\u00a0[41]: Copied! <pre># Count the number of correct predictions\nvit_test_pred_df.correct.value_counts()\n</pre> # Count the number of correct predictions vit_test_pred_df.correct.value_counts() Out[41]: <pre>True     148\nFalse      2\nName: correct, dtype: int64</pre> <p>Woah!</p> <p>Our ViT model did a little better than our EffNetB2 model in terms of correct predictions, only two samples wrong across the whole test dataset.</p> <p>As an extension you might want to visualize the ViT model's wrong predictions and see if there's any reason why it might've got them wrong.</p> <p>How about we calculate how long the ViT model took per prediction?</p> In\u00a0[42]: Copied! <pre># Calculate average time per prediction for ViT model\nvit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")\n</pre> # Calculate average time per prediction for ViT model vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4) print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\") <pre>ViT average time per prediction: 0.0641 seconds\n</pre> <p>Well, that looks a little slower than our EffNetB2 model's average time per prediction but how does it look in terms of our second criteria: speed?</p> <p>For now, let's add the value to our <code>vit_stats</code> dictionary so we can compare it to our EffNetB2 model's stats.</p> <p>Note: The average time per prediction values will be highly dependent on the hardware you make them on. For example, for the ViT model, my average time per prediction (on the CPU) was 0.0693-0.0777 seconds on my local deep learning PC with an Intel i9 CPU. Where as on Google Colab, my average time per prediction with the ViT model was 0.6766-0.7113 seconds.</p> In\u00a0[43]: Copied! <pre># Add average prediction time for ViT model on CPU\nvit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\nvit_stats\n</pre> # Add average prediction time for ViT model on CPU vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred vit_stats Out[43]: <pre>{'test_loss': 0.06418210905976593,\n 'test_acc': 0.984659090909091,\n 'number_of_parameters': 85800963,\n 'model_size (MB)': 327,\n 'time_per_pred_cpu': 0.0641}</pre> In\u00a0[44]: Copied! <pre># Turn stat dictionaries into DataFrame\ndf = pd.DataFrame([effnetb2_stats, vit_stats])\n\n# Add column for model names\ndf[\"model\"] = [\"EffNetB2\", \"ViT\"]\n\n# Convert accuracy to percentages\ndf[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n\ndf\n</pre> # Turn stat dictionaries into DataFrame df = pd.DataFrame([effnetb2_stats, vit_stats])  # Add column for model names df[\"model\"] = [\"EffNetB2\", \"ViT\"]  # Convert accuracy to percentages df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)  df Out[44]: test_loss test_acc number_of_parameters model_size (MB) time_per_pred_cpu model 0 0.281287 96.88 7705221 29 0.0269 EffNetB2 1 0.064182 98.47 85800963 327 0.0641 ViT <p>Wonderful!</p> <p>It seems our models are quite close in terms of overall test accuracy but how do they look across the other fields?</p> <p>One way to find out would be to divide the ViT model statistics by the EffNetB2 model statistics to find out the different ratios between the models.</p> <p>Let's create another DataFrame to do so.</p> In\u00a0[45]: Copied! <pre># Compare ViT to EffNetB2 across different characteristics\npd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n             columns=[\"ViT to EffNetB2 ratios\"]).T\n</pre> # Compare ViT to EffNetB2 across different characteristics pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics              columns=[\"ViT to EffNetB2 ratios\"]).T Out[45]: test_loss test_acc number_of_parameters model_size (MB) time_per_pred_cpu ViT to EffNetB2 ratios 0.228173 1.016412 11.135432 11.275862 2.3829 <p>It seems our ViT model outperforms the EffNetB2 model across the performance metrics (test loss, where lower is better and test accuracy, where higher is better) but at the expense of having:</p> <ul> <li>11x+ the number of parameters.</li> <li>11x+ the model size.</li> <li>2.5x+ the prediction time per image.</li> </ul> <p>Are these tradeoffs worth it?</p> <p>Perhaps if we had unlimited compute power but for our use case of deploying the FoodVision Mini model to a smaller device (e.g. a mobile phone), we'd likely start out with the EffNetB2 model for faster predictions at a slightly reduced performance but dramatically smaller</p> In\u00a0[46]: Copied! <pre># 1. Create a plot from model comparison DataFrame\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(data=df, \n                     x=\"time_per_pred_cpu\", \n                     y=\"test_acc\", \n                     c=[\"blue\", \"orange\"], # what colours to use?\n                     s=\"model_size (MB)\") # size the dots by the model sizes\n\n# 2. Add titles, labels and customize fontsize for aesthetics\nax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\nax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\nax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\nax.tick_params(axis='both', labelsize=12)\nax.grid(True)\n\n# 3. Annotate with model names\nfor index, row in df.iterrows():\n    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n                size=12)\n\n# 4. Create a legend based on model sizes\nhandles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\nmodel_size_legend = ax.legend(handles, \n                              labels, \n                              loc=\"lower right\", \n                              title=\"Model size (MB)\",\n                              fontsize=12)\n\n# Save the figure\nplt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n\n# Show the figure\nplt.show()\n</pre> # 1. Create a plot from model comparison DataFrame fig, ax = plt.subplots(figsize=(12, 8)) scatter = ax.scatter(data=df,                       x=\"time_per_pred_cpu\",                       y=\"test_acc\",                       c=[\"blue\", \"orange\"], # what colours to use?                      s=\"model_size (MB)\") # size the dots by the model sizes  # 2. Add titles, labels and customize fontsize for aesthetics ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18) ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14) ax.set_ylabel(\"Test accuracy (%)\", fontsize=14) ax.tick_params(axis='both', labelsize=12) ax.grid(True)  # 3. Annotate with model names for index, row in df.iterrows():     ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270                  xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),                 size=12)  # 4. Create a legend based on model sizes handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5) model_size_legend = ax.legend(handles,                                labels,                                loc=\"lower right\",                                title=\"Model size (MB)\",                               fontsize=12)  # Save the figure plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")  # Show the figure plt.show() <p>Woah!</p> <p>The plot really visualizes the speed vs. performance tradeoff, in other words, when you have a larger, better performing deep model (like our ViT model), it generally takes longer to perform inference (higher latency).</p> <p>There are exceptions to the rule and new research is being published all the time to help make larger models perform faster.</p> <p>And it can be tempting to just deploy the best performing model but it's also good to take into considersation where the model is going to be performing.</p> <p>In our case, the differences between our model's performance levels (on the test loss and test accuracy) aren't too extreme.</p> <p>But since we'd like to put an emphasis on speed to begin with, we're going to stick with deploying EffNetB2 since it's faster and has a much smaller footprint.</p> <p>Note: Prediction times will be different across different hardware types (e.g. Intel i9 vs Google Colab CPU vs GPU) so it's important to think about and test where your model is going to end up. Asking questions like \"where is the model going to be run?\" or \"what is the ideal scenario for running the model?\" and then running experiments to try and provide answers on your way to deployment is very helpful.</p> In\u00a0[47]: Copied! <pre># Import/install Gradio \ntry:\n    import gradio as gr\nexcept: \n    !pip -q install gradio\n    import gradio as gr\n    \nprint(f\"Gradio version: {gr.__version__}\")\n</pre> # Import/install Gradio  try:     import gradio as gr except:      !pip -q install gradio     import gradio as gr      print(f\"Gradio version: {gr.__version__}\") <pre>Gradio version: 3.1.4\n</pre> <p>Gradio ready!</p> <p>Let's turn FoodVision Mini into a demo application.</p> In\u00a0[48]: Copied! <pre># Put EffNetB2 on CPU\neffnetb2.to(\"cpu\") \n\n# Check the device\nnext(iter(effnetb2.parameters())).device\n</pre> # Put EffNetB2 on CPU effnetb2.to(\"cpu\")   # Check the device next(iter(effnetb2.parameters())).device Out[48]: <pre>device(type='cpu')</pre> <p>And now let's create a function called <code>predict()</code> to replicate the workflow above.</p> In\u00a0[49]: Copied! <pre>from typing import Tuple, Dict\n\ndef predict(img) -&gt; Tuple[Dict, float]:\n\"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n</pre> from typing import Tuple, Dict  def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time <p>Beautiful!</p> <p>Now let's see our function in action by performing a prediction on a random image from the test dataset.</p> <p>We'll start by getting a list of all the image paths from the test directory and then randomly selecting one.</p> <p>Then we'll open the randomly selected image with <code>PIL.Image.open()</code>.</p> <p>Finally, we'll pass the image to our <code>predict()</code> function.</p> In\u00a0[50]: Copied! <pre>import random\nfrom PIL import Image\n\n# Get a list of all test image filepaths\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n\n# Randomly select a test image path\nrandom_image_path = random.sample(test_data_paths, k=1)[0]\n\n# Open the target image\nimage = Image.open(random_image_path)\nprint(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n\n# Predict on the target image and print out the outputs\npred_dict, pred_time = predict(img=image)\nprint(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\nprint(f\"Prediction time: {pred_time} seconds\")\n</pre> import random from PIL import Image  # Get a list of all test image filepaths test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))  # Randomly select a test image path random_image_path = random.sample(test_data_paths, k=1)[0]  # Open the target image image = Image.open(random_image_path) print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")  # Predict on the target image and print out the outputs pred_dict, pred_time = predict(img=image) print(f\"Prediction label and probability dictionary: \\n{pred_dict}\") print(f\"Prediction time: {pred_time} seconds\") <pre>[INFO] Predicting on image at path: data/pizza_steak_sushi_20_percent/test/pizza/3770514.jpg\n\nPrediction label and probability dictionary: \n{'pizza': 0.9785208702087402, 'steak': 0.01169557310640812, 'sushi': 0.009783552028238773}\nPrediction time: 0.027 seconds\n</pre> <p>Nice!</p> <p>Running the cell above a few times we can see different prediction probabilities for each label from our EffNetB2 model as well as the time it took per prediction.</p> In\u00a0[51]: Copied! <pre># Create a list of example inputs to our Gradio demo\nexample_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\nexample_list\n</pre> # Create a list of example inputs to our Gradio demo example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)] example_list Out[51]: <pre>[['data/pizza_steak_sushi_20_percent/test/sushi/804460.jpg'],\n ['data/pizza_steak_sushi_20_percent/test/steak/746921.jpg'],\n ['data/pizza_steak_sushi_20_percent/test/steak/2117351.jpg']]</pre> <p>Perfect!</p> <p>Our Gradio demo will showcase these as example inputs to our demo so people can try it out and see what it does without uploading any of their own data.</p> In\u00a0[52]: Copied! <pre>import gradio as gr\n\n# Create title, description and article strings\ntitle = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch(debug=False, # print errors locally?\n            share=True) # generate a publically shareable URL?\n</pre> import gradio as gr  # Create title, description and article strings title = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Create the Gradio demo demo = gr.Interface(fn=predict, # mapping function from input to output                     inputs=gr.Image(type=\"pil\"), # what are the inputs?                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?                              gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs                     examples=example_list,                      title=title,                     description=description,                     article=article)  # Launch the demo! demo.launch(debug=False, # print errors locally?             share=True) # generate a publically shareable URL? <pre>Running on local URL:  http://127.0.0.1:7860/\nRunning on public URL: https://27541.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n</pre> Out[52]: <pre>(&lt;gradio.routes.App at 0x7f122dd0f0d0&gt;,\n 'http://127.0.0.1:7860/',\n 'https://27541.gradio.app')</pre> <p>FoodVision Mini Gradio demo running in Google Colab and in the browser (the link when running from Google Colab only lasts for 72 hours). You can see the permanent live demo on Hugging Face Spaces.</p> <p>Woohoo!!! What an epic demo!!!</p> <p>FoodVision Mini has officially come to life in an interface someone could use and try out.</p> <p>If you set the parameter <code>share=True</code> in the <code>launch()</code> method, Gradio also provides you with a shareable link such as <code>https://123XYZ.gradio.app</code> (this link is an example only and likely expired) which is valid for 72-hours.</p> <p>The link provides a proxy back to the Gradio interface you launched.</p> <p>For more permanent hosting, you can upload your Gradio app to Hugging Face Spaces or anywhere that runs Python code.</p> In\u00a0[53]: Copied! <pre>import shutil\nfrom pathlib import Path\n\n# Create FoodVision mini demo path\nfoodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n\n# Remove files that might already exist there and create new directory\nif foodvision_mini_demo_path.exists():\n    shutil.rmtree(foodvision_mini_demo_path)\n    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n                                    exist_ok=True) # create it even if it already exists?\nelse:\n    # If the file doesn't exist, create it anyway\n    foodvision_mini_demo_path.mkdir(parents=True, \n                                    exist_ok=True)\n    \n# Check what's in the folder\n!ls demos/foodvision_mini/\n</pre> import shutil from pathlib import Path  # Create FoodVision mini demo path foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")  # Remove files that might already exist there and create new directory if foodvision_mini_demo_path.exists():     shutil.rmtree(foodvision_mini_demo_path)     foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?                                     exist_ok=True) # create it even if it already exists? else:     # If the file doesn't exist, create it anyway     foodvision_mini_demo_path.mkdir(parents=True,                                      exist_ok=True)      # Check what's in the folder !ls demos/foodvision_mini/ In\u00a0[54]: Copied! <pre>import shutil\nfrom pathlib import Path\n\n# 1. Create an examples directory\nfoodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\nfoodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n\n# 2. Collect three random test dataset image paths\nfoodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n\n# 3. Copy the three random images to the examples directory\nfor example in foodvision_mini_examples:\n    destination = foodvision_mini_examples_path / example.name\n    print(f\"[INFO] Copying {example} to {destination}\")\n    shutil.copy2(src=example, dst=destination)\n</pre> import shutil from pathlib import Path  # 1. Create an examples directory foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\" foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)  # 2. Collect three random test dataset image paths foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),                             Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),                             Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]  # 3. Copy the three random images to the examples directory for example in foodvision_mini_examples:     destination = foodvision_mini_examples_path / example.name     print(f\"[INFO] Copying {example} to {destination}\")     shutil.copy2(src=example, dst=destination) <pre>[INFO] Copying data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg to demos/foodvision_mini/examples/592799.jpg\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg to demos/foodvision_mini/examples/3622237.jpg\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg to demos/foodvision_mini/examples/2582289.jpg\n</pre> <p>Now to verify our examples are present, let's list the contents of our <code>demos/foodvision_mini/examples/</code> directory with <code>os.listdir()</code> and then format the filepaths into a list of lists (so it's compatible with Gradio's <code>gradio.Interface()</code> <code>example</code> parameter).</p> In\u00a0[55]: Copied! <pre>import os\n\n# Get example filepaths in a list of lists\nexample_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\nexample_list\n</pre> import os  # Get example filepaths in a list of lists example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)] example_list Out[55]: <pre>[['examples/3622237.jpg'], ['examples/592799.jpg'], ['examples/2582289.jpg']]</pre> In\u00a0[56]: Copied! <pre>import shutil\n\n# Create a source path for our target model\neffnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n\n# Create a destination path for our target model \neffnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n\n# Try to move the file\ntry:\n    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n    \n    # Move the model\n    shutil.move(src=effnetb2_foodvision_mini_model_path, \n                dst=effnetb2_foodvision_mini_model_destination)\n    \n    print(f\"[INFO] Model move complete.\")\n\n# If the model has already been moved, check if it exists\nexcept:\n    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n</pre> import shutil  # Create a source path for our target model effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"  # Create a destination path for our target model  effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]  # Try to move the file try:     print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")          # Move the model     shutil.move(src=effnetb2_foodvision_mini_model_path,                  dst=effnetb2_foodvision_mini_model_destination)          print(f\"[INFO] Model move complete.\")  # If the model has already been moved, check if it exists except:     print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")     print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\") <pre>[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n[INFO] Model move complete.\n</pre> In\u00a0[57]: Copied! <pre>%%writefile demos/foodvision_mini/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n\"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> %%writefile demos/foodvision_mini/model.py import torch import torchvision  from torch import nn   def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <pre>Writing demos/foodvision_mini/model.py\n</pre> In\u00a0[58]: Copied! <pre>%%writefile demos/foodvision_mini/app.py\n### 1. Imports and class names setup ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Setup class names\nclass_names = [\"pizza\", \"steak\", \"sushi\"]\n\n### 2. Model and transforms preparation ###\n\n# Create EffNetB2 model\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=3, # len(class_names) would also work\n)\n\n# Load saved weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n### 3. Predict function ###\n\n# Create predict function\ndef predict(img) -&gt; Tuple[Dict, float]:\n\"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article strings\ntitle = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create examples list from \"examples/\" directory\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    # Create examples list from \"examples/\" directory\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch()\n</pre> %%writefile demos/foodvision_mini/app.py ### 1. Imports and class names setup ###  import gradio as gr import os import torch  from model import create_effnetb2_model from timeit import default_timer as timer from typing import Tuple, Dict  # Setup class names class_names = [\"pizza\", \"steak\", \"sushi\"]  ### 2. Model and transforms preparation ###  # Create EffNetB2 model effnetb2, effnetb2_transforms = create_effnetb2_model(     num_classes=3, # len(class_names) would also work )  # Load saved weights effnetb2.load_state_dict(     torch.load(         f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",         map_location=torch.device(\"cpu\"),  # load to CPU     ) )  ### 3. Predict function ###  # Create predict function def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time  ### 4. Gradio app ###  # Create title, description and article strings title = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Create examples list from \"examples/\" directory example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]  # Create the Gradio demo demo = gr.Interface(fn=predict, # mapping function from input to output                     inputs=gr.Image(type=\"pil\"), # what are the inputs?                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?                              gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs                     # Create examples list from \"examples/\" directory                     examples=example_list,                      title=title,                     description=description,                     article=article)  # Launch the demo! demo.launch() <pre>Writing demos/foodvision_mini/app.py\n</pre> In\u00a0[59]: Copied! <pre>%%writefile demos/foodvision_mini/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n</pre> %%writefile demos/foodvision_mini/requirements.txt torch==1.12.0 torchvision==0.13.0 gradio==3.1.4 <pre>Writing demos/foodvision_mini/requirements.txt\n</pre> <p>Nice!</p> <p>We've officially got all the files we need to deploy our FoodVision Mini demo!</p> In\u00a0[60]: Copied! <pre>!ls demos/foodvision_mini\n</pre> !ls demos/foodvision_mini <pre>09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\napp.py\nexamples\nmodel.py\nrequirements.txt\n</pre> <p>These are all files that we've created!</p> <p>To begin uploading our files to Hugging Face, let's now download them from Google Colab (or wherever you're running this notebook).</p> <p>To do so, we'll first compress the files into a single zip folder via the command:</p> <pre><code>zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n</code></pre> <p>Where:</p> <ul> <li><code>zip</code> stands for \"zip\" as in \"please zip together the files in the following directory\".</li> <li><code>-r</code> stands for \"recursive\" as in, \"go through all of the files in the target directory\".</li> <li><code>../foodvision_mini.zip</code> is the target directory we'd like our files to be zipped to.</li> <li><code>*</code> stands for \"all the files in the current directory\".</li> <li><code>-x</code> stands for \"exclude these files\".</li> </ul> <p>We can download our zip file from Google Colab using <code>google.colab.files.download(\"demos/foodvision_mini.zip\")</code> (we'll put this inside a <code>try</code> and <code>except</code> block just in case we're not running the code inside Google Colab, and if so we'll print a message saying to manually download the files).</p> <p>Let's try it out!</p> In\u00a0[61]: Copied! <pre># Change into and then zip the foodvision_mini folder but exclude certain files\n!cd demos/foodvision_mini &amp;&amp; zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Download the zipped FoodVision Mini app (if running in Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_mini.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")\n</pre> # Change into and then zip the foodvision_mini folder but exclude certain files !cd demos/foodvision_mini &amp;&amp; zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"  # Download the zipped FoodVision Mini app (if running in Google Colab) try:     from google.colab import files     files.download(\"demos/foodvision_mini.zip\") except:     print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\") <pre>updating: 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth (deflated 8%)\nupdating: app.py (deflated 57%)\nupdating: examples/ (stored 0%)\nupdating: examples/3622237.jpg (deflated 0%)\nupdating: examples/592799.jpg (deflated 1%)\nupdating: examples/2582289.jpg (deflated 17%)\nupdating: model.py (deflated 56%)\nupdating: requirements.txt (deflated 4%)\nNot running in Google Colab, can't use google.colab.files.download(), please manually download.\n</pre> <p>Woohoo!</p> <p>Looks like our <code>zip</code> command was successful.</p> <p>If you're running this notebook in Google Colab, you should see a file start to download in your browser.</p> <p>Otherwise, you can see the <code>foodvision_mini.zip</code> folder (and more) on the course GitHub under the <code>demos/</code> directory.</p> In\u00a0[62]: Copied! <pre># IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)\n</pre> # IPython is a library to help make Python interactive from IPython.display import IFrame  # Embed FoodVision Mini Gradio demo IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750) Out[62]: In\u00a0[63]: Copied! <pre># Create EffNetB2 model capable of fitting to 101 classes for Food101\neffnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n</pre> # Create EffNetB2 model capable of fitting to 101 classes for Food101 effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101) <p>Beautiful!</p> <p>Let's now get a summary of our model.</p> In\u00a0[64]: Copied! <pre>from torchinfo import summary\n\n# # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n# summary(effnetb2_food101, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output) # summary(effnetb2_food101,  #         input_size=(1, 3, 224, 224), #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>Nice!</p> <p>See how just like our EffNetB2 model for FoodVision Mini the base layers are frozen (these are pretrained on ImageNet) and the outer layers (the <code>classifier</code> layers) are trainble with an ouput shape of <code>[batch_size, 101]</code> (<code>101</code> for 101 classes in Food101).</p> <p>Now since we're going to be dealing with a fair bit more data than usual, how about we add a little data augmentation to our transforms (<code>effnetb2_transforms</code>) to augment the training data.</p> <p>Note: Data augmentation is a technique used to alter the appearance of an input training sample (e.g. rotating an image or slightly skewing it) to artificially increase the diversity of a training dataset to hopefully prevent overfitting. You can see more on data augmentation in 04. PyTorch Custom Datasets section 6.</p> <p>Let's compose a <code>torchvision.transforms</code> pipeline to use <code>torchvision.transforms.TrivialAugmentWide()</code> (the same data augmentation used by the PyTorch team in their computer vision recipes) as well as the <code>effnetb2_transforms</code> to transform our training images.</p> In\u00a0[65]: Copied! <pre># Create Food101 training data transforms (only perform data augmentation on the training images)\nfood101_train_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.TrivialAugmentWide(),\n    effnetb2_transforms,\n])\n</pre> # Create Food101 training data transforms (only perform data augmentation on the training images) food101_train_transforms = torchvision.transforms.Compose([     torchvision.transforms.TrivialAugmentWide(),     effnetb2_transforms, ]) <p>Epic!</p> <p>Now let's compare <code>food101_train_transforms</code> (for the training data) and <code>effnetb2_transforms</code> (for the testing/inference data).</p> In\u00a0[66]: Copied! <pre>print(f\"Training transforms:\\n{food101_train_transforms}\\n\") \nprint(f\"Testing transforms:\\n{effnetb2_transforms}\")\n</pre> print(f\"Training transforms:\\n{food101_train_transforms}\\n\")  print(f\"Testing transforms:\\n{effnetb2_transforms}\") <pre>Training transforms:\nCompose(\n    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n    ImageClassification(\n    crop_size=[288]\n    resize_size=[288]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n)\n\nTesting transforms:\nImageClassification(\n    crop_size=[288]\n    resize_size=[288]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n</pre> In\u00a0[67]: Copied! <pre>from torchvision import datasets\n\n# Setup data directory\nfrom pathlib import Path\ndata_dir = Path(\"data\")\n\n# Get training data (~750 images x 101 food classes)\ntrain_data = datasets.Food101(root=data_dir, # path to download data to\n                              split=\"train\", # dataset split to get\n                              transform=food101_train_transforms, # perform data augmentation on training data\n                              download=True) # want to download?\n\n# Get testing data (~250 images x 101 food classes)\ntest_data = datasets.Food101(root=data_dir,\n                             split=\"test\",\n                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n                             download=True)\n</pre> from torchvision import datasets  # Setup data directory from pathlib import Path data_dir = Path(\"data\")  # Get training data (~750 images x 101 food classes) train_data = datasets.Food101(root=data_dir, # path to download data to                               split=\"train\", # dataset split to get                               transform=food101_train_transforms, # perform data augmentation on training data                               download=True) # want to download?  # Get testing data (~250 images x 101 food classes) test_data = datasets.Food101(root=data_dir,                              split=\"test\",                              transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data                              download=True) <p>Data downloaded!</p> <p>Now we can get a list of all the class names using <code>train_data.classes</code>.</p> In\u00a0[68]: Copied! <pre># Get Food101 class names\nfood101_class_names = train_data.classes\n\n# View the first 10\nfood101_class_names[:10]\n</pre> # Get Food101 class names food101_class_names = train_data.classes  # View the first 10 food101_class_names[:10] Out[68]: <pre>['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']</pre> <p>Ho ho! Those are some delicious sounding foods (although I've never heard of \"beignets\"... update: after a quick Google search, beignets also look delicious).</p> <p>You can see a full list of the Food101 class names on the course GitHub under <code>extras/food101_class_names.txt</code>.</p> In\u00a0[69]: Copied! <pre>def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n\"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n\n    Args:\n        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n        split_size (float, optional): How much of the dataset should be split? \n            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n        seed (int, optional): Seed for random generator. Defaults to 42.\n\n    Returns:\n        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n            random_split_2 is of size (1-split_size)*len(dataset).\n    \"\"\"\n    # Create split lengths based on original dataset length\n    length_1 = int(len(dataset) * split_size) # desired length\n    length_2 = len(dataset) - length_1 # remaining length\n        \n    # Print out info\n    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n    \n    # Create splits with given random seed\n    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n                                                                   lengths=[length_1, length_2],\n                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n    return random_split_1, random_split_2\n</pre> def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):     \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.      Args:         dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.         split_size (float, optional): How much of the dataset should be split?              E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.         seed (int, optional): Seed for random generator. Defaults to 42.      Returns:         tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and              random_split_2 is of size (1-split_size)*len(dataset).     \"\"\"     # Create split lengths based on original dataset length     length_1 = int(len(dataset) * split_size) # desired length     length_2 = len(dataset) - length_1 # remaining length              # Print out info     print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")          # Create splits with given random seed     random_split_1, random_split_2 = torch.utils.data.random_split(dataset,                                                                     lengths=[length_1, length_2],                                                                    generator=torch.manual_seed(seed)) # set the random seed for reproducible splits     return random_split_1, random_split_2 <p>Dataset split function created!</p> <p>Now let's test it out by creating a 20% training and testing dataset split of Food101.</p> In\u00a0[70]: Copied! <pre># Create training 20% split of Food101\ntrain_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n                                                 split_size=0.2)\n\n# Create testing 20% split of Food101\ntest_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n                                                split_size=0.2)\n\nlen(train_data_food101_20_percent), len(test_data_food101_20_percent)\n</pre> # Create training 20% split of Food101 train_data_food101_20_percent, _ = split_dataset(dataset=train_data,                                                  split_size=0.2)  # Create testing 20% split of Food101 test_data_food101_20_percent, _ = split_dataset(dataset=test_data,                                                 split_size=0.2)  len(train_data_food101_20_percent), len(test_data_food101_20_percent) <pre>[INFO] Splitting dataset of length 75750 into splits of size: 15150 (20%), 60600 (80%)\n[INFO] Splitting dataset of length 25250 into splits of size: 5050 (20%), 20200 (80%)\n</pre> Out[70]: <pre>(15150, 5050)</pre> <p>Excellent!</p> In\u00a0[71]: Copied! <pre>import os\nimport torch\n\nBATCH_SIZE = 32\nNUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n\n# Create Food101 20 percent training DataLoader\ntrain_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n                                                                  batch_size=BATCH_SIZE,\n                                                                  shuffle=True,\n                                                                  num_workers=NUM_WORKERS)\n# Create Food101 20 percent testing DataLoader\ntest_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n                                                                 batch_size=BATCH_SIZE,\n                                                                 shuffle=False,\n                                                                 num_workers=NUM_WORKERS)\n</pre> import os import torch  BATCH_SIZE = 32 NUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs  # Create Food101 20 percent training DataLoader train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,                                                                   batch_size=BATCH_SIZE,                                                                   shuffle=True,                                                                   num_workers=NUM_WORKERS) # Create Food101 20 percent testing DataLoader test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,                                                                  batch_size=BATCH_SIZE,                                                                  shuffle=False,                                                                  num_workers=NUM_WORKERS) In\u00a0[72]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n                             lr=1e-3)\n\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n\n# Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset\nset_seeds()    \neffnetb2_food101_results = engine.train(model=effnetb2_food101,\n                                        train_dataloader=train_dataloader_food101_20_percent,\n                                        test_dataloader=test_dataloader_food101_20_percent,\n                                        optimizer=optimizer,\n                                        loss_fn=loss_fn,\n                                        epochs=5,\n                                        device=device)\n</pre> from going_modular.going_modular import engine  # Setup optimizer optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),                              lr=1e-3)  # Setup loss function loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes  # Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset set_seeds()     effnetb2_food101_results = engine.train(model=effnetb2_food101,                                         train_dataloader=train_dataloader_food101_20_percent,                                         test_dataloader=test_dataloader_food101_20_percent,                                         optimizer=optimizer,                                         loss_fn=loss_fn,                                         epochs=5,                                         device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 3.6317 | train_acc: 0.2869 | test_loss: 2.7670 | test_acc: 0.4937\nEpoch: 2 | train_loss: 2.8615 | train_acc: 0.4388 | test_loss: 2.4653 | test_acc: 0.5387\nEpoch: 3 | train_loss: 2.6585 | train_acc: 0.4844 | test_loss: 2.3547 | test_acc: 0.5649\nEpoch: 4 | train_loss: 2.5494 | train_acc: 0.5116 | test_loss: 2.3038 | test_acc: 0.5755\nEpoch: 5 | train_loss: 2.5006 | train_acc: 0.5239 | test_loss: 2.2805 | test_acc: 0.5810\n</pre> <p>Woohoo!!!!</p> <p>Looks like we beat the original Food101 paper's results of 56.4% accuracy with only 20% of the training data (though we only evaluated on 20% of the testing data too, to fully replicate the results, we could evaluate on 100% of the testing data).</p> <p>That's the power of transfer learning!</p> In\u00a0[73]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Check out the loss curves for FoodVision Big\nplot_loss_curves(effnetb2_food101_results)\n</pre> from helper_functions import plot_loss_curves  # Check out the loss curves for FoodVision Big plot_loss_curves(effnetb2_food101_results) <p>Nice!!!</p> <p>It looks like our regularization techniques (data augmentation and label smoothing) helped prevent our model from overfitting (the training loss is still higher than the test loss) this indicates our model has a bit more capacity to learn and could improve with further training.</p> In\u00a0[74]: Copied! <pre>from going_modular.going_modular import utils\n\n# Create a model path\neffnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n\n# Save FoodVision Big model\nutils.save_model(model=effnetb2_food101,\n                 target_dir=\"models\",\n                 model_name=effnetb2_food101_model_path)\n</pre> from going_modular.going_modular import utils  # Create a model path effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"   # Save FoodVision Big model utils.save_model(model=effnetb2_food101,                  target_dir=\"models\",                  model_name=effnetb2_food101_model_path) <pre>[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n</pre> <p>Model saved!</p> <p>Before we move on, let's make sure we can load it back in.</p> <p>We'll do so by creating a model instance first with <code>create_effnetb2_model(num_classes=101)</code> (101 classes for all Food101 classes).</p> <p>And then loading the saved <code>state_dict()</code> with <code>torch.nn.Module.load_state_dict()</code> and <code>torch.load()</code>.</p> In\u00a0[75]: Copied! <pre># Create Food101 compatible EffNetB2 instance\nloaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\n# Load the saved model's state_dict()\nloaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))\n</pre> # Create Food101 compatible EffNetB2 instance loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)  # Load the saved model's state_dict() loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\")) Out[75]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[76]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\") <pre>Pretrained EffNetB2 feature extractor Food101 model size: 30 MB\n</pre> <p>Hmm, it looks like the model size stayed largely the same (30 MB for FoodVision Big and 29 MB for FoodVision Mini) despite the large increase in the number of classes.</p> <p>This is because all the extra parameters for FoodVision Big are only in the last layer (the classifier head).</p> <p>All of the base layers are the same between FoodVision Big and FoodVision Mini.</p> <p>Going back up and comparing the model summaries will give more details.</p> Model Output shape (num classes) Trainable parameters Total parameters Model size (MB) FoodVision Mini (EffNetB2 feature extractor) 3 4,227 7,705,221 29 FoodVision Big (EffNetB2 feature extractor) 101 142,309 7,843,303 30 In\u00a0[77]: Copied! <pre>from pathlib import Path\n\n# Create FoodVision Big demo path\nfoodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n\n# Make FoodVision Big demo directory\nfoodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n\n# Make FoodVision Big demo examples directory\n(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)\n</pre> from pathlib import Path  # Create FoodVision Big demo path foodvision_big_demo_path = Path(\"demos/foodvision_big/\")  # Make FoodVision Big demo directory foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)  # Make FoodVision Big demo examples directory (foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True) In\u00a0[78]: Copied! <pre># Download and move an example image\n!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n\n# Move trained model to FoodVision Big demo folder (will error if model is already moved)\n!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big\n</pre> # Download and move an example image !wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg  !mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg  # Move trained model to FoodVision Big demo folder (will error if model is already moved) !mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big <pre>--2022-08-25 14:24:41--  https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2874848 (2.7M) [image/jpeg]\nSaving to: '04-pizza-dad.jpeg\u2019\n\n04-pizza-dad.jpeg   100%[===================&gt;]   2.74M  7.85MB/s    in 0.3s    \n\n2022-08-25 14:24:43 (7.85 MB/s) - '04-pizza-dad.jpeg\u2019 saved [2874848/2874848]\n\n</pre> In\u00a0[79]: Copied! <pre># Check out the first 10 Food101 class names\nfood101_class_names[:10]\n</pre> # Check out the first 10 Food101 class names food101_class_names[:10] Out[79]: <pre>['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']</pre> <p>Wonderful, now we can write these to a text file by first creating a path to <code>demos/foodvision_big/class_names.txt</code> and then opening a file with Python's <code>open()</code> and then writing to it leaving a new line for each class.</p> <p>Ideally, we want our class names to be saved like:</p> <pre><code>apple_pie\nbaby_back_ribs\nbaklava\nbeef_carpaccio\nbeef_tartare\n...\n</code></pre> In\u00a0[80]: Copied! <pre># Create path to Food101 class names\nfoodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n\n# Write Food101 class names list to file\nwith open(foodvision_big_class_names_path, \"w\") as f:\n    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class\n</pre> # Create path to Food101 class names foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"  # Write Food101 class names list to file with open(foodvision_big_class_names_path, \"w\") as f:     print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")     f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class <pre>[INFO] Saving Food101 class names to demos/foodvision_big/class_names.txt\n</pre> <p>Excellent, now let's make sure we can read them in.</p> <p>To do so we'll use Python's <code>open()</code> in read mode (<code>\"r\"</code>) and then use the <code>readlines()</code> method to read each line of our <code>class_names.txt</code> file.</p> <p>And we can save the class names to a list by stripping the newline value of each of them with a list comprehension and <code>strip()</code>.</p> In\u00a0[81]: Copied! <pre># Open Food101 class names file and read each line into a list\nwith open(foodvision_big_class_names_path, \"r\") as f:\n    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n    \n# View the first 5 class names loaded back in\nfood101_class_names_loaded[:5]\n</pre> # Open Food101 class names file and read each line into a list with open(foodvision_big_class_names_path, \"r\") as f:     food101_class_names_loaded = [food.strip() for food in  f.readlines()]      # View the first 5 class names loaded back in food101_class_names_loaded[:5] Out[81]: <pre>['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']</pre> In\u00a0[82]: Copied! <pre>%%writefile demos/foodvision_big/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n\"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> %%writefile demos/foodvision_big/model.py import torch import torchvision  from torch import nn   def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <pre>Overwriting demos/foodvision_big/model.py\n</pre> In\u00a0[83]: Copied! <pre>%%writefile demos/foodvision_big/app.py\n### 1. Imports and class names setup ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Setup class names\nwith open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n    class_names = [food_name.strip() for food_name in  f.readlines()]\n    \n### 2. Model and transforms preparation ###    \n\n# Create model\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=101, # could also use len(class_names)\n)\n\n# Load saved weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n### 3. Predict function ###\n\n# Create predict function\ndef predict(img) -&gt; Tuple[Dict, float]:\n\"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article strings\ntitle = \"FoodVision Big \ud83c\udf54\ud83d\udc41\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create examples list from \"examples/\" directory\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create Gradio interface \ndemo = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=[\n        gr.Label(num_top_classes=5, label=\"Predictions\"),\n        gr.Number(label=\"Prediction time (s)\"),\n    ],\n    examples=example_list,\n    title=title,\n    description=description,\n    article=article,\n)\n\n# Launch the app!\ndemo.launch()\n</pre> %%writefile demos/foodvision_big/app.py ### 1. Imports and class names setup ###  import gradio as gr import os import torch  from model import create_effnetb2_model from timeit import default_timer as timer from typing import Tuple, Dict  # Setup class names with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt     class_names = [food_name.strip() for food_name in  f.readlines()]      ### 2. Model and transforms preparation ###      # Create model effnetb2, effnetb2_transforms = create_effnetb2_model(     num_classes=101, # could also use len(class_names) )  # Load saved weights effnetb2.load_state_dict(     torch.load(         f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",         map_location=torch.device(\"cpu\"),  # load to CPU     ) )  ### 3. Predict function ###  # Create predict function def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time  ### 4. Gradio app ###  # Create title, description and article strings title = \"FoodVision Big \ud83c\udf54\ud83d\udc41\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Create examples list from \"examples/\" directory example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]  # Create Gradio interface  demo = gr.Interface(     fn=predict,     inputs=gr.Image(type=\"pil\"),     outputs=[         gr.Label(num_top_classes=5, label=\"Predictions\"),         gr.Number(label=\"Prediction time (s)\"),     ],     examples=example_list,     title=title,     description=description,     article=article, )  # Launch the app! demo.launch() <pre>Overwriting demos/foodvision_big/app.py\n</pre> In\u00a0[84]: Copied! <pre>%%writefile demos/foodvision_big/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n</pre> %%writefile demos/foodvision_big/requirements.txt torch==1.12.0 torchvision==0.13.0 gradio==3.1.4 <pre>Overwriting demos/foodvision_big/requirements.txt\n</pre> In\u00a0[85]: Copied! <pre># Zip foodvision_big folder but exclude certain files\n!cd demos/foodvision_big &amp;&amp; zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Download the zipped FoodVision Big app (if running in Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_big.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download()\")\n</pre> # Zip foodvision_big folder but exclude certain files !cd demos/foodvision_big &amp;&amp; zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"  # Download the zipped FoodVision Big app (if running in Google Colab) try:     from google.colab import files     files.download(\"demos/foodvision_big.zip\") except:     print(\"Not running in Google Colab, can't use google.colab.files.download()\") <pre>updating: 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth (deflated 8%)\nupdating: app.py (deflated 54%)\nupdating: class_names.txt (deflated 48%)\nupdating: examples/ (stored 0%)\nupdating: flagged/ (stored 0%)\nupdating: model.py (deflated 56%)\nupdating: requirements.txt (deflated 4%)\nupdating: examples/04-pizza-dad.jpg (deflated 0%)\nNot running in Google Colab, can't use google.colab.files.download()\n</pre> In\u00a0[86]: Copied! <pre># IPython is a library to help work with Python iteractively \nfrom IPython.display import IFrame\n\n# Embed FoodVision Big Gradio demo as an iFrame\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)\n</pre> # IPython is a library to help work with Python iteractively  from IPython.display import IFrame  # Embed FoodVision Big Gradio demo as an iFrame IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750) Out[86]: <p>How cool is that!?!</p> <p>We've come a long way from building PyTorch models to predict a straight line... now we're building computer vision models accessible to people all around the world!</p>"},{"location":"09_pytorch_model_deployment/#09-pytorch-model-deployment","title":"09. PyTorch Model Deployment\u00b6","text":"<p>Welcome to Milestone Project 3: PyTorch Model Deployment!</p> <p>We've come a long way with our FoodVision Mini project.</p> <p>But so far our PyTorch models have only been accessible to us.</p> <p>How about we bring FoodVision Mini to life and make it publically accessible?</p> <p>In other words, we're going to deploy our FoodVision Mini model to the internet as a usable app!</p> <p>Trying out the deployed version of FoodVision Mini (what we're going to build) on my lunch. The model got it right too \ud83c\udf63!</p>"},{"location":"09_pytorch_model_deployment/#what-is-machine-learning-model-deployment","title":"What is machine learning model deployment?\u00b6","text":"<p>Machine learning model deployment is the process of making your machine learning model accessible to someone or something else.</p> <p>Someone else being a person who can interact with your model in some way.</p> <p>For example, someone taking a photo on their smartphone of food and then having our FoodVision Mini model classify it into pizza, steak or sushi.</p> <p>Something else might be another program, app or even another model that interacts with your machine learning model(s).</p> <p>For example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.</p> <p>Or an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.</p> <p>These use cases can be mixed and matched as well.</p> <p>For example, a Tesla car's computer vision system will interact with the car's route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).</p> <p>Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or Nutrify). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.</p>"},{"location":"09_pytorch_model_deployment/#why-deploy-a-machine-learning-model","title":"Why deploy a machine learning model?\u00b6","text":"<p>One of the most important philosophical questions in machine learning is:</p> <p>Deploying a model is as important as training one.</p> <p>Because although you can get a pretty good idea of how your model's going to function by evaluting it on a well crafted test set or visualizing its results, you never really know how it's going to perform until you release it to the wild.</p> <p>Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.</p> <p>For example, what happens if someone was to upload a photo that wasn't of food to our FoodVision Mini model?</p> <p>One solution would be to create another model that first classifies images as \"food\" or \"not food\" and passing the target image through that model first (this is what Nutrify does).</p> <p>Then if the image is of \"food\" it goes to our FoodVision Mini model and gets classified into pizza, steak or sushi.</p> <p>And if it's \"not food\", a message is displayed.</p> <p>But what if these predictions were wrong?</p> <p>What happens then?</p> <p>You can see how these questions could keep going.</p> <p>Thus this highlights the importance of model deployment: it helps you figure out errors in your model that aren't obvious during training/testing.</p> <p>We covered a PyTorch workflow back in 01. PyTorch Workflow. But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world. For more resources on deployment and monitoring see PyTorch Extra Resources.</p>"},{"location":"09_pytorch_model_deployment/#different-types-of-machine-learning-model-deployment","title":"Different types of machine learning model deployment\u00b6","text":"<p>Whole books could be written on the different types of machine learning model deployment (and many good ones are listed in PyTorch Extra Resources).</p> <p>And the field is still developing in terms of best practices.</p> <p>But I like to start with the question:</p> <p>\"What is the most ideal scenario for my machine learning model to be used?\"</p> <p>And then work backwards from there.</p> <p>Of course, you may not know this ahead of time. But you're smart enough to imagine such things.</p> <p>In the case of FoodVision Mini, our ideal scenario might be:</p> <ul> <li>Someone takes a photo on a mobile device (through an app or web broswer).</li> <li>The prediction comes back fast.</li> </ul> <p>Easy.</p> <p>So we've got two main criteria:</p> <ol> <li>The model should work on a mobile device (this means there will be some compute constraints).</li> <li>The model should make predictions fast (because a slow app is a boring app).</li> </ol> <p>And of course, depending on your use case, your requirements may vary.</p> <p>You may notice the above two points break down into another two questions:</p> <ol> <li>Where's it going to go? - As in, where is it going to be stored?</li> <li>How's it going to function? - As in, does it return predictions immediately? Or do they come later?</li> </ol> <p>When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model's going to go and then how it's going to function.</p>"},{"location":"09_pytorch_model_deployment/#wheres-it-going-to-go","title":"Where's it going to go?\u00b6","text":"<p>When you deploy your machine learning model, where does it live?</p> <p>The main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the actual device someone/something calls the model from).</p> <p>Both have their pros and cons.</p> Deployment location Pros Cons On-device (edge/in the browser) Can be very fast (since no data leaves the device) Limited compute power (larger models take longer to run) Privacy preserving (again no data has to leave the device) Limited storage space (smaller model size required) No internet connection required (sometimes) Device-specific skills often required On cloud Near unlimited compute power (can scale up when needed) Costs can get out of hand (if proper scaling limits aren't enforced) Can deploy one model and use everywhere (via API) Predictions can be slower due to data having to leave device and predictions having to come back (network latency) Links into existing cloud ecosystem Data has to leave device (this may cause privacy concerns) <p>There are more details to these but I've left resources in the extra-curriculum to learn more.</p> <p>Let's give an example.</p> <p>If we're deploying FoodVision Mini as an app, we want it to perform well and fast.</p> <p>So which model would we prefer?</p> <ol> <li>A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.</li> <li>A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per per prediction (bigger, better model but takes longer to compute).</li> </ol> <p>I've made these numbers up but they showcase a potential difference between on-device and on the cloud.</p> <p>Option 1 could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.</p> <p>Option 2 could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).</p> <p>For FoodVision Mini, we'd likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.</p> <p>In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even possible with poor signal areas).</p> <p>Note: For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the PyTorch tutorial on achieving real-time inference (30fps+) with a computer vision model on a Raspberry Pi.</p>"},{"location":"09_pytorch_model_deployment/#hows-it-going-to-function","title":"How's it going to function?\u00b6","text":"<p>Back to the ideal use case, when you deploy your machine learning model, how should it work?</p> <p>As in, would you like predictions returned immediately?</p> <p>Or is it okay for them to happen later?</p> <p>These two scenarios are generally referred to as:</p> <ul> <li>Online (real-time) - Predicitions/inference happen immediately. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fradulent by a model so the purchase can go through.</li> <li>Offline (batch) - Predictions/inference happen periodically. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.</li> </ul> <p>Note: \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (mutliple images being predicted/trained on at once).</p> <p>The main difference between each being: predictions being made immediately or periodically.</p> <p>Periodically can have a varying timescale too, from every few seconds to every few hours or days.</p> <p>And you can mix and match the two.</p> <p>In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).</p> <p>But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters.</p>"},{"location":"09_pytorch_model_deployment/#ways-to-deploy-a-machine-learning-model","title":"Ways to deploy a machine learning model\u00b6","text":"<p>We've discussed a couple of options for deploying machine learning models (on-device and cloud).</p> <p>And each of these will have their specific requirements:</p> Tool/resource Deployment type Google's ML Kit On-device (Android and iOS) Apple's Core ML and <code>coremltools</code> Python package On-device (all Apple devices) Amazon Web Service's (AWS) Sagemaker Cloud Google Cloud's Vertex AI Cloud Microsoft's Azure Machine Learning Cloud Hugging Face Spaces Cloud API with FastAPI Cloud/self-hosted server API with TorchServe Cloud/self-hosted server ONNX (Open Neural Network Exchange) Many/general Many more... <p>Note: An application programming interface (API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.</p> <p>Which option you choose will be highly dependent on what you're building/who you're working with.</p> <p>But with so many options, it can be very intimidating.</p> <p>So best to start small and keep it simple.</p> <p>And one of the best ways to do so is by turning your machine learning model into a demo app with Gradio and then deploying it on Hugging Face Spaces.</p> <p>We'll be doing just that with FoodVision Mini later on.</p> <p>A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a discussion on GitHub.</p>"},{"location":"09_pytorch_model_deployment/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Enough talking about deploying a machine learning model.</p> <p>Let's become machine learning engineers and actually deploy one.</p> <p>Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:</p> <ol> <li>Performance: 95%+ accuracy.</li> <li>Speed: real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).</li> </ol> <p>We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.</p> <p>Then we'll deploy the one which performs closest to our goal metrics.</p> <p>Finally, we'll finish with a (BIG) surprise bonus.</p> Topic Contents 0. Getting setup We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. 1. Get data Let's download the <code>pizza_steak_sushi_20_percent.zip</code> dataset so we can train our previously best performing models on the same dataset. 2. FoodVision Mini model deployment experiment outline Even on the third milestone project, we're still going to be running multiple experiments to see which model (EffNetB2 or ViT) achieves closest to our goal metrics. 3. Creating an EffNetB2 feature extractor An EfficientNetB2 feature extractor performed the best on our pizza, steak, sushi dataset in 07. PyTorch Experiment Tracking, let's recreate it as a candidate for deployment. 4. Creating a ViT feature extractor A ViT feature extractor has been the best performing model yet on our pizza, steak, sushi dataset in 08. PyTorch Paper Replicating, let's recreate it as a candidate for deployment alongside EffNetB2. 5. Making predictions with our trained models and timing them We've built two of the best performing models yet, let's make predictions with them and track their results. 6. Comparing model results, prediction times and size Let's compare our models to see which performs best with our goals. 7. Bringing FoodVision Mini to life by creating a Gradio demo One of our models performs better than the other (in terms of our goals), so let's turn it into a working app demo! 8. Turning our FoodVision Mini Gradio demo into a deployable app Our Gradio app demo works locally, let's prepare it for deployment! 9. Deploying our Gradio demo to HuggingFace Spaces Let's take FoodVision Mini to the web and make it pubically accessible for all! 10. Creating a BIG surprise We've built FoodVision Mini, time to step things up a notch. 11. Deploying our BIG surprise Deploying one app was fun, how about we make it two?"},{"location":"09_pytorch_model_deployment/#where-can-you-get-help","title":"Where can you get help?\u00b6","text":"<p>All of the materials for this course are available on GitHub.</p> <p>If you run into trouble, you can ask a question on the course GitHub Discussions page.</p> <p>And of course, there's the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.</p>"},{"location":"09_pytorch_model_deployment/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>As we've done previously, let's make sure we've got all of the modules we'll need for this section.</p> <p>We'll import the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in 05. PyTorch Going Modular.</p> <p>To do so, we'll download <code>going_modular</code> directory from the <code>pytorch-deep-learning</code> repository (if we don't already have it).</p> <p>We'll also get the <code>torchinfo</code> package if it's not available.</p> <p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p> <p>And since later on we'll be using <code>torchvision</code> v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.</p> <p>Note: If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p>"},{"location":"09_pytorch_model_deployment/#1-getting-data","title":"1. Getting data\u00b6","text":"<p>We left off in 08. PyTorch Paper Replicating comparing our own Vision Transformer (ViT) feature extractor model to the EfficientNetB2 (EffNetB2) feature extractor model we created in 07. PyTorch Experiment Tracking.</p> <p>And we found that there was a slight difference in the comparison.</p> <p>The EffNetB2 model was trained on 20% of the pizza, steak and sushi data from Food101 where as the ViT model was trained on 10%.</p> <p>Since our goal is to deploy the best model for our FoodVision Mini problem, let's start by downloading the 20% pizza, steak and sushi dataset and train an EffNetB2 feature extractor and ViT feature extractor on it and then compare the two models.</p> <p>This way we'll be comparing apples to apples (one model trained on a dataset to another model trained on the same dataset).</p> <p>Note: The dataset we're downloading is a sample of the entire Food101 dataset (101 food classes with 1,000 images each). More specifically, 20% refers to 20% of images from the pizza, steak and sushi classes selected at random. You can see how this dataset was created in <code>extras/04_custom_data_creation.ipynb</code> and more details in 04. PyTorch Custom Datasets section 1.</p> <p>We can download the data using the <code>download_data()</code> function we created in 07. PyTorch Experiment Tracking section 1 from <code>helper_functions.py</code>.</p>"},{"location":"09_pytorch_model_deployment/#2-foodvision-mini-model-deployment-experiment-outline","title":"2. FoodVision Mini model deployment experiment outline\u00b6","text":"<p>The ideal deployed model FoodVision Mini performs well and fast.</p> <p>We'd like our model to perform as close to real-time as possible.</p> <p>Real-time in this case being ~30FPS (frames per second) because that's about how fast the human eye can see (there is debate on this but let's just use ~30FPS as our benchmark).</p> <p>And for classifying three different classes (pizza, steak and sushi), we'd like a model that performs at 95%+ accuracy.</p> <p>Of course, higher accuracy would be nice but this might sacrifice speed.</p> <p>So our goals are:</p> <ol> <li>Performance - A model that performs at 95%+ accuracy.</li> <li>Speed - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).</li> </ol> <p>FoodVision Mini deployment goals. We'd like a fast predicting well-performing model (because a slow app is boring).</p> <p>We'll put an emphasis on speed, meaning, we'd prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.</p> <p>To try and achieve these results, let's bring in our best performing models from the previous sections:</p> <ol> <li>EffNetB2 feature extractor (EffNetB2 for short) - originally created in 07. PyTorch Experiment Tracking section 7.5 using <code>torchvision.models.efficientnet_b2()</code> with adjusted <code>classifier</code> layers.</li> <li>ViT-B/16 feature extractor (ViT for short) - originally created in 08. PyTorch Paper Replicating section 10 using <code>torchvision.models.vit_b_16()</code> with adjusted <code>head</code> layers.<ul> <li>Note ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".</li> </ul> </li> </ol> <p>Note: A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your own problem. The pretrained model's base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your own problem by training on your own data. We covered the concept of a feature extractor model in 06. PyTorch Transfer Learning section 3.4.</p>"},{"location":"09_pytorch_model_deployment/#3-creating-an-effnetb2-feature-extractor","title":"3. Creating an EffNetB2 feature extractor\u00b6","text":"<p>We first created an EffNetB2 feature extractor model in 07. PyTorch Experiment Tracking section 7.5.</p> <p>And by the end of that section we saw it performed very well.</p> <p>So let's now recreate it here so we can compare its results to a ViT feature extractor trained on the same data.</p> <p>To do so we can:</p> <ol> <li>Setup the pretrained weights as <code>weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT</code>, where \"<code>DEFAULT</code>\" means \"best currently available\" (or could use <code>weights=\"DEFAULT\"</code>).</li> <li>Get the pretrained model image transforms from the weights with the <code>transforms()</code> method (we need these so we can convert our images into the same format as the pretrained EffNetB2 was trained on).</li> <li>Create a pretrained model instance by passing the weights to an instance of <code>torchvision.models.efficientnet_b2</code>.</li> <li>Freeze the base layers in the model.</li> <li>Update the classifier head to suit our own data.</li> </ol>"},{"location":"09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor","title":"3.1 Creating a function to make an EffNetB2 feature extractor\u00b6","text":"<p>Looks like our EffNetB2 feature extractor is ready to go, however, since there's quite a few steps involved here, how about we turn the code above into a function we can re-use later?</p> <p>We'll call it <code>create_effnetb2_model()</code> and it'll take a customizable number of classes and a random seed parameter for reproducibility.</p> <p>Ideally, it will return an EffNetB2 feature extractor along with its assosciated transforms.</p>"},{"location":"09_pytorch_model_deployment/#32-creating-dataloaders-for-effnetb2","title":"3.2 Creating DataLoaders for EffNetB2\u00b6","text":"<p>Our EffNetB2 feature extractor is ready, time to create some <code>DataLoader</code>s.</p> <p>We can do this by using the <code>data_setup.create_dataloaders()</code> function we created in 05. PyTorch Going Modular section 2.</p> <p>We'll use a <code>batch_size</code> of 32 and transform our images using the <code>effnetb2_transforms</code> so they're in the same format that our <code>effnetb2</code> model was trained on.</p>"},{"location":"09_pytorch_model_deployment/#33-training-effnetb2-feature-extractor","title":"3.3 Training EffNetB2 feature extractor\u00b6","text":"<p>Model ready, <code>DataLoader</code>s ready, let's train!</p> <p>Just like in 07. PyTorch Experiment Tracking section 7.6, ten epochs should be enough to get good results.</p> <p>We can do so by creating an optimizer (we'll use <code>torch.optim.Adam()</code> with a learning rate of <code>1e-3</code>), a loss function (we'll use <code>torch.nn.CrossEntropyLoss()</code> for multi-class classification) and then passing these as well as our <code>DataLoader</code>s to the <code>engine.train()</code> function we created in 05. PyTorch Going Modular section 4.</p>"},{"location":"09_pytorch_model_deployment/#34-inspecting-effnetb2-loss-curves","title":"3.4 Inspecting EffNetB2 loss curves\u00b6","text":"<p>Nice!</p> <p>As we saw in 07. PyTorch Experiment Tracking, the EffNetB2 feature extractor model works quite well on our data.</p> <p>Let's turn its results into loss curves to inspect them further.</p> <p>Note: Loss curves are one of the best ways to visualize how your model's performing. For more on loss curves, check out 04. PyTorch Custom Datasets section 8: What should an ideal loss curve look like?</p>"},{"location":"09_pytorch_model_deployment/#35-saving-effnetb2-feature-extractor","title":"3.5 Saving EffNetB2 feature extractor\u00b6","text":"<p>Now we've got a well-performing trained model, let's save it to file so we can import and use it later.</p> <p>To save our model we can use the <code>utils.save_model()</code> function we created in 05. PyTorch Going Modular section 5.</p> <p>We'll set the <code>target_dir</code> to <code>\"models\"</code> and the <code>model_name</code> to <code>\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"</code> (a little comprehensive but at least we know what's going on).</p>"},{"location":"09_pytorch_model_deployment/#36-checking-the-size-of-effnetb2-feature-extractor","title":"3.6 Checking the size of EffNetB2 feature extractor\u00b6","text":"<p>Since one of our criteria for deploying a model to power FoodVision Mini is speed (~30FPS or better), let's check the size of our model.</p> <p>Why check the size?</p> <p>Well, while not always the case, the size of a model can influence its inference speed.</p> <p>As in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.</p> <p>And because we'd like our model to work on devices with limited computing power (e.g. on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).</p> <p>To check our model's size in bytes, we can use Python's <code>pathlib.Path.stat(\"path_to_model\").st_size</code> and then we can convert it (roughly) to megabytes by dividing it by <code>(1024*1024)</code>.</p>"},{"location":"09_pytorch_model_deployment/#37-collecting-effnetb2-feature-extractor-stats","title":"3.7 Collecting EffNetB2 feature extractor stats\u00b6","text":"<p>We've got a few statistics about our EffNetB2 feature extractor model such as test loss, test accuracy and model size, how about we collect them all in a dictionary so we can compare them to the upcoming ViT feature extractor.</p> <p>And we'll calculate an extra one for fun, total number of parameters.</p> <p>We can do so by counting the number of elements (or patterns/weights) in <code>effnetb2.parameters()</code>. We'll access the number of elements in each parameter using the <code>torch.numel()</code> (short for \"number of elements\") method.</p>"},{"location":"09_pytorch_model_deployment/#4-creating-a-vit-feature-extractor","title":"4. Creating a ViT feature extractor\u00b6","text":"<p>Time to continue with our FoodVision Mini modelling experiments.</p> <p>This time we're going to create a ViT feature extractor.</p> <p>And we'll do it in much the same way as the EffNetB2 feature extractor except this time with <code>torchvision.models.vit_b_16()</code> instead of <code>torchvision.models.efficientnet_b2()</code>.</p> <p>We'll start by creating a function called <code>create_vit_model()</code> which will be very similar to <code>create_effnetb2_model()</code> except of course returning a ViT feature extractor model and transforms rather than EffNetB2.</p> <p>Another slight difference is that <code>torchvision.models.vit_b_16()</code>'s output layer is called <code>heads</code> rather than <code>classifier</code>.</p>"},{"location":"09_pytorch_model_deployment/#41-create-dataloaders-for-vit","title":"4.1 Create DataLoaders for ViT\u00b6","text":"<p>We've got our ViT model ready, now let's create some <code>DataLoader</code>s for it.</p> <p>We'll do this in the same way we did for EffNetB2 except we'll use <code>vit_transforms</code> to transform our images into the same format the ViT model was trained on.</p>"},{"location":"09_pytorch_model_deployment/#42-training-vit-feature-extractor","title":"4.2 Training ViT feature extractor\u00b6","text":"<p>You know what time it is...</p> <p>...it's traininggggggg time (sung in the same tune as the song Closing Time).</p> <p>Let's train our ViT feature extractor model for 10 epochs using our <code>engine.train()</code> function with <code>torch.optim.Adam()</code> and a learning rate of <code>1e-3</code> as our optimizer and <code>torch.nn.CrossEntropyLoss()</code> as our loss function.</p> <p>We'll use our <code>set_seeds()</code> function before training to try and make our results as reproducible as possible.</p>"},{"location":"09_pytorch_model_deployment/#43-inspecting-vit-loss-curves","title":"4.3 Inspecting ViT loss curves\u00b6","text":"<p>Alright, alright, alright, ViT model trained, let's get visual and see some loss curves.</p> <p>Note: Don't forget you can see what an ideal set of loss curves should look like in 04. PyTorch Custom Datasets section 8.</p>"},{"location":"09_pytorch_model_deployment/#44-saving-vit-feature-extractor","title":"4.4 Saving ViT feature extractor\u00b6","text":"<p>Our ViT model is performing outstanding!</p> <p>So let's save it to file so we can import it and use it later if we wish.</p> <p>We can do so using the <code>utils.save_model()</code> function we created in 05. PyTorch Going Modular section 5.</p>"},{"location":"09_pytorch_model_deployment/#45-checking-the-size-of-vit-feature-extractor","title":"4.5 Checking the size of ViT feature extractor\u00b6","text":"<p>And since we want to compare our EffNetB2 model to our ViT model across a number of characteristics, let's find out its size.</p> <p>To check our model's size in bytes, we can use Python's <code>pathlib.Path.stat(\"path_to_model\").st_size</code> and then we can convert it (roughly) to megabytes by dividing it by <code>(1024*1024)</code>.</p>"},{"location":"09_pytorch_model_deployment/#46-collecting-vit-feature-extractor-stats","title":"4.6 Collecting ViT feature extractor stats\u00b6","text":"<p>Let's put together all of our ViT feature extractor model statistics.</p> <p>We saw it in the summary output above but we'll calculate its total number of parameters.</p>"},{"location":"09_pytorch_model_deployment/#5-making-predictions-with-our-trained-models-and-timing-them","title":"5. Making predictions with our trained models and timing them\u00b6","text":"<p>We've got a couple of trained models, both performing pretty well.</p> <p>Now how about we test them out doing what we'd like them to do?</p> <p>As in, let's see how they go making predictions (performing inference).</p> <p>We know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?</p> <p>Ideally, if we're deploying our FoodVision Mini model to a mobile device so people can take photos of their food and identify it, we'd like the predictions to happen at real-time (~30 frames per second).</p> <p>That's why our second criteria is: a fast model.</p> <p>To find out how long each of our models take to performance inference, let's create a function called <code>pred_and_store()</code> to iterate over each of the test dataset images one by one and perform a prediction.</p> <p>We'll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary).</p> <p>Note: We time the predictions one by one rather than by batch because when our model is deployed, it will likely only be making a prediction on one image at a time. As in, someone takes a photo and our model predicts on that single image.</p> <p>Since we'd like to make predictions across all the images in the test set, let's first get a list of all of the test image paths so we can iterate over them.</p> <p>To do so, we'll use Python's <code>pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))</code> to find all of the filepaths in a target directory with the extension <code>.jpg</code> (all of our test images).</p>"},{"location":"09_pytorch_model_deployment/#51-creating-a-function-to-make-predictions-across-the-test-dataset","title":"5.1 Creating a function to make predictions across the test dataset\u00b6","text":"<p>Now we've got a list of our test image paths, let's get to work on our <code>pred_and_store()</code> function:</p> <ol> <li>Create a function that takes a list of paths, a trained PyTorch model, a series of transforms (to prepare images), a list of target class names and a target device.</li> <li>Create an empty list to store prediction dictionaries (we want the function to return a list of dictionaries, one for each prediction).</li> <li>Loop through the target input paths (steps 4-14 will happen inside the loop).</li> <li>Create an empty dictionary for each iteration in the loop to store prediction values per sample.</li> <li>Get the sample path and ground truth class name (we can do this by infering the class from the path).</li> <li>Start the prediction timer using Python's <code>timeit.default_timer()</code>.</li> <li>Open the image using <code>PIL.Image.open(path)</code>.</li> <li>Transform the image so it's capable of being using with the target model as well as add a batch dimension and send the image to the target device.</li> <li>Prepare the model for inference by sending it to the target device and turning on <code>eval()</code> mode.</li> <li>Turn on <code>torch.inference_mode()</code> and pass the target transformed image to the model and calculate the prediction probability using <code>torch.softmax()</code> and the target label using <code>torch.argmax()</code>.</li> <li>Add the prediction probability and prediction class to the prediction dictionary created in step 4. Also make sure the prediction probability is on the CPU so it can be used with non-GPU libraries such as NumPy and pandas for later inspection.</li> <li>End the prediction timer started in step 6 and add the time to the prediction dictionary created in step 4.</li> <li>See if the predicted class matches the ground truth class from step 5 and add the result to the prediction dictionary created in step 4.</li> <li>Append the updated prediction dictionary to the empty list of predictions created in step 2.</li> <li>Return the list of prediction dictionaries.</li> </ol> <p>A bunch of steps, but nothing we can't handle!</p> <p>Let's do it.</p>"},{"location":"09_pytorch_model_deployment/#52-making-and-timing-predictions-with-effnetb2","title":"5.2 Making and timing predictions with EffNetB2\u00b6","text":"<p>Time to test out our <code>pred_and_store()</code> function!</p> <p>Let's start by using it to make predictions across the test dataset with our EffNetB2 model, paying attention to two details:</p> <ol> <li>Device - We'll hard code the <code>device</code> parameter to use <code>\"cpu\"</code> because when we deploy our model, we won't always have access to a <code>\"cuda\"</code> (GPU) device.<ul> <li>Making the predictions on CPU will be a good indicator of speed of inference too because generally predictions on CPU devices are slower than GPU devices.</li> </ul> </li> <li>Transforms - We'll also be sure to set the <code>transform</code> parameter to <code>effnetb2_transforms</code> to make sure the images are opened and transformed in the same way our <code>effnetb2</code> model has been trained on.</li> </ol>"},{"location":"09_pytorch_model_deployment/#53-making-and-timing-predictions-with-vit","title":"5.3 Making and timing predictions with ViT\u00b6","text":"<p>We've made predictions with our EffNetB2 model, now let's do the same for our ViT model.</p> <p>To do so, we can use the <code>pred_and_store()</code> function we created above except this time we'll pass in our <code>vit</code> model as well as the <code>vit_transforms</code>.</p> <p>And we'll keep the predictions on the CPU via <code>device=\"cpu\"</code> (a natural extension here would be to test the prediction times on CPU and on GPU).</p>"},{"location":"09_pytorch_model_deployment/#6-comparing-model-results-prediction-times-and-size","title":"6. Comparing model results, prediction times and size\u00b6","text":"<p>Our two best model contenders have been trained and evaluated.</p> <p>Now let's put them head to head and compare across their different statistics.</p> <p>To do so, let's turn our <code>effnetb2_stats</code> and <code>vit_stats</code> dictionaries into a pandas DataFrame.</p> <p>We'll add a column to view the model names as well as the convert the test accuracy to a whole percentage rather than decimal.</p>"},{"location":"09_pytorch_model_deployment/#61-visualizing-the-speed-vs-performance-tradeoff","title":"6.1 Visualizing the speed vs. performance tradeoff\u00b6","text":"<p>We've seen that our ViT model outperforms our EffNetB2 model in terms of performance metrics such as test loss and test accuracy.</p> <p>However, our EffNetB2 model makes performs predictions faster and has a much small model size.</p> <p>Note: Performance or inference time is also often referred to as \"latency\".</p> <p>How about we make this fact visual?</p> <p>We can do so by creating a plot with matplotlib:</p> <ol> <li>Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT <code>time_per_pred_cpu</code> and <code>test_acc</code> values.</li> <li>Add titles and labels respective of the data and customize the fontsize for aesthetics.</li> <li>Annotate the samples on the scatter plot from step 1 with their appropriate labels (the model names).</li> <li>Create a legend based on the model sizes (<code>model_size (MB)</code>).</li> </ol>"},{"location":"09_pytorch_model_deployment/#7-bringing-foodvision-mini-to-life-by-creating-a-gradio-demo","title":"7. Bringing FoodVision Mini to life by creating a Gradio demo\u00b6","text":"<p>We've decided we'd like to deploy the EffNetB2 model (to begin with, this could always be changed later).</p> <p>So how can we do that?</p> <p>There are several ways to deploy a machine learning model each with specific use cases (as discussed above).</p> <p>We're going to be focused on perhaps the quickest and certainly one of the most fun ways to get a model deployed to the internet.</p> <p>And that's by using Gradio.</p> <p>What's Gradio?</p> <p>The homepage describes it beautifully:</p> <p>Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!</p> <p>Why create a demo of your models?</p> <p>Because metrics on the test set look nice but you never really know how you're model performs until you use it in the wild.</p> <p>So let's get deploying!</p> <p>We'll start by importing Gradio with the common alias <code>gr</code> and if it's not present, we'll install it.</p>"},{"location":"09_pytorch_model_deployment/#71-gradio-overview","title":"7.1 Gradio overview\u00b6","text":"<p>The overall premise of Gradio is very similar to what we've been repeating throughout the course.</p> <p>What are our inputs and outputs?</p> <p>And how should we get there?</p> <p>Well that's what our machine learning model does.</p> <pre><code>inputs -&gt; ML model -&gt; outputs\n</code></pre> <p>In our case, for FoodVision Mini, our inputs are images of food, our ML model is EffNetB2 and our outputs are classes of food (pizza, steak or sushi).</p> <pre><code>images of food -&gt; EffNetB2 -&gt; outputs\n</code></pre> <p>Though the concepts of inputs and outputs can be bridged to almost any other kind of ML problem.</p> <p>Your inputs and outputs might be any combination of the following:</p> <ul> <li>Images</li> <li>Text</li> <li>Video</li> <li>Tabular data</li> <li>Audio</li> <li>Numbers</li> <li>&amp; more</li> </ul> <p>And the ML model you build will depend on your inputs and outputs.</p> <p>Gradio emulates this paradigm by creating an interface (<code>gradio.Interface()</code>) to from inputs to outputs.</p> <pre><code>gradio.Interface(fn, inputs, outputs)\n</code></pre> <p>Where, <code>fn</code> is a Python function to map the <code>inputs</code> to the <code>outputs</code>.</p> <p>Gradio provides a very helpful <code>Interface</code> class to easily create an inputs -&gt; model/function -&gt; outputs workflow where the inputs and outputs could be almost anything you want. For example, you might input Tweets (text) to see if they're about machine learning or not or input a text prompt to generate images.</p> <p>Note: Gradio has a vast number of possible <code>inputs</code> and <code>outputs</code> options known as \"Components\" from images to text to numbers to audio to videos and more. You can see all of these in the Gradio Components documentation.</p>"},{"location":"09_pytorch_model_deployment/#72-creating-a-function-to-map-our-inputs-and-outputs","title":"7.2 Creating a function to map our inputs and outputs\u00b6","text":"<p>To create our FoodVision Mini demo with Gradio, we'll need a function to map our inputs to our outputs.</p> <p>We created a function earlier called <code>pred_and_store()</code> to make predictions with a given model across a list of target files and store them in a list of dictionaries.</p> <p>How about we create a similar function but this time focusing on a making a prediction on a single image with our EffNetB2 model?</p> <p>More specifically, we want a function that takes an image as input, preprocesses (transforms) it, makes a prediction with EffNetB2 and then returns the prediction (pred or pred label for short) as well as the prediction probability (pred prob).</p> <p>And while we're here, let's return the time it took to do so too:</p> <pre><code>input: image -&gt; transform -&gt; predict with EffNetB2 -&gt; output: pred, pred prob, time taken\n</code></pre> <p>This will be our <code>fn</code> parameter for our Gradio interface.</p> <p>First, let's make sure our EffNetB2 model is on the CPU (since we're sticking with CPU-only predictions, however you could change this if you have access to a GPU).</p>"},{"location":"09_pytorch_model_deployment/#73-creating-a-list-of-example-images","title":"7.3 Creating a list of example images\u00b6","text":"<p>Our <code>predict()</code> function enables us to go from inputs -&gt; transform -&gt; ML model -&gt; outputs.</p> <p>Which is exactly what we need for our Graido demo.</p> <p>But before we create the demo, let's create one more thing: a list of examples.</p> <p>Gradio's <code>Interface</code> class takes a list of <code>examples</code> of as an optional parameter (<code>gradio.Interface(examples=List[Any])</code>).</p> <p>And the format for the <code>examples</code> parameter is a list of lists.</p> <p>So let's create a list of lists containing random filepaths to our test images.</p> <p>Three examples should be enough.</p>"},{"location":"09_pytorch_model_deployment/#74-building-a-gradio-interface","title":"7.4 Building a Gradio interface\u00b6","text":"<p>Time to put everything together and bring our FoodVision Mini demo to life!</p> <p>Let's create a Gradio interface to replicate the workflow:</p> <pre><code>input: image -&gt; transform -&gt; predict with EffNetB2 -&gt; output: pred, pred prob, time taken\n</code></pre> <p>We can do with the <code>gradio.Interface()</code> class with the following parameters:</p> <ul> <li><code>fn</code> - a Python function to map <code>inputs</code> to <code>outputs</code>, in our case, we'll use our <code>predict()</code> function.</li> <li><code>inputs</code> - the input to our interface, such as an image using <code>gradio.Image()</code> or <code>\"image\"</code>.</li> <li><code>outputs</code> - the output of our interface once the <code>inputs</code> have gone through the <code>fn</code>, such as a label using <code>gradio.Label()</code> (for our model's predicted labels) or number using <code>gradio.Number()</code> (for our model's prediction time).<ul> <li>Note: Gradio comes with many in-built <code>inputs</code> and <code>outputs</code> options known as \"Components\".</li> </ul> </li> <li><code>examples</code> - a list of examples to showcase for the demo.</li> <li><code>title</code> - a string title of the demo.</li> <li><code>description</code> - a string description of the demo.</li> <li><code>article</code> - a reference note at the bottom of the demo.</li> </ul> <p>Once we've created our demo instance of <code>gr.Interface()</code>, we can bring it to life using <code>gradio.Interface().launch()</code> or <code>demo.launch()</code> command.</p> <p>Easy!</p>"},{"location":"09_pytorch_model_deployment/#8-turning-our-foodvision-mini-gradio-demo-into-a-deployable-app","title":"8. Turning our FoodVision Mini Gradio Demo into a deployable app\u00b6","text":"<p>We've seen our FoodVision Mini model come to life through a Gradio demo.</p> <p>But what if we wanted to share it with our friends?</p> <p>Well, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.</p> <p>To make our FoodVision Mini demo more permanent, we can package it into an app and upload it to Hugging Face Spaces.</p>"},{"location":"09_pytorch_model_deployment/#81-what-is-hugging-face-spaces","title":"8.1 What is Hugging Face Spaces?\u00b6","text":"<p>Hugging Face Spaces is a resource that allows you to host and share machine learning apps.</p> <p>Building a demo is one of the best ways to showcase and test what you've done.</p> <p>And Spaces allows you to do just that.</p> <p>You can think of Hugging Face as the GitHub of machine learning.</p> <p>If having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities.</p> <p>Note: There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors, however, we're going to use Hugging Face Spaces due to the ease of use and wide adoption by the machine learning community.</p>"},{"location":"09_pytorch_model_deployment/#82-deployed-gradio-app-structure","title":"8.2 Deployed Gradio app structure\u00b6","text":"<p>To upload our demo Gradio app, we'll want to put everything relating to it into a single directory.</p> <p>For example, our demo might live at the path <code>demos/foodvision_mini/</code> with the file structure:</p> <pre><code>demos/\n\u2514\u2500\u2500 foodvision_mini/\n    \u251c\u2500\u2500 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n    \u251c\u2500\u2500 app.py\n    \u251c\u2500\u2500 examples/\n    \u2502   \u251c\u2500\u2500 example_1.jpg\n    \u2502   \u251c\u2500\u2500 example_2.jpg\n    \u2502   \u2514\u2500\u2500 example_3.jpg\n    \u251c\u2500\u2500 model.py\n    \u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Where:</p> <ul> <li><code>09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code> is our trained PyTorch model file.</li> <li><code>app.py</code> contains our Gradio app (similar to the code that launched the app).<ul> <li>Note: <code>app.py</code> is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called <code>app.py</code> to run. This is changable in settings.</li> </ul> </li> <li><code>examples/</code> contains example images to use with our Gradio app.</li> <li><code>model.py</code> contains the model defintion as well as any transforms assosciated with the model.</li> <li><code>requirements.txt</code> contains the dependencies to run our app such as <code>torch</code>, <code>torchvision</code> and <code>gradio</code>.</li> </ul> <p>Why this way?</p> <p>Because it's one of the simplest layouts we could begin with.</p> <p>Our focus is: experiment, experiment, experiment!</p> <p>The quicker we can run smaller experiments, the better our bigger ones will be.</p> <p>We're going to work towards recreating the structure above but you can see a live demo app running on Hugging Face Spaces as well as the file structure:</p> <ul> <li>Live Gradio demo of FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63.</li> <li>FoodVision Mini file structure on Hugging Face Spaces.</li> </ul>"},{"location":"09_pytorch_model_deployment/#83-creating-a-demos-folder-to-store-our-foodvision-mini-app-files","title":"8.3 Creating a <code>demos</code> folder to store our FoodVision Mini app files\u00b6","text":"<p>To begin, let's first create a <code>demos/</code> directory to store all of our FoodVision Mini app files.</p> <p>We can do with Python's <code>pathlib.Path(\"path_to_dir\")</code> to establish the directory path and <code>pathlib.Path(\"path_to_dir\").mkdir()</code> to create it.</p>"},{"location":"09_pytorch_model_deployment/#84-creating-a-folder-of-example-images-to-use-with-our-foodvision-mini-demo","title":"8.4 Creating a folder of example images to use with our FoodVision Mini demo\u00b6","text":"<p>Now we've got a directory to store our FoodVision Mini demo files, let's add some examples to it.</p> <p>Three example images from the test dataset should be enough.</p> <p>To do so we'll:</p> <ol> <li>Create an <code>examples/</code> directory within the <code>demos/foodvision_mini</code> directory.</li> <li>Choose three random images from the test dataset and collect their filepaths in a list.</li> <li>Copy the three random images from the test dataset to the <code>demos/foodvision_mini/examples/</code> directory.</li> </ol>"},{"location":"09_pytorch_model_deployment/#85-moving-our-trained-effnetb2-model-to-our-foodvision-mini-demo-directory","title":"8.5 Moving our trained EffNetB2 model to our FoodVision Mini demo directory\u00b6","text":"<p>We previously saved our FoodVision Mini EffNetB2 feature extractor model under <code>models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code>.</p> <p>And rather double up on saved model files, let's move our model to our <code>demos/foodvision_mini</code> directory.</p> <p>We can do so using Python's <code>shutil.move()</code> method and passing in <code>src</code> (the source path of the target file) and <code>dst</code> (the destination path of the target file to be moved to) parameters.</p>"},{"location":"09_pytorch_model_deployment/#86-turning-our-effnetb2-model-into-a-python-script-modelpy","title":"8.6 Turning our EffNetB2 model into a Python script (<code>model.py</code>)\u00b6","text":"<p>Our current model's <code>state_dict</code> is saved to <code>demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code>.</p> <p>To load it in we can use <code>model.load_state_dict()</code> along with <code>torch.load()</code>.</p> <p>Note: For a refresh on saving and loading a model (or a model's <code>state_dict</code> in PyTorch, see 01. PyTorch Workflow Fundamentals section 5: Saving and loading a PyTorch model or see the PyTorch recipe for What is a <code>state_dict</code> in PyTorch?</p> <p>But before we can do this, we first need a way to instantiate a <code>model</code>.</p> <p>To do this in a modular fashion we'll create a script called <code>model.py</code> which contains our <code>create_effnetb2_model()</code> function we created in section 3.1: Creating a function to make an EffNetB2 feature extractor.</p> <p>That way we can import the function in another script (see <code>app.py</code> below) and then use it to create our EffNetB2 <code>model</code> instance as well as get its appropriate transforms.</p> <p>Just like in 05. PyTorch Going Modular, we'll use the <code>%%writefile path/to/file</code> magic command to turn a cell of code into a file.</p>"},{"location":"09_pytorch_model_deployment/#87-turning-our-foodvision-mini-gradio-app-into-a-python-script-apppy","title":"8.7 Turning our FoodVision Mini Gradio app into a Python script (<code>app.py</code>)\u00b6","text":"<p>We've now got a <code>model.py</code> script as well as a path to a saved model <code>state_dict</code> that we can load in.</p> <p>Time to construct <code>app.py</code>.</p> <p>We call it <code>app.py</code> because by default when you create a HuggingFace Space, it looks for a file called <code>app.py</code> to run and host (though you can change this in settings).</p> <p>Our <code>app.py</code> script will put together all of the pieces of the puzzle to create our Gradio demo and will have four main parts:</p> <ol> <li>Imports and class names setup - Here we'll import the various dependencies for our demo including the <code>create_effnetb2_model()</code> function from <code>model.py</code> as well as setup the different class names for our FoodVision Mini app.</li> <li>Model and transforms preparation - Here we'll create an EffNetB2 model instance along with the transforms to go with it and then we'll load in the saved model weights/<code>state_dict</code>. When we load the model we'll also set <code>map_location=torch.device(\"cpu\")</code> in <code>torch.load()</code> so our model gets loaded onto the CPU regardless of the device it trained on (we do this because we won't necessarily have a GPU when we deploy and we'll get an error if our model is trained on GPU but we try to deploy it to CPU without explicitly saying so).</li> <li>Predict function - Gradio's <code>gradio.Interface()</code> takes a <code>fn</code> parameter to map inputs to outputs, our <code>predict()</code> function will be the same as the one we defined above in section 7.2: Creating a function to map our inputs and outputs, it will take in an image and then use the loaded transforms to preprocess it before using the loaded model to make a prediction on it.<ul> <li>Note: We'll have to create the example list on the fly via the <code>examples</code> parameter. We can do so by creating a list of the files inside the <code>examples/</code> directory with: <code>[[\"examples/\" + example] for example in os.listdir(\"examples\")]</code>.</li> </ul> </li> <li>Gradio app - This is where the main logic of our demo will live, we'll create a <code>gradio.Interface()</code> instance called <code>demo</code> to put together our inputs, <code>predict()</code> function and outputs. And we'll finish the script by calling <code>demo.launch()</code> to launch our FoodVision Mini demo!</li> </ol>"},{"location":"09_pytorch_model_deployment/#88-creating-a-requirements-file-for-foodvision-mini-requirementstxt","title":"8.8 Creating a requirements file for FoodVision Mini (<code>requirements.txt</code>)\u00b6","text":"<p>The last file we need to create for our FoodVision Mini app is a <code>requirements.txt</code> file.</p> <p>This will be a text file containing all of the required dependencies for our demo.</p> <p>When we deploy our demo app to Hugging Face Spaces, it will search through this file and install the dependencies we define so our app can run.</p> <p>The good news is, there's only three!</p> <ol> <li><code>torch==1.12.0</code></li> <li><code>torchvision==0.13.0</code></li> <li><code>gradio==3.1.4</code></li> </ol> <p>The \"<code>==1.12.0</code>\" states the version number to install.</p> <p>Defining the version number is not 100% required but we will for now so if any breaking updates occur in future releases, our app still runs (PS if you find any errors, feel free to post on the course GitHub Issues).</p>"},{"location":"09_pytorch_model_deployment/#9-deploying-our-foodvision-mini-app-to-huggingface-spaces","title":"9. Deploying our FoodVision Mini app to HuggingFace Spaces\u00b6","text":"<p>We've got a file containing our FoodVision Mini demo, now how do we get it to run on Hugging Face Spaces?</p> <p>There are two main options for uploading to a Hugging Face Space (also called a Hugging Face Repository, similar to a git repository):</p> <ol> <li>Uploading via the Hugging Face Web interface (easiest).</li> <li>Uploading via the command line or terminal.<ul> <li>Bonus: You can also use the <code>huggingface_hub</code> library to interact with Hugging Face, this would be a good extension to the above two options.</li> </ul> </li> </ol> <p>Feel free to read the documentation on both options but we're going to go with option two.</p> <p>Note: To host anything on Hugging Face, you will to sign up for a free Hugging Face account.</p>"},{"location":"09_pytorch_model_deployment/#91-downloading-our-foodvision-mini-app-files","title":"9.1 Downloading our FoodVision Mini app files\u00b6","text":"<p>Let's check out the demo files we've got inside <code>demos/foodvision_mini</code>.</p> <p>To do so we can use the <code>!ls</code> command followed by the target filepath.</p> <p><code>ls</code> stands for \"list\" and the <code>!</code> means we want to execute the command at the shell level.</p>"},{"location":"09_pytorch_model_deployment/#92-running-our-foodvision-mini-demo-locally","title":"9.2 Running our FoodVision Mini demo locally\u00b6","text":"<p>If you download the <code>foodvision_mini.zip</code> file, you can test it locally by:</p> <ol> <li>Unzipping the file.</li> <li>Opening terminal or a command line prompt.</li> <li>Changing into the <code>foodvision_mini</code> directory (<code>cd foodvision_mini</code>).</li> <li>Creating an environment (<code>python3 -m venv env</code>).</li> <li>Activating the environment (<code>source env/bin/activate</code>).</li> <li>Installing the requirements (<code>pip install -r requirements.txt</code>, the \"<code>-r</code>\" is for recursive).<ul> <li>Note: This step may take 5-10 minutes depending on your internet connection. And if you're facing errors, you may need to upgrade <code>pip</code> first: <code>pip install --upgrade pip</code>.</li> </ul> </li> <li>Run the app (<code>python3 app.py</code>).</li> </ol> <p>This should result in a Gradio demo just like the one we built above running locally on your machine at a URL such as <code>http://127.0.0.1:7860/</code>.</p> <p>Note: If you run the app locally and you notice a <code>flagged/</code> directory appear, it contains samples that have been \"flagged\".</p> <p>For example, if someone tries the demo and the model produces an incorrect result, the sample can be \"flagged\" and reviewed for later.</p> <p>For more on flagging in Gradio, see the flagging documentation.</p>"},{"location":"09_pytorch_model_deployment/#93-uploading-to-hugging-face","title":"9.3 Uploading to Hugging Face\u00b6","text":"<p>We've verfied our FoodVision Mini app works locally, however, the fun of creating a machine learning demo is to show it to other people and allow them to use it.</p> <p>To do so, we're going to upload our FoodVision Mini demo to Hugging Face.</p> <p>Note: The following series of steps uses a Git (a file tracking system) workflow. For more on how Git works, I'd recommend going through the Git and GitHub for Beginners tutorial on freeCodeCamp.</p> <ol> <li>Sign up for a Hugging Face account.</li> <li>Start a new Hugging Face Space by going to your profile and then clicking \"New Space\".<ul> <li>Note: A Space in Hugging Face is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.</li> </ul> </li> <li>Give the Space a name, for example, mine is called <code>mrdbourke/foodvision_mini</code>, you can see it here: https://huggingface.co/spaces/mrdbourke/foodvision_mini</li> <li>Select a license (I used MIT).</li> <li>Select Gradio as the Space SDK (software development kit).<ul> <li>Note: You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.</li> </ul> </li> <li>Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).</li> <li>Click \"Create Space\".</li> <li>Clone the repo locally by running something like: <code>git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]</code> in terminal or command prompt.<ul> <li>Note: You can also add files via uploading them under the \"Files and versions\" tab.</li> </ul> </li> <li>Copy/move the contents of the downloaded <code>foodvision_mini</code> folder to the cloned repo folder.</li> <li>To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to install Git LFS (which stands for \"git large file storage\").</li> <li>After you've installed Git LFS, you can activate it by running <code>git lfs install</code>.</li> <li>In the <code>foodvision_mini</code> directory, track the files over 10MB with Git LFS with <code>git lfs track \"*.file_extension\"</code>.<ul> <li>Track EffNetB2 PyTorch model file with <code>git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"</code>.</li> </ul> </li> <li>Track <code>.gitattributes</code> (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example <code>.gitattributes</code> file on the FoodVision Mini Hugging Face Space.<ul> <li><code>git add .gitattributes</code></li> </ul> </li> <li>Add the rest of the <code>foodvision_mini</code> app files and commit them with:<ul> <li><code>git add *</code></li> <li><code>git commit -m \"first commit\"</code></li> </ul> </li> <li>Push (upload) the files to Hugging Face:<ul> <li><code>git push</code></li> </ul> </li> <li>Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!</li> </ol> <p>If everything worked, you should see a live running example of our FoodVision Mini Gradio demo like the one here: https://huggingface.co/spaces/mrdbourke/foodvision_mini</p> <p>And we can even embed our FoodVision Mini Gradio demo into our notebook as an iframe with <code>IPython.display.IFrame</code> and a link to our space in the format <code>https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+</code>.</p>"},{"location":"09_pytorch_model_deployment/#10-creating-foodvision-big","title":"10. Creating FoodVision Big\u00b6","text":"<p>We've spent the past few sections and chapters working on bringing FoodVision Mini to life.</p> <p>And now we've seen it working in a live demo, how about we step things up a notch?</p> <p>How?</p> <p>FoodVision Big!</p> <p>Since FoodVision Mini is trained on pizza, steak and sushi images from the Food101 dataset (101 classes of food x 1000 images each), how about we make FoodVision Big by training a model on all 101 classes!</p> <p>We'll go from three classes to 101!</p> <p>From pizza, steak, sushi to pizza, steak, sushi, hot dog, apple pie, carrot cake, chocolate cake, french fires, garlic bread, ramen, nachos, tacos and more!</p> <p>How?</p> <p>Well, we've got all the steps in place, all we have to do is alter our EffNetB2 model slightly as well as prepare a different dataset.</p> <p>To finish Milestone Project 3, let's recreate a Gradio demo similar to FoodVision Mini (three classes) but for FoodVision Big (101 classes).</p> <p>FoodVision Mini works with three food classes: pizza, steak and sushi. And FoodVision Big steps it up a notch to work across 101 food classes: all of the classes in the Food101 dataset.</p>"},{"location":"09_pytorch_model_deployment/#101-creating-a-model-and-transforms-for-foodvision-big","title":"10.1 Creating a model and transforms for FoodVision Big\u00b6","text":"<p>When creating FoodVision Mini we saw that the EffNetB2 model was a good tradeoff between speed and performance (it performed well with a fast speed).</p> <p>So we'll continue using the same model for FoodVision Big.</p> <p>We can create an EffNetB2 feature extractor for Food101 by using our <code>create_effnetb2_model()</code> function we created above, in section 3.1, and passing it the parameter <code>num_classes=101</code> (since Food101 has 101 classes).</p>"},{"location":"09_pytorch_model_deployment/#102-getting-data-for-foodvision-big","title":"10.2 Getting data for FoodVision Big\u00b6","text":"<p>For FoodVision Mini, we made our own custom data splits of the entire Food101 dataset.</p> <p>To get the whole Food101 dataset, we can use <code>torchvision.datasets.Food101()</code>.</p> <p>We'll first setup a path to directory <code>data/</code> to store the images.</p> <p>Then we'll download and transform the training and testing dataset splits using <code>food101_train_transforms</code> and <code>effnetb2_transforms</code> to transform each dataset respectively.</p> <p>Note: If you're using Google Colab, the cell below will take ~3-5 minutes to fully run and download the Food101 images from PyTorch.</p> <p>This is because there is over 100,000 images being downloaded (101 classes x 1000 images per class). If you restart your Google Colab runtime and come back to this cell, the images will have to redownload. Alternatively, if you're running this notebook locally, the images will be cached and stored in the directory specified by the <code>root</code> parameter of <code>torchvision.datasets.Food101()</code>.</p>"},{"location":"09_pytorch_model_deployment/#103-creating-a-subset-of-the-food101-dataset-for-faster-experimenting","title":"10.3 Creating a subset of the Food101 dataset for faster experimenting\u00b6","text":"<p>This is optional.</p> <p>We don't need to create another subset of the Food101 dataset, we could train and evaluate a model across the whole 101,000 images.</p> <p>But to keep training fast, let's create a 20% split of the training and test datasets.</p> <p>Our goal will be to see if we can beat the original Food101 paper's best results with only 20% of the data.</p> <p>To breakdown the datasets we've used/will use:</p> Notebook(s) Project name Dataset Number of classes Training images Testing images 04, 05, 06, 07, 08 FoodVision Mini (10% data) Food101 custom split 3 (pizza, steak, sushi) 225 75 07, 08, 09 FoodVision Mini (20% data) Food101 custom split 3 (pizza, steak, sushi) 450 150 09 (this one) FoodVision Big (20% data) Food101 custom split 101 (all Food101 classes) 15150 5050 Extension FoodVision Big Food101 all data 101 75750 25250 <p>Can you see the trend?</p> <p>Just like our model size slowly increased overtime, so has the size of the dataset we've been using for experiments.</p> <p>Note: To truly beat the original Food101 paper's results with 20% of the data, we'd have to train a model on 20% of the training data and then evaluate our model on the whole test set rather than the split we created. I'll leave this as an extension exercise for you to try. I'd also encourage you to try training a model on the entire Food101 training dataset.</p> <p>To make our FoodVision Big (20% data) split, let's create a function called <code>split_dataset()</code> to split a given dataset into certain proportions.</p> <p>We can use <code>torch.utils.data.random_split()</code> to create splits of given sizes using the <code>lengths</code> parameter.</p> <p>The <code>lengths</code> parameter accepts a list of desired split lengths where the total of the list must equal the overall length of the dataset.</p> <p>For example, with a dataset of size 100, you could pass in <code>lengths=[20, 80]</code> to receive a 20% and 80% split.</p> <p>We'll want our function to return two splits, one with the target length (e.g. 20% of the training data) and the other with the remaining length (e.g. the remaining 80% of the training data).</p> <p>Finally, we'll set <code>generator</code> parameter to a <code>torch.manual_seed()</code> value for reproducibility.</p>"},{"location":"09_pytorch_model_deployment/#104-turning-our-food101-datasets-into-dataloaders","title":"10.4 Turning our Food101 datasets into <code>DataLoader</code>s\u00b6","text":"<p>Now let's turn our Food101 20% dataset splits into <code>DataLoader</code>'s using <code>torch.utils.data.DataLoader()</code>.</p> <p>We'll set <code>shuffle=True</code> for the training data only and the batch size to <code>32</code> for both datasets.</p> <p>And we'll set <code>num_workers</code> to <code>4</code> if the CPU count is available or <code>2</code> if it's not (though the value of <code>num_workers</code> is very experimental and will depend on the hardware you're using, there's an active discussion thread about this on the PyTorch forums).</p>"},{"location":"09_pytorch_model_deployment/#105-training-foodvision-big-model","title":"10.5 Training FoodVision Big model\u00b6","text":"<p>FoodVision Big model and <code>DataLoader</code>s ready!</p> <p>Time for training.</p> <p>We'll create an optimizer using <code>torch.optim.Adam()</code> and a learning rate of <code>1e-3</code>.</p> <p>And because we've got so many classes, we'll also setup a loss function using <code>torch.nn.CrossEntropyLoss()</code> with <code>label_smoothing=0.1</code>, inline with <code>torchvision</code>'s state-of-the-art training recipe.</p> <p>What's label smoothing?</p> <p>Label smoothing is a regularization technique (regularization is another word to describe the process of preventing overfitting) that reduces the value a model gives to anyone label and spreads it across the other labels.</p> <p>In essence, rather than a model getting too confident on a single label, label smoothing gives a non-zero value to other labels to help aid in generalization.</p> <p>For example, if a model without label smoothing had the following outputs for 5 classes:</p> <pre><code>[0, 0, 0.99, 0.01, 0]\n</code></pre> <p>A model with label smoothing may have the following outputs:</p> <pre><code>[0.01, 0.01, 0.96, 0.01, 0.01]\n</code></pre> <p>The model is still confident on its prediction of class 3 but giving small values to the other labels forces the model to at least consider other options.</p> <p>Finally, to keep things quick, we'll train our model for five epochs using the <code>engine.train()</code> function we created in 05. PyTorch Going Modular section 4 with the goal of beating the original Food101 paper's result of 56.4% accuracy on the test set.</p> <p>Let's train our biggest model yet!</p> <p>Note: Running the cell below will take ~15-20 minutes to run on Google Colab. This is because it's training the biggest model with the largest amount of data we've used so far (15,150 training images, 5050 testing images). And it's a reason we decided to split 20% of the full Food101 dataset off before (so training didn't take over an hour).</p>"},{"location":"09_pytorch_model_deployment/#106-inspecting-loss-curves-of-foodvision-big-model","title":"10.6 Inspecting loss curves of FoodVision Big model\u00b6","text":"<p>Let's make our FoodVision Big loss curves visual.</p> <p>We can do so with the <code>plot_loss_curves()</code> function from <code>helper_functions.py</code>.</p>"},{"location":"09_pytorch_model_deployment/#107-saving-and-loading-foodvision-big","title":"10.7 Saving and loading FoodVision Big\u00b6","text":"<p>Now we've trained our biggest model yet, let's save it so we can load it back in later.</p>"},{"location":"09_pytorch_model_deployment/#108-checking-foodvision-big-model-size","title":"10.8 Checking FoodVision Big model size\u00b6","text":"<p>Our FoodVision Big model is capable of classifying 101 classes versus FoodVision Mini's 3 classes, a 33.6x increase!</p> <p>How does this effect the model size?</p> <p>Let's find out.</p>"},{"location":"09_pytorch_model_deployment/#11-turning-our-foodvision-big-model-into-a-deployable-app","title":"11. Turning our FoodVision Big model into a deployable app\u00b6","text":"<p>We've got a trained and saved EffNetB2 model on 20% of the Food101 dataset.</p> <p>And instead of letting our model live in a folder all its life, let's deploy it!</p> <p>We'll deploy our FoodVision Big model in the same way we deployed our FoodVision Mini model, as a Gradio demo on Hugging Face Spaces.</p> <p>To begin, let's create a <code>demos/foodvision_big/</code> directory to store our FoodVision Big demo files as well as a <code>demos/foodvision_big/examples</code> directory to hold an example image to test the demo with.</p> <p>When we're finished we'll have the following file structure:</p> <pre><code>demos/\n  foodvision_big/\n    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n    app.py\n    class_names.txt\n    examples/\n      example_1.jpg\n    model.py\n    requirements.txt\n</code></pre> <p>Where:</p> <ul> <li><code>09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth</code> is our trained PyTorch model file.</li> <li><code>app.py</code> contains our FoodVision Big Gradio app.</li> <li><code>class_names.txt</code> contains all of the class names for FoodVision Big.</li> <li><code>examples/</code> contains example images to use with our Gradio app.</li> <li><code>model.py</code> contains the model defintion as well as any transforms assosciated with the model.</li> <li><code>requirements.txt</code> contains the dependencies to run our app such as <code>torch</code>, <code>torchvision</code> and <code>gradio</code>.</li> </ul>"},{"location":"09_pytorch_model_deployment/#111-downloading-an-example-image-and-moving-it-to-the-examples-directory","title":"11.1 Downloading an example image and moving it to the <code>examples</code> directory\u00b6","text":"<p>For our example image, we're going to use the faithful <code>pizza-dad</code> image (a photo of my dad eating pizza).</p> <p>So let's download it from the course GitHub via the <code>!wget</code> command and then we can move it to <code>demos/foodvision_big/examples</code> with the <code>!mv</code> command (short for \"move\").</p> <p>While we're here we'll move our trained Food101 EffNetB2 model from <code>models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth</code> to <code>demos/foodvision_big</code> as well.</p>"},{"location":"09_pytorch_model_deployment/#112-saving-food101-class-names-to-file-class_namestxt","title":"11.2 Saving Food101 class names to file (<code>class_names.txt</code>)\u00b6","text":"<p>Because there are so many classes in the Food101 dataset, instead of storing them as a list in our <code>app.py</code> file, let's saved them to a <code>.txt</code> file and read them in when necessary instead.</p> <p>We'll just remind ourselves what they look like first by checking out <code>food101_class_names</code>.</p>"},{"location":"09_pytorch_model_deployment/#113-turning-our-foodvision-big-model-into-a-python-script-modelpy","title":"11.3 Turning our FoodVision Big model into a Python script (<code>model.py</code>)\u00b6","text":"<p>Just like the FoodVision Mini demo, let's create a script that's capable of instantiating an EffNetB2 feature extractor model along with its necessary transforms.</p>"},{"location":"09_pytorch_model_deployment/#114-turning-our-foodvision-big-gradio-app-into-a-python-script-apppy","title":"11.4 Turning our FoodVision Big Gradio app into a Python script (<code>app.py</code>)\u00b6","text":"<p>We've got a FoodVision Big <code>model.py</code> script, now let's create a FoodVision Big <code>app.py</code> script.</p> <p>This will again mostly be the same as the FoodVision Mini <code>app.py</code> script except we'll change:</p> <ol> <li>Imports and class names setup - The <code>class_names</code> variable will be a list for all of the Food101 classes rather than pizza, steak, sushi. We can access these via <code>demos/foodvision_big/class_names.txt</code>.</li> <li>Model and transforms preparation - The <code>model</code> will have <code>num_classes=101</code> rather than <code>num_classes=3</code>. We'll also be sure to load the weights from <code>\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"</code> (our FoodVision Big model path).</li> <li>Predict function - This will stay the same as FoodVision Mini's <code>app.py</code>.</li> <li>Gradio app - The Gradio interace will have different <code>title</code>, <code>description</code> and <code>article</code> parameters to reflect the details of FoodVision Big.</li> </ol> <p>We'll also make sure to save it to <code>demos/foodvision_big/app.py</code> using the <code>%%writefile</code> magic command.</p>"},{"location":"09_pytorch_model_deployment/#115-creating-a-requirements-file-for-foodvision-big-requirementstxt","title":"11.5 Creating a requirements file for FoodVision Big (<code>requirements.txt</code>)\u00b6","text":"<p>Now all we need is a <code>requirements.txt</code> file to tell our Hugging Face Space what dependencies our FoodVision Big app requires.</p>"},{"location":"09_pytorch_model_deployment/#116-downloading-our-foodvision-big-app-files","title":"11.6 Downloading our FoodVision Big app files\u00b6","text":"<p>We've got all the files we need to deploy our FoodVision Big app on Hugging Face, let's now zip them together and download them.</p> <p>We'll use the same process we used for the FoodVision Mini app above in section 9.1: Downloading our Foodvision Mini app files.</p>"},{"location":"09_pytorch_model_deployment/#117-deploying-our-foodvision-big-app-to-huggingface-spaces","title":"11.7 Deploying our FoodVision Big app to HuggingFace Spaces\u00b6","text":"<p>B, E, A, Utiful!</p> <p>Time to bring our biggest model of the whole course to life!</p> <p>Let's deploy our FoodVision Big Gradio demo to Hugging Face Spaces so we can test it interactively and let others experience the magic of our machine learning efforts!</p> <p>Note: There are several ways to upload files to Hugging Face Spaces. The following steps treat Hugging Face as a git repository to track files. However, you can also upload directly to Hugging Face Spaces via the web interface or by the <code>huggingface_hub</code> library.</p> <p>The good news is, we've already done the steps to do so with FoodVision Mini, so now all we have to do is customize them to suit FoodVision Big:</p> <ol> <li>Sign up for a Hugging Face account.</li> <li>Start a new Hugging Face Space by going to your profile and then clicking \"New Space\".<ul> <li>Note: A Space in Hugging Face is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.</li> </ul> </li> <li>Give the Space a name, for example, mine is called <code>mrdbourke/foodvision_big</code>, you can see it here: https://huggingface.co/spaces/mrdbourke/foodvision_big</li> <li>Select a license (I used MIT).</li> <li>Select Gradio as the Space SDK (software development kit).<ul> <li>Note: You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.</li> </ul> </li> <li>Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).</li> <li>Click \"Create Space\".</li> <li>Clone the repo locally by running: <code>git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]</code> in terminal or command prompt.<ul> <li>Note: You can also add files via uploading them under the \"Files and versions\" tab.</li> </ul> </li> <li>Copy/move the contents of the downloaded <code>foodvision_big</code> folder to the cloned repo folder.</li> <li>To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to install Git LFS (which stands for \"git large file storage\").</li> <li>After you've installed Git LFS, you can activate it by running <code>git lfs install</code>.</li> <li>In the <code>foodvision_big</code> directory, track the files over 10MB with Git LFS with <code>git lfs track \"*.file_extension\"</code>.<ul> <li>Track EffNetB2 PyTorch model file with <code>git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"</code>.</li> <li>Note: If you get any errors uploading images, you may have to track them with <code>git lfs</code> too, for example <code>git lfs track \"examples/04-pizza-dad.jpg\"</code></li> </ul> </li> <li>Track <code>.gitattributes</code> (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example <code>.gitattributes</code> file on the FoodVision Big Hugging Face Space.<ul> <li><code>git add .gitattributes</code></li> </ul> </li> <li>Add the rest of the <code>foodvision_big</code> app files and commit them with:<ul> <li><code>git add *</code></li> <li><code>git commit -m \"first commit\"</code></li> </ul> </li> <li>Push (upload) the files to Hugging Face:<ul> <li><code>git push</code></li> </ul> </li> <li>Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!</li> </ol> <p>If everything worked correctly, our FoodVision Big Gradio demo should be ready to classify!</p> <p>You can see my version here: https://huggingface.co/spaces/mrdbourke/foodvision_big/</p> <p>Or we can even embed our FoodVision Big Gradio demo right within our notebook as an iframe with <code>IPython.display.IFrame</code> and a link to our space in the format <code>https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+</code>.</p>"},{"location":"09_pytorch_model_deployment/#main-takeaways","title":"Main takeaways\u00b6","text":"<ul> <li>Deployment is as important as training. Once you\u2019ve got a good working model, your first question should be: how can I deploy this and make it accessible to others? Deployment allows you to test your model in the real world rather than on private training and test sets.</li> <li>Three questions for machine learning model deployment:<ol> <li>What\u2019s the most ideal use case for the model (how well and how fast does it perform)?</li> <li>Where\u2019s the model going to go (is it on-device or on the cloud)?</li> <li>How\u2019s the model going to function (are predictions online or offline)?</li> </ol> </li> <li>Deployment options are a plenty. But best to start simple. One of the best current ways (I say current because these things are always changing) is to use Gradio to create a demo and host it on Hugging Face Spaces. Start simple and scale up when needed.</li> <li>Never stop experimenting. Your machine learning model needs will likely change overtime so deploying a single model is not the last step. You might find the dataset changes, so you\u2019ll have to update your model. Or new research gets released and there\u2019s a better architecture to use.<ul> <li>So deploying one model is an excellent step, but you'll likely want to update it over time.</li> </ul> </li> <li>Machine learning model deployment is part of the engineering practice of MLOps (machine learning operations). MLOps is an extension of DevOps (development operations) and involves all the engineering parts around training a model: data collection and storage, data preprocessing, model deployment, model monitoring, versioning and more. It\u2019s a rapidly evolving field but there are some solid resources out there to learn more, many of which are in PyTorch Extra Resources.</li> </ul>"},{"location":"09_pytorch_model_deployment/#exercises","title":"Exercises\u00b6","text":"<p>All of the exercises are focused on practicing the code above.</p> <p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p> <p>Resources:</p> <ul> <li>Exercise template notebook for 09.</li> <li>Example solutions notebook for 09 try the exercises before looking at this.<ul> <li>See a live video walkthrough of the solutions on YouTube (errors and all).</li> </ul> </li> </ul> <ol> <li>Make and time predictions with both feature extractor models on the test dataset using the GPU (<code>device=\"cuda\"</code>). Compare the model's prediction times on GPU vs CPU - does this close the gap between them? As in, does making predictions on the GPU make the ViT feature extractor prediction times closer to the EffNetB2 feature extractor prediction times?<ul> <li>You'll find code to do these steps in section 5. Making predictions with our trained models and timing them and section 6. Comparing model results, prediction times and size.</li> </ul> </li> <li>The ViT feature extractor seems to have more learning capacity (due to more parameters) than EffNetB2, how does it go on the larger 20% split of the entire Food101 dataset?<ul> <li>Train a ViT feature extractor on the 20% Food101 dataset for 5 epochs, just like we did with EffNetB2 in section 10. Creating FoodVision Big.</li> </ul> </li> <li>Make predictions across the 20% Food101 test dataset with the ViT feature extractor from exercise 2 and find the \"most wrong\" predictions.<ul> <li>The predictions will be the ones with the highest prediction probability but with the wrong predicted label.</li> <li>Write a sentence or two about why you think the model got these predictions wrong.</li> </ul> </li> <li>Evaluate the ViT feature extractor across the whole Food101 test dataset rather than just the 20% version, how does it perform?<ul> <li>Does it beat the original Food101 paper's best result of 56.4% accuracy?</li> </ul> </li> <li>Head to Paperswithcode.com and find the current best performing model on the Food101 dataset.<ul> <li>What model architecture does it use?</li> </ul> </li> <li>Write down 1-3 potential failure points of our deployed FoodVision models and what some potential solutions might be.<ul> <li>For example, what happens if someone was to upload a photo that wasn't of food to our FoodVision Mini model?</li> </ul> </li> <li>Pick any dataset from <code>torchvision.datasets</code> and train a feature extractor model on it using a model from <code>torchvision.models</code> (you could use one of the model's we've already created, e.g. EffNetB2 or ViT) for 5 epochs and then deploy your model as a Gradio app to Hugging Face Spaces.<ul> <li>You may want to pick smaller dataset/make a smaller split of it so training doesn't take too long.</li> <li>I'd love to see your deployed models! So be sure to share them in Discord or on the course GitHub Discussions page.</li> </ul> </li> </ol>"},{"location":"09_pytorch_model_deployment/#extra-curriculum","title":"Extra-curriculum\u00b6","text":"<ul> <li>Machine learning model deployment is generally an engineering challenge rather than a pure machine learning challenge, see the PyTorch Extra Resources machine learning engineering section for resources on learning more.<ul> <li>Inside you'll find recommendations for resources such as Chip Huyen's book Designing Machine Learning Systems (especially chapter 7 on model deployment) and Goku Mohandas's Made with ML MLOps course.</li> </ul> </li> <li>As you start to build more and more of your own projects, you'll likely start using Git (and potentially GitHub) quite frequently. To learn more about both, I'd recommend the Git and GitHub for Beginners - Crash Course video on the freeCodeCamp YouTube channel.</li> <li>We've only scratched the surface with what's possible with Gradio. For more, I'd recommend checking out the full documentation, especially:<ul> <li>All of the different kinds of input and output components.</li> <li>The Gradio Blocks API for more advanced workflows.</li> <li>The Hugging Face Course chapter on how to use Gradio with Hugging Face.</li> </ul> </li> <li>Edge devices aren't limited to mobile phones, they include small computers like the Raspberry Pi and the PyTorch team have a fantastic blog post tutorial on deploying a PyTorch model to one.</li> <li>For a fanstastic guide on developing AI and ML-powered applications, see Google's People + AI Guidebook. One of my favourites is the section on setting the right expectations.<ul> <li>I covered more of these kinds of resources, including guides from Apple, Microsoft and more in the April 2021 edition of Machine Learning Monthly (a monthly newsletter I send out with the latest and greatest of the ML field).</li> </ul> </li> <li>If you'd like to speed up your model's runtime on CPU, you should be aware of TorchScript, ONNX (Open Neural Network Exchange) and OpenVINO. Going from pure PyTorch to ONNX/OpenVINO models I've seen a ~2x+ increase in performance.</li> <li>For turning models into a deployable and scalable API, see the TorchServe library.</li> <li>For a terrific example and rationale as to why deploying a machine learning model in the browser (a form of edge deployment) offers several benefits (no network transfer latency delay), see Jo Kristian Bergum's article on Moving ML Inference from the Cloud to the Edge.</li> </ul>"},{"location":"10_pytorch_profiling/","title":"08: PyTorch Profiling","text":"In\u00a0[10]: Copied! <pre>import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom going_modular import data_setup, engine\n</pre> import torch import torchvision from torch import nn from torchvision import transforms, datasets from torchinfo import summary  import numpy as np import matplotlib.pyplot as plt  from going_modular import data_setup, engine In\u00a0[11]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[11]: <pre>'cuda'</pre> In\u00a0[12]: Copied! <pre>import os\nimport requests\nfrom zipfile import ZipFile\n\ndef get_food_image_data():\n    if not os.path.exists(\"data/10_whole_foods\"):\n        os.makedirs(\"data/\", exist_ok=True)\n        # Download data\n        data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"\n        print(f\"Downloading data from {data_url}...\")\n        requests.get(data_url)\n        # Unzip data\n        targ_dir = \"data/10_whole_foods\"\n        print(f\"Extracting data to {targ_dir}...\")\n        with ZipFile(\"10_whole_foods.zip\") as zip_ref:\n            zip_ref.extractall(targ_dir)\n    else:\n        print(\"data/10_whole_foods dir exists, skipping download\")\n\nget_food_image_data()\n</pre> import os import requests from zipfile import ZipFile  def get_food_image_data():     if not os.path.exists(\"data/10_whole_foods\"):         os.makedirs(\"data/\", exist_ok=True)         # Download data         data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"         print(f\"Downloading data from {data_url}...\")         requests.get(data_url)         # Unzip data         targ_dir = \"data/10_whole_foods\"         print(f\"Extracting data to {targ_dir}...\")         with ZipFile(\"10_whole_foods.zip\") as zip_ref:             zip_ref.extractall(targ_dir)     else:         print(\"data/10_whole_foods dir exists, skipping download\")  get_food_image_data() <pre>data/10_whole_foods dir exists, skipping download\n</pre> In\u00a0[38]: Copied! <pre># Setup dirs\ntrain_dir = \"data/10_whole_foods/train\"\ntest_dir = \"data/10_whole_foods/test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create starter transform\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=32,\n    num_workers=8\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = \"data/10_whole_foods/train\" test_dir = \"data/10_whole_foods/test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create starter transform simple_transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])             # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=simple_transform,     batch_size=32,     num_workers=8 )  train_dataloader, test_dataloader, class_names Out[38]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f052ab26e20&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f05299eed00&gt;,\n ['apple',\n  'banana',\n  'beef',\n  'blueberries',\n  'carrots',\n  'chicken_wings',\n  'egg',\n  'honey',\n  'mushrooms',\n  'strawberries'])</pre> In\u00a0[66]: Copied! <pre>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n# model\n</pre> model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # model In\u00a0[67]: Copied! <pre># Update the classifier\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2),\n    nn.Linear(1280, len(class_names)).to(device))\n\n# Freeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Update the classifier model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2),     nn.Linear(1280, len(class_names)).to(device))  # Freeze all base layers  for param in model.features.parameters():     param.requires_grad = False In\u00a0[68]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[69]: Copied! <pre>model.name = \"EfficietNetB0\"\nmodel.name\n</pre> model.name = \"EfficietNetB0\" model.name Out[69]: <pre>'EfficietNetB0'</pre> In\u00a0[70]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\nfrom going_modular.engine import train_step, test_step\nfrom tqdm import tqdm\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter from going_modular.engine import train_step, test_step from tqdm import tqdm writer = SummaryWriter() <p>Update the <code>train_step()</code> function to include the PyTorch profiler.</p> In\u00a0[71]: Copied! <pre>def train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    train_loss, train_acc = 0, 0\n    ## NEW: Add PyTorch profiler\n\n    dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))\n    with torch.profiler.profile(\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),\n        # with_stack=True # this adds a lot of overhead to training (tracing all the stack)\n    ):\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to GPU\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            \n            # Turn on mixed precision if available\n            with torch.autocast(device_type=device, enabled=True):\n                # 1. Forward pass\n                y_pred = model(X)\n\n                # 2. Calculate loss\n                loss = loss_fn(y_pred, y)\n\n            # 3. Optimizer zero grad\n            optimizer.zero_grad()\n\n            # 4. Loss backward\n            loss.backward()\n\n            # 5. Optimizer step\n            optimizer.step()\n\n            # 6. Calculate metrics\n            train_loss += loss.item()\n            y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)\n            # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")\n            # print(f\"y argmax: {y_pred.argmax(dim=1)}\")\n            # print(f\"Equal: {(y_pred_class == y)}\")\n            train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n            # print(f\"batch: {batch} train_acc: {train_acc}\")\n\n    # Adjust returned metrics\n    return train_loss / len(dataloader), train_acc / len(dataloader)\n</pre> def train_step(model, dataloader, loss_fn, optimizer):     model.train()     train_loss, train_acc = 0, 0     ## NEW: Add PyTorch profiler      dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))     with torch.profiler.profile(         on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),         # with_stack=True # this adds a lot of overhead to training (tracing all the stack)     ):         for batch, (X, y) in enumerate(dataloader):             # Send data to GPU             X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)                          # Turn on mixed precision if available             with torch.autocast(device_type=device, enabled=True):                 # 1. Forward pass                 y_pred = model(X)                  # 2. Calculate loss                 loss = loss_fn(y_pred, y)              # 3. Optimizer zero grad             optimizer.zero_grad()              # 4. Loss backward             loss.backward()              # 5. Optimizer step             optimizer.step()              # 6. Calculate metrics             train_loss += loss.item()             y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)             # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")             # print(f\"y argmax: {y_pred.argmax(dim=1)}\")             # print(f\"Equal: {(y_pred_class == y)}\")             train_acc += (y_pred_class == y).sum().item() / len(y_pred)             # print(f\"batch: {batch} train_acc: {train_acc}\")      # Adjust returned metrics     return train_loss / len(dataloader), train_acc / len(dataloader) <p>TK - Now to use the writer, we've got to adjust the <code>train()</code> function...</p> In\u00a0[72]: Copied! <pre>def train(\n    model,\n    train_dataloader,\n    test_dataloader,\n    optimizer,\n    loss_fn=nn.CrossEntropyLoss(),\n    epochs=5,\n):\n\n    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n        )\n        test_loss, test_acc = test_step(\n            model=model, dataloader=test_dataloader, loss_fn=loss_fn\n        )\n\n        # Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        # Add results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n    \n    # Close the writer\n    writer.close()\n\n    return results\n</pre> def train(     model,     train_dataloader,     test_dataloader,     optimizer,     loss_fn=nn.CrossEntropyLoss(),     epochs=5, ):      results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}      for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(             model=model,             dataloader=train_dataloader,             loss_fn=loss_fn,             optimizer=optimizer,         )         test_loss, test_acc = test_step(             model=model, dataloader=test_dataloader, loss_fn=loss_fn         )          # Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # Update results         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          # Add results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)          # Close the writer     writer.close()      return results In\u00a0[73]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:05&lt;00:21,  5.27s/it]</pre> <pre>Epoch: 1 | train_loss: 1.9644 | train_acc: 0.4386 | test_loss: 1.5205 | test_acc: 0.7865\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:09&lt;00:14,  4.94s/it]</pre> <pre>Epoch: 2 | train_loss: 1.2589 | train_acc: 0.7878 | test_loss: 1.1589 | test_acc: 0.7604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:14&lt;00:09,  4.72s/it]</pre> <pre>Epoch: 3 | train_loss: 0.8642 | train_acc: 0.8776 | test_loss: 0.9347 | test_acc: 0.7917\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.6827 | train_acc: 0.8856 | test_loss: 0.6637 | test_acc: 0.8750\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:23&lt;00:00,  4.65s/it]</pre> <pre>Epoch: 5 | train_loss: 0.5688 | train_acc: 0.9069 | test_loss: 0.6175 | test_acc: 0.8854\n</pre> <pre>\n</pre> <p>Looks like mixed precision doesn't offer much benefit for smaller feature extraction models...</p> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750\n\n# # With mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906\n</pre> # # Without mixed precision #  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750  # # With mixed precision #  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906 In\u00a0[74]: Copied! <pre># Unfreeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = True\n\n# for param in model.features.parameters():\n#     print(param.requires_grad)\n</pre> # Unfreeze all base layers  for param in model.features.parameters():     param.requires_grad = True  # for param in model.features.parameters(): #     print(param.requires_grad) In\u00a0[75]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]</pre> <pre>Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]</pre> <pre>Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]</pre> <pre>Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]</pre> <pre>Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385\n\n# # With mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> # # Without mixed precision... #  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385  # # With mixed precision... #  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125 <p>Checking the PyTorch profiler, it seems that mixed precision utilises some Tensor Cores, however, these aren't large numbers.</p> <p>E.g. it uses 9-12% Tensor Cores. Perhaps the slow down when using mixed precision is because the tensors have to get altered and converted when there isn't very many of them. For example only 9-12% of tensors get converted so the speed up gains aren't realised on these tensors because they get cancelled out by the conversion time.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"10_pytorch_profiling/#08-pytorch-profiling","title":"08: PyTorch Profiling\u00b6","text":"<p>This notebook is an experiment to try out the PyTorch profiler.</p> <p>See here for more:</p> <ul> <li>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</li> <li>https://pytorch.org/docs/stable/profiler.html</li> </ul>"},{"location":"10_pytorch_profiling/#setup-device","title":"Setup device\u00b6","text":""},{"location":"10_pytorch_profiling/#get-and-load-data","title":"Get and load data\u00b6","text":""},{"location":"10_pytorch_profiling/#load-model","title":"Load model\u00b6","text":""},{"location":"10_pytorch_profiling/#train-model-and-track-results","title":"Train model and track results\u00b6","text":""},{"location":"10_pytorch_profiling/#adjust-training-function-to-track-results-with-summarywriter","title":"Adjust training function to track results with <code>SummaryWriter</code>\u00b6","text":""},{"location":"10_pytorch_profiling/#try-mixed-precision-with-larger-model","title":"Try mixed precision with larger model\u00b6","text":"<p>Now we'll try turn on mixed precision with a larger model (e.g. EffifientNetB0 with all layers tuneable).</p>"},{"location":"10_pytorch_profiling/#extensions","title":"Extensions\u00b6","text":"<ul> <li>Does changing the data input size to EfficientNetB4 change its results? E.g. input image size of (380, 380) instead of (224, 224)?</li> </ul>"},{"location":"pytorch_2_intro/","title":"A Quick PyTorch 2.0 Tutorial","text":"<p>View Source Code</p> In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Notebook last updated: {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last updated: {datetime.datetime.now()}\") <pre>Notebook last updated: 2023-04-14 15:24:37.007274\n</pre> In\u00a0[2]: Copied! <pre>import torch\nimport torchvision\n\nmodel = torchvision.models.resnet50() # note: this could be any model\n\n### Train model ###\n\n### Test model ###\n</pre> import torch import torchvision  model = torchvision.models.resnet50() # note: this could be any model  ### Train model ###  ### Test model ### In\u00a0[3]: Copied! <pre>import torch\nimport torchvision\n\nmodel = torchvision.models.resnet50() # note: this could be any model\ncompiled_model = torch.compile(model) # &lt;- magic happens!\n\n### Train model ### &lt;- faster!\n\n### Test model ### &lt;- faster!\n</pre> import torch import torchvision  model = torchvision.models.resnet50() # note: this could be any model compiled_model = torch.compile(model) # &lt;- magic happens!  ### Train model ### &lt;- faster!  ### Test model ### &lt;- faster! In\u00a0[4]: Copied! <pre>import torch\n\n# Check PyTorch version\npt_version = torch.__version__\nprint(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n\n# Install PyTorch 2.0 if necessary\nif pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1 \n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n    import torch\n    pt_version = torch.__version__\n    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\nelse:\n    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")\n</pre> import torch  # Check PyTorch version pt_version = torch.__version__ print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")  # Install PyTorch 2.0 if necessary if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1      !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118     print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\           Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")     import torch     pt_version = torch.__version__     print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\") else:     print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\") <pre>[INFO] Current PyTorch version: 2.0.0+cu118 (should be 2.x+)\n[INFO] PyTorch 2.x installed, you'll be able to use the new features.\n</pre> <p>Wonderful!</p> <p>Now PyTorch 2.x is installed, let's try out the new features!</p> In\u00a0[5]: Copied! <pre># Make sure we're using a NVIDIA GPU\nif torch.cuda.is_available():\n  gpu_info = !nvidia-smi\n  gpu_info = '\\n'.join(gpu_info)\n  if gpu_info.find(\"failed\") &gt;= 0:\n    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n\n  # Get GPU name\n  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n  gpu_name = gpu_name[1]\n  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n  print(f'GPU name: {GPU_NAME}')\n\n  # Get GPU capability score\n  GPU_SCORE = torch.cuda.get_device_capability()\n  print(f\"GPU capability score: {GPU_SCORE}\")\n  if GPU_SCORE &gt;= (8, 0):\n    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n  else:\n    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n  \n  # Print GPU info\n  print(f\"GPU information:\\n{gpu_info}\")\n\nelse:\n  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n</pre> # Make sure we're using a NVIDIA GPU if torch.cuda.is_available():   gpu_info = !nvidia-smi   gpu_info = '\\n'.join(gpu_info)   if gpu_info.find(\"failed\") &gt;= 0:     print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")    # Get GPU name   gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv   gpu_name = gpu_name[1]   GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving   print(f'GPU name: {GPU_NAME}')    # Get GPU capability score   GPU_SCORE = torch.cuda.get_device_capability()   print(f\"GPU capability score: {GPU_SCORE}\")   if GPU_SCORE &gt;= (8, 0):     print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")   else:     print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")      # Print GPU info   print(f\"GPU information:\\n{gpu_info}\")  else:   print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\") <pre>GPU name: NVIDIA_TITAN_RTX\nGPU capability score: (7, 5)\nGPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\nGPU information:\nFri Apr 14 15:24:38 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA TITAN RTX    Off  | 00000000:01:00.0 Off |                  N/A |\n| 40%   50C    P8     9W / 280W |    260MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1020      G   /usr/lib/xorg/Xorg                 53MiB |\n|    0   N/A  N/A   1415245      G   /usr/lib/xorg/Xorg                162MiB |\n|    0   N/A  N/A   1415374      G   /usr/bin/gnome-shell                8MiB |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[6]: Copied! <pre>import torch\n\n# Set the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Set the device with context manager (requires PyTorch 2.x+)\nwith torch.device(device):\n    # All tensors created in this block will be on device\n    layer = torch.nn.Linear(20, 30)\n    print(f\"Layer weights are on device: {layer.weight.device}\")\n    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")\n</pre> import torch  # Set the device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Set the device with context manager (requires PyTorch 2.x+) with torch.device(device):     # All tensors created in this block will be on device     layer = torch.nn.Linear(20, 30)     print(f\"Layer weights are on device: {layer.weight.device}\")     print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\") <pre>Layer weights are on device: cuda:0\nLayer creating data on device: cuda:0\n</pre> <p>Now how about setting the global device?</p> <p>This will mean that any tensors created without an explicit device will be created on the device you set by default.</p> In\u00a0[7]: Copied! <pre>import torch\n\n# Set the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Set the device globally\ntorch.set_default_device(device)\n\n# All tensors created will be on the global device by default\nlayer = torch.nn.Linear(20, 30)\nprint(f\"Layer weights are on device: {layer.weight.device}\")\nprint(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")\n</pre> import torch  # Set the device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Set the device globally torch.set_default_device(device)  # All tensors created will be on the global device by default layer = torch.nn.Linear(20, 30) print(f\"Layer weights are on device: {layer.weight.device}\") print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\") <pre>Layer weights are on device: cuda:0\nLayer creating data on device: cuda:0\n</pre> <p>And now back to CPU.</p> In\u00a0[8]: Copied! <pre>import torch \n\n# Set the device globally\ntorch.set_default_device(\"cpu\")\n\n# All tensors created will be on \"cpu\"\nlayer = torch.nn.Linear(20, 30)\nprint(f\"Layer weights are on device: {layer.weight.device}\")\nprint(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")\n</pre> import torch   # Set the device globally torch.set_default_device(\"cpu\")  # All tensors created will be on \"cpu\" layer = torch.nn.Linear(20, 30) print(f\"Layer weights are on device: {layer.weight.device}\") print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\") <pre>Layer weights are on device: cpu\nLayer creating data on device: cpu\n</pre> In\u00a0[9]: Copied! <pre>import torch\nimport torchvision\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"TorchVision version: {torchvision.__version__}\")\n\n# Set the target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Using device: {device}\")\n</pre> import torch import torchvision  print(f\"PyTorch version: {torch.__version__}\") print(f\"TorchVision version: {torchvision.__version__}\")  # Set the target device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  print(f\"Using device: {device}\") <pre>PyTorch version: 2.0.0+cu118\nTorchVision version: 0.15.1+cu118\nUsing device: cuda\n</pre> In\u00a0[10]: Copied! <pre># Create model weights and transforms\nmodel_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # &lt;- use the latest weights (could also use .DEFAULT)\ntransforms = model_weights.transforms()\n\n# Setup model\nmodel = torchvision.models.resnet50(weights=model_weights)\n\n# Count the number of parameters in the model \ntotal_params = sum(\n    param.numel() for param in model.parameters() # &lt;- all params\n\t# param.numel() for param in model.parameters() if param.requires_grad # &lt;- only trainable params\n)\n\nprint(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\nprint(f\"Model transforms:\\n{transforms}\")\n</pre> # Create model weights and transforms model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # &lt;- use the latest weights (could also use .DEFAULT) transforms = model_weights.transforms()  # Setup model model = torchvision.models.resnet50(weights=model_weights)  # Count the number of parameters in the model  total_params = sum(     param.numel() for param in model.parameters() # &lt;- all params \t# param.numel() for param in model.parameters() if param.requires_grad # &lt;- only trainable params )  print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\") print(f\"Model transforms:\\n{transforms}\") <pre>Total parameters of model: 25557032 (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\nModel transforms:\nImageClassification(\n    crop_size=[224]\n    resize_size=[232]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> <p>Now let's turn the above code into a function so we can replicate it later, we'll also adjust the last layer's (<code>model.fc</code>) output features to match the number of classes in CIFAR10 (10).</p> In\u00a0[11]: Copied! <pre>def create_model(num_classes=10):\n\"\"\"\n  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n  \"\"\"\n  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n  transforms = model_weights.transforms()\n  model = torchvision.models.resnet50(weights=model_weights)\n  \n  # Adjust the number of output features in model to match the number of classes in the dataset\n  model.fc = torch.nn.Linear(in_features=2048, \n                             out_features=num_classes)\n  return model, transforms\n\nmodel, transforms = create_model()\n</pre> def create_model(num_classes=10):   \"\"\"   Creates a ResNet50 model with the latest weights and transforms via torchvision.   \"\"\"   model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2   transforms = model_weights.transforms()   model = torchvision.models.resnet50(weights=model_weights)      # Adjust the number of output features in model to match the number of classes in the dataset   model.fc = torch.nn.Linear(in_features=2048,                               out_features=num_classes)   return model, transforms  model, transforms = create_model() In\u00a0[12]: Copied! <pre># Check available GPU memory and total GPU memory \ntotal_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\nprint(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\nprint(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n</pre> # Check available GPU memory and total GPU memory  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info() print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\") print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\") <pre>Total free GPU memory: 24.187 GB\nTotal GPU memory: 25.386 GB\n</pre> <p>Wonderful!</p> <p>The takeaways here are:</p> <ol> <li>The higher the memory available on your GPU, the bigger your batch size can be, the bigger your model can be, the bigger your data samples can be.</li> <li>For speedups, you should always be trying to use as much of the GPU(s) as possible.</li> </ol> <p>Let's write some code to use a larger batch size if more GPU memory is available.</p> <p>Note: The ideal batch size you use will depend on the specific GPU and dataset and model you're working with. The code below is specifically targeted for the A100 GPU available on Google Colab Pro. However, you may to adjust it for your own GPU. As if you set the batch size too high, you may run into CUDA out of memory errors.</p> <p>If the total memory on the GPU available is above 16GB, let's use a batch size of 128 and an image size of 224 (both of these values can be increased on GPUs with more memory).</p> <p>If the total memory on the GPU available is below 16GB, let's use a batch size of 32 and an image size of 64 (both of these values can be altered on GPUs with less memory).</p> In\u00a0[13]: Copied! <pre># Set batch size depending on amount of GPU memory\ntotal_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\nif total_free_gpu_memory_gb &gt;= 16:\n  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n  IMAGE_SIZE = 224\n  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\nelse:\n  BATCH_SIZE = 32\n  IMAGE_SIZE = 128\n  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n</pre> # Set batch size depending on amount of GPU memory total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3) if total_free_gpu_memory_gb &gt;= 16:   BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.   IMAGE_SIZE = 224   print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\") else:   BATCH_SIZE = 32   IMAGE_SIZE = 128   print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\") <pre>GPU memory available is 24.187 GB, using batch size of 128 and image size 224\n</pre> <p>Now let's adjust the <code>transforms</code> to use the respective <code>IMAGE_SIZE</code>.</p> In\u00a0[14]: Copied! <pre>transforms.crop_size = IMAGE_SIZE\ntransforms.resize_size = IMAGE_SIZE \nprint(f\"Updated data transforms:\\n{transforms}\")\n</pre> transforms.crop_size = IMAGE_SIZE transforms.resize_size = IMAGE_SIZE  print(f\"Updated data transforms:\\n{transforms}\") <pre>Updated data transforms:\nImageClassification(\n    crop_size=224\n    resize_size=224\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> In\u00a0[15]: Copied! <pre>if GPU_SCORE &gt;= (8, 0):\n  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n  torch.backends.cuda.matmul.allow_tf32 = True\nelse:\n  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score &gt;= (8, 0)\")\n  torch.backends.cuda.matmul.allow_tf32 = False\n</pre> if GPU_SCORE &gt;= (8, 0):   print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")   torch.backends.cuda.matmul.allow_tf32 = True else:   print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score &gt;= (8, 0)\")   torch.backends.cuda.matmul.allow_tf32 = False <pre>[INFO] Using GPU with score: (7, 5), TensorFloat32 (TF32) not available, to use it you need a GPU with score &gt;= (8, 0)\n</pre> In\u00a0[16]: Copied! <pre># Create train and test datasets\ntrain_dataset = torchvision.datasets.CIFAR10(root='.', \n                                             train=True, \n                                             download=True, \n                                             transform=transforms)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='.', \n                                            train=False, # want the test split\n                                            download=True, \n                                            transform=transforms)\n\n# Get the lengths of the datasets\ntrain_len = len(train_dataset)\ntest_len = len(test_dataset)\n\nprint(f\"[INFO] Train dataset length: {train_len}\")\nprint(f\"[INFO] Test dataset length: {test_len}\")\n</pre> # Create train and test datasets train_dataset = torchvision.datasets.CIFAR10(root='.',                                               train=True,                                               download=True,                                               transform=transforms)  test_dataset = torchvision.datasets.CIFAR10(root='.',                                              train=False, # want the test split                                             download=True,                                              transform=transforms)  # Get the lengths of the datasets train_len = len(train_dataset) test_len = len(test_dataset)  print(f\"[INFO] Train dataset length: {train_len}\") print(f\"[INFO] Test dataset length: {test_len}\") <pre>Files already downloaded and verified\nFiles already downloaded and verified\n[INFO] Train dataset length: 50000\n[INFO] Test dataset length: 10000\n</pre> In\u00a0[17]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Create DataLoaders\nimport os\nNUM_WORKERS = os.cpu_count() # &lt;- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              num_workers=NUM_WORKERS)\n\ntest_dataloader = DataLoader(dataset=test_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=False,\n                              num_workers=NUM_WORKERS)\n\n# Print details\nprint(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\nprint(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\")\nprint(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\")\n</pre> from torch.utils.data import DataLoader  # Create DataLoaders import os NUM_WORKERS = os.cpu_count() # &lt;- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)  train_dataloader = DataLoader(dataset=train_dataset,                               batch_size=BATCH_SIZE,                               shuffle=True,                               num_workers=NUM_WORKERS)  test_dataloader = DataLoader(dataset=test_dataset,                               batch_size=BATCH_SIZE,                               shuffle=False,                               num_workers=NUM_WORKERS)  # Print details print(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\") print(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\") print(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\") <pre>Train dataloader length: 391 batches of size 128\nTest dataloader length: 79 batches of size 128\nUsing number of workers: 16 (generally more workers means faster dataloading from CPU to GPU)\n</pre> In\u00a0[18]: Copied! <pre>import time\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(epoch: int,\n               model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device,\n               disable_progress_bar: bool = False) -&gt; Tuple[float, float]:\n\"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  progress_bar = tqdm(\n        enumerate(dataloader), \n        desc=f\"Training Epoch {epoch}\", \n        total=len(dataloader),\n        disable=disable_progress_bar\n    )\n\n  for batch, (X, y) in progress_bar:\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n      # Update progress bar\n      progress_bar.set_postfix(\n            {\n                \"train_loss\": train_loss / (batch + 1),\n                \"train_acc\": train_acc / (batch + 1),\n            }\n        )\n\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(epoch: int,\n              model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device,\n              disable_progress_bar: bool = False) -&gt; Tuple[float, float]:\n\"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Loop through data loader data batches\n  progress_bar = tqdm(\n      enumerate(dataloader), \n      desc=f\"Testing Epoch {epoch}\", \n      total=len(dataloader),\n      disable=disable_progress_bar\n  )\n\n  # Turn on inference context manager\n  with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case\n      # Loop through DataLoader batches\n      for batch, (X, y) in progress_bar:\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n          # Update progress bar\n          progress_bar.set_postfix(\n              {\n                  \"test_loss\": test_loss / (batch + 1),\n                  \"test_acc\": test_acc / (batch + 1),\n              }\n          )\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device,\n          disable_progress_bar: bool = False) -&gt; Dict[str, List]:\n\"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": [],\n      \"train_epoch_time\": [],\n      \"test_epoch_time\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs), disable=disable_progress_bar):\n\n      # Perform training step and time it\n      train_epoch_start_time = time.time()\n      train_loss, train_acc = train_step(epoch=epoch, \n                                        model=model,\n                                        dataloader=train_dataloader,\n                                        loss_fn=loss_fn,\n                                        optimizer=optimizer,\n                                        device=device,\n                                        disable_progress_bar=disable_progress_bar)\n      train_epoch_end_time = time.time()\n      train_epoch_time = train_epoch_end_time - train_epoch_start_time\n      \n      # Perform testing step and time it\n      test_epoch_start_time = time.time()\n      test_loss, test_acc = test_step(epoch=epoch,\n                                      model=model,\n                                      dataloader=test_dataloader,\n                                      loss_fn=loss_fn,\n                                      device=device,\n                                      disable_progress_bar=disable_progress_bar)\n      test_epoch_end_time = time.time()\n      test_epoch_time = test_epoch_end_time - test_epoch_start_time\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f} | \"\n          f\"train_epoch_time: {train_epoch_time:.4f} | \"\n          f\"test_epoch_time: {test_epoch_time:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n      results[\"train_epoch_time\"].append(train_epoch_time)\n      results[\"test_epoch_time\"].append(test_epoch_time)\n\n  # Return the filled results at the end of the epochs\n  return results\n</pre> import time from tqdm.auto import tqdm from typing import Dict, List, Tuple  def train_step(epoch: int,                model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer,                device: torch.device,                disable_progress_bar: bool = False) -&gt; Tuple[float, float]:   \"\"\"Trains a PyTorch model for a single epoch.    Turns a target PyTorch model to training mode and then   runs through all of the required training steps (forward   pass, loss calculation, optimizer step).    Args:     model: A PyTorch model to be trained.     dataloader: A DataLoader instance for the model to be trained on.     loss_fn: A PyTorch loss function to minimize.     optimizer: A PyTorch optimizer to help minimize the loss function.     device: A target device to compute on (e.g. \"cuda\" or \"cpu\").    Returns:     A tuple of training loss and training accuracy metrics.     In the form (train_loss, train_accuracy). For example:      (0.1112, 0.8743)   \"\"\"   # Put model in train mode   model.train()    # Setup train loss and train accuracy values   train_loss, train_acc = 0, 0    # Loop through data loader data batches   progress_bar = tqdm(         enumerate(dataloader),          desc=f\"Training Epoch {epoch}\",          total=len(dataloader),         disable=disable_progress_bar     )    for batch, (X, y) in progress_bar:       # Send data to target device       X, y = X.to(device), y.to(device)        # 1. Forward pass       y_pred = model(X)        # 2. Calculate  and accumulate loss       loss = loss_fn(y_pred, y)       train_loss += loss.item()         # 3. Optimizer zero grad       optimizer.zero_grad()        # 4. Loss backward       loss.backward()        # 5. Optimizer step       optimizer.step()        # Calculate and accumulate accuracy metric across all batches       y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)       train_acc += (y_pred_class == y).sum().item()/len(y_pred)        # Update progress bar       progress_bar.set_postfix(             {                 \"train_loss\": train_loss / (batch + 1),                 \"train_acc\": train_acc / (batch + 1),             }         )     # Adjust metrics to get average loss and accuracy per batch    train_loss = train_loss / len(dataloader)   train_acc = train_acc / len(dataloader)   return train_loss, train_acc  def test_step(epoch: int,               model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,               device: torch.device,               disable_progress_bar: bool = False) -&gt; Tuple[float, float]:   \"\"\"Tests a PyTorch model for a single epoch.    Turns a target PyTorch model to \"eval\" mode and then performs   a forward pass on a testing dataset.    Args:     model: A PyTorch model to be tested.     dataloader: A DataLoader instance for the model to be tested on.     loss_fn: A PyTorch loss function to calculate loss on the test data.     device: A target device to compute on (e.g. \"cuda\" or \"cpu\").    Returns:     A tuple of testing loss and testing accuracy metrics.     In the form (test_loss, test_accuracy). For example:      (0.0223, 0.8985)   \"\"\"   # Put model in eval mode   model.eval()     # Setup test loss and test accuracy values   test_loss, test_acc = 0, 0    # Loop through data loader data batches   progress_bar = tqdm(       enumerate(dataloader),        desc=f\"Testing Epoch {epoch}\",        total=len(dataloader),       disable=disable_progress_bar   )    # Turn on inference context manager   with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case       # Loop through DataLoader batches       for batch, (X, y) in progress_bar:           # Send data to target device           X, y = X.to(device), y.to(device)            # 1. Forward pass           test_pred_logits = model(X)            # 2. Calculate and accumulate loss           loss = loss_fn(test_pred_logits, y)           test_loss += loss.item()            # Calculate and accumulate accuracy           test_pred_labels = test_pred_logits.argmax(dim=1)           test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))            # Update progress bar           progress_bar.set_postfix(               {                   \"test_loss\": test_loss / (batch + 1),                   \"test_acc\": test_acc / (batch + 1),               }           )    # Adjust metrics to get average loss and accuracy per batch    test_loss = test_loss / len(dataloader)   test_acc = test_acc / len(dataloader)   return test_loss, test_acc  def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,           disable_progress_bar: bool = False) -&gt; Dict[str, List]:   \"\"\"Trains and tests a PyTorch model.    Passes a target PyTorch models through train_step() and test_step()   functions for a number of epochs, training and testing the model   in the same epoch loop.    Calculates, prints and stores evaluation metrics throughout.    Args:     model: A PyTorch model to be trained and tested.     train_dataloader: A DataLoader instance for the model to be trained on.     test_dataloader: A DataLoader instance for the model to be tested on.     optimizer: A PyTorch optimizer to help minimize the loss function.     loss_fn: A PyTorch loss function to calculate loss on both datasets.     epochs: An integer indicating how many epochs to train for.     device: A target device to compute on (e.g. \"cuda\" or \"cpu\").    Returns:     A dictionary of training and testing loss as well as training and     testing accuracy metrics. Each metric has a value in a list for      each epoch.     In the form: {train_loss: [...],                   train_acc: [...],                   test_loss: [...],                   test_acc: [...]}      For example if training for epochs=2:                   {train_loss: [2.0616, 1.0537],                   train_acc: [0.3945, 0.3945],                   test_loss: [1.2641, 1.5706],                   test_acc: [0.3400, 0.2973]}    \"\"\"   # Create empty results dictionary   results = {\"train_loss\": [],       \"train_acc\": [],       \"test_loss\": [],       \"test_acc\": [],       \"train_epoch_time\": [],       \"test_epoch_time\": []   }    # Loop through training and testing steps for a number of epochs   for epoch in tqdm(range(epochs), disable=disable_progress_bar):        # Perform training step and time it       train_epoch_start_time = time.time()       train_loss, train_acc = train_step(epoch=epoch,                                          model=model,                                         dataloader=train_dataloader,                                         loss_fn=loss_fn,                                         optimizer=optimizer,                                         device=device,                                         disable_progress_bar=disable_progress_bar)       train_epoch_end_time = time.time()       train_epoch_time = train_epoch_end_time - train_epoch_start_time              # Perform testing step and time it       test_epoch_start_time = time.time()       test_loss, test_acc = test_step(epoch=epoch,                                       model=model,                                       dataloader=test_dataloader,                                       loss_fn=loss_fn,                                       device=device,                                       disable_progress_bar=disable_progress_bar)       test_epoch_end_time = time.time()       test_epoch_time = test_epoch_end_time - test_epoch_start_time        # Print out what's happening       print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f} | \"           f\"train_epoch_time: {train_epoch_time:.4f} | \"           f\"test_epoch_time: {test_epoch_time:.4f}\"       )        # Update results dictionary       results[\"train_loss\"].append(train_loss)       results[\"train_acc\"].append(train_acc)       results[\"test_loss\"].append(test_loss)       results[\"test_acc\"].append(test_acc)       results[\"train_epoch_time\"].append(train_epoch_time)       results[\"test_epoch_time\"].append(test_epoch_time)    # Return the filled results at the end of the epochs   return results In\u00a0[19]: Copied! <pre># Set the number of epochs as a constant\nNUM_EPOCHS = 5\n\n# Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time)\nLEARNING_RATE = 0.003\n</pre> # Set the number of epochs as a constant NUM_EPOCHS = 5  # Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time) LEARNING_RATE = 0.003 <p>Note: Depending on the speed of your GPU, the following code can take a little while to run. For example, it took around 16 minutes on my local NVIDIA TITAN RTX and around 7 minutes on a NVIDIA A100 GPU on Google Colab Pro.</p> In\u00a0[20]: Copied! <pre># Create model\nmodel, transforms = create_model()\nmodel.to(device)\n\n# Create loss function and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Train model and track results\nsingle_run_no_compile_results = train(model=model,\n                                      train_dataloader=train_dataloader,\n                                      test_dataloader=test_dataloader,\n                                      loss_fn=loss_fn,\n                                      optimizer=optimizer,\n                                      epochs=NUM_EPOCHS,\n                                      device=device)\n</pre> # Create model model, transforms = create_model() model.to(device)  # Create loss function and optimizer loss_fn = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),                              lr=LEARNING_RATE)  # Train model and track results single_run_no_compile_results = train(model=model,                                       train_dataloader=train_dataloader,                                       test_dataloader=test_dataloader,                                       loss_fn=loss_fn,                                       optimizer=optimizer,                                       epochs=NUM_EPOCHS,                                       device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Training Epoch 0:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 0:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7734 | train_acc: 0.7333 | test_loss: 0.8021 | test_acc: 0.7477 | train_epoch_time: 184.9701 | test_epoch_time: 12.9893\n</pre> <pre>Training Epoch 1:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 1:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 2 | train_loss: 0.4337 | train_acc: 0.8501 | test_loss: 0.4794 | test_acc: 0.8338 | train_epoch_time: 185.3404 | test_epoch_time: 12.9515\n</pre> <pre>Training Epoch 2:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 2:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 3 | train_loss: 0.3055 | train_acc: 0.8944 | test_loss: 0.4282 | test_acc: 0.8533 | train_epoch_time: 185.3870 | test_epoch_time: 13.0559\n</pre> <pre>Training Epoch 3:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 3:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 4 | train_loss: 0.2268 | train_acc: 0.9198 | test_loss: 0.4387 | test_acc: 0.8580 | train_epoch_time: 185.5914 | test_epoch_time: 13.0495\n</pre> <pre>Training Epoch 4:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 4:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 5 | train_loss: 0.1723 | train_acc: 0.9395 | test_loss: 0.3901 | test_acc: 0.8754 | train_epoch_time: 185.5304 | test_epoch_time: 13.0517\n</pre> In\u00a0[21]: Copied! <pre># Create model and transforms\nmodel, transforms = create_model()\nmodel.to(device)\n\n# Create loss function and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Compile the model and time how long it takes\ncompile_start_time = time.time()\n\n### New in PyTorch 2.x ###\ncompiled_model = torch.compile(model)\n##########################\n\ncompile_end_time = time.time()\ncompile_time = compile_end_time - compile_start_time\nprint(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n\n# Train the compiled model\nsingle_run_compile_results = train(model=compiled_model,\n                                   train_dataloader=train_dataloader,\n                                   test_dataloader=test_dataloader,\n                                   loss_fn=loss_fn,\n                                   optimizer=optimizer,\n                                   epochs=NUM_EPOCHS,\n                                   device=device)\n</pre> # Create model and transforms model, transforms = create_model() model.to(device)  # Create loss function and optimizer loss_fn = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),                              lr=LEARNING_RATE)  # Compile the model and time how long it takes compile_start_time = time.time()  ### New in PyTorch 2.x ### compiled_model = torch.compile(model) ##########################  compile_end_time = time.time() compile_time = compile_end_time - compile_start_time print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")  # Train the compiled model single_run_compile_results = train(model=compiled_model,                                    train_dataloader=train_dataloader,                                    test_dataloader=test_dataloader,                                    loss_fn=loss_fn,                                    optimizer=optimizer,                                    epochs=NUM_EPOCHS,                                    device=device) <pre>Time to compile: 0.00491642951965332 | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Training Epoch 0:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 0:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7585 | train_acc: 0.7364 | test_loss: 0.5852 | test_acc: 0.8004 | train_epoch_time: 196.4621 | test_epoch_time: 21.0730\n</pre> <pre>Training Epoch 1:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 1:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 2 | train_loss: 0.4288 | train_acc: 0.8521 | test_loss: 0.5468 | test_acc: 0.8108 | train_epoch_time: 169.9891 | test_epoch_time: 11.0555\n</pre> <pre>Training Epoch 2:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 2:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 3 | train_loss: 0.3080 | train_acc: 0.8928 | test_loss: 0.4791 | test_acc: 0.8377 | train_epoch_time: 170.4004 | test_epoch_time: 10.9841\n</pre> <pre>Training Epoch 3:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 3:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 4 | train_loss: 0.2322 | train_acc: 0.9184 | test_loss: 0.5551 | test_acc: 0.8306 | train_epoch_time: 170.1974 | test_epoch_time: 11.0482\n</pre> <pre>Training Epoch 4:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 4:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 5 | train_loss: 0.1766 | train_acc: 0.9376 | test_loss: 0.3410 | test_acc: 0.8874 | train_epoch_time: 170.0547 | test_epoch_time: 10.9239\n</pre> In\u00a0[22]: Copied! <pre># Turn experiment results into dataframes\nimport pandas as pd\nsingle_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results)\nsingle_run_compile_results_df = pd.DataFrame(single_run_compile_results)\n</pre> # Turn experiment results into dataframes import pandas as pd single_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results) single_run_compile_results_df = pd.DataFrame(single_run_compile_results) In\u00a0[23]: Copied! <pre># Check out the head of one of the results dataframes\nsingle_run_no_compile_results_df.head()\n</pre> # Check out the head of one of the results dataframes single_run_no_compile_results_df.head() Out[23]: train_loss train_acc test_loss test_acc train_epoch_time test_epoch_time 0 0.773435 0.733272 0.802100 0.747725 184.970135 12.989331 1 0.433699 0.850052 0.479412 0.833762 185.340373 12.951483 2 0.305494 0.894429 0.428212 0.853343 185.386973 13.055891 3 0.226751 0.919829 0.438668 0.857991 185.591368 13.049541 4 0.172269 0.939482 0.390148 0.875396 185.530370 13.051713 <p>Got the results for experiments 1 and 2!</p> <p>Now let's write a function to take in the results and compare them with a bar chart.</p> <p>We'll add some metadata to the function so it can display some information about the experiments.</p> <p>Namely all of the parameters in our experiment setup:</p> <ul> <li>The dataset name.</li> <li>The model name.</li> <li>The number of epochs.</li> <li>The batch size.</li> <li>The image size.</li> </ul> In\u00a0[24]: Copied! <pre># Create filename to save the results\nDATASET_NAME = \"CIFAR10\"\nMODEL_NAME = \"ResNet50\"\n</pre> # Create filename to save the results DATASET_NAME = \"CIFAR10\" MODEL_NAME = \"ResNet50\" In\u00a0[25]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_mean_epoch_times(non_compiled_results: pd.DataFrame, \n                          compiled_results: pd.DataFrame, \n                          multi_runs: bool=False, \n                          num_runs: int=0, \n                          save: bool=False, \n                          save_path: str=\"\",\n                          dataset_name: str=DATASET_NAME,\n                          model_name: str=MODEL_NAME,\n                          num_epochs: int=NUM_EPOCHS,\n                          image_size: int=IMAGE_SIZE,\n                          batch_size: int=BATCH_SIZE) -&gt; plt.figure:\n    \n    # Get the mean epoch times from the non-compiled models\n    mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()\n    mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()\n    mean_results = [mean_train_epoch_time, mean_test_epoch_time]\n\n    # Get the mean epoch times from the compiled models\n    mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()\n    mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()\n    mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]\n\n    # Calculate the percentage difference between the mean compile and non-compile train epoch times\n    train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time\n    train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100\n\n    # Calculate the percentage difference between the mean compile and non-compile test epoch times\n    test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time\n    test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100\n\n    # Print the mean difference percentages\n    print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")\n    print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")\n\n    # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models\n    plt.figure(figsize=(10, 7))\n    width = 0.3\n    x_indicies = np.arange(len(mean_results))\n\n    plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")\n    plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")\n    plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))\n    plt.ylabel(\"Mean epoch time (seconds, lower is better)\")\n\n    # Create the title based on the parameters passed to the function\n    if multi_runs:\n        plt.suptitle(\"Multiple run results\")\n        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n    else:\n        plt.suptitle(\"Single run results\")\n        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n    plt.legend();\n\n    # Save the figure\n    if save:\n        assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"\n        plt.savefig(save_path)\n        print(f\"[INFO] Plot saved to {save_path}\")\n</pre> import matplotlib.pyplot as plt import numpy as np  def plot_mean_epoch_times(non_compiled_results: pd.DataFrame,                            compiled_results: pd.DataFrame,                            multi_runs: bool=False,                            num_runs: int=0,                            save: bool=False,                            save_path: str=\"\",                           dataset_name: str=DATASET_NAME,                           model_name: str=MODEL_NAME,                           num_epochs: int=NUM_EPOCHS,                           image_size: int=IMAGE_SIZE,                           batch_size: int=BATCH_SIZE) -&gt; plt.figure:          # Get the mean epoch times from the non-compiled models     mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()     mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()     mean_results = [mean_train_epoch_time, mean_test_epoch_time]      # Get the mean epoch times from the compiled models     mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()     mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()     mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]      # Calculate the percentage difference between the mean compile and non-compile train epoch times     train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time     train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100      # Calculate the percentage difference between the mean compile and non-compile test epoch times     test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time     test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100      # Print the mean difference percentages     print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")     print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")      # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models     plt.figure(figsize=(10, 7))     width = 0.3     x_indicies = np.arange(len(mean_results))      plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")     plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")     plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))     plt.ylabel(\"Mean epoch time (seconds, lower is better)\")      # Create the title based on the parameters passed to the function     if multi_runs:         plt.suptitle(\"Multiple run results\")         plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")     else:         plt.suptitle(\"Single run results\")         plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")     plt.legend();      # Save the figure     if save:         assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"         plt.savefig(save_path)         print(f\"[INFO] Plot saved to {save_path}\") <p>Plot function ready!</p> <p>Let's create a directory to store our figures in and then plot the results of our first two experiments.</p> In\u00a0[26]: Copied! <pre># Create directory for saving figures\nimport os\ndir_to_save_figures_in = \"pytorch_2_results/figures/\" \nos.makedirs(dir_to_save_figures_in, exist_ok=True)\n\n# Create a save path for the single run results\nsave_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\nprint(f\"[INFO] Save path for single run results: {save_path_multi_run}\")\n\n# Plot the results and save the figures\nplot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df, \n                      compiled_results=single_run_compile_results_df, \n                      multi_runs=False, \n                      save_path=save_path_multi_run, \n                      save=True)\n</pre> # Create directory for saving figures import os dir_to_save_figures_in = \"pytorch_2_results/figures/\"  os.makedirs(dir_to_save_figures_in, exist_ok=True)  # Create a save path for the single run results save_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\" print(f\"[INFO] Save path for single run results: {save_path_multi_run}\")  # Plot the results and save the figures plot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df,                        compiled_results=single_run_compile_results_df,                        multi_runs=False,                        save_path=save_path_multi_run,                        save=True) <pre>[INFO] Save path for single run results: pytorch_2_results/figures/single_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\nMean train epoch time difference: -5.364% (negative means faster)\nMean test epoch time difference: -0.02% (negative means faster)\n[INFO] Plot saved to pytorch_2_results/figures/single_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n</pre> <p>Hmm... what's happening here?</p> <p>It looks like the model with <code>torch.compile()</code> took longer than the model without it (on an A100, this is the case, but on my local NVIDIA TITAN RTX, the compiled model is slightly faster).</p> <p>Why might this be the case?</p> <p>Well on a per epoch time we can see that although experiment 2 (with <code>torch.compile()</code>) was far slower for the first epoch, it started being faster than experiment 1 (without <code>torch.compile()</code>) for subsequent epochs.</p> <p>This is because behind the scenes <code>torch.compile()</code> spends the first steps of a training run \"warming up\" the model and performing optimization steps behind the scenes.</p> <p>These optimization steps take time up front but mean subsequent steps should be faster.</p> <p>To test if this is true, you could try training the model above for longer (say 50 epochs rather than 5) and see what the average training times come out to be.</p> In\u00a0[27]: Copied! <pre># Make a directory for single_run results\nimport os\npytorch_2_results_dir = \"pytorch_2_results\"\npytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\"\nos.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)\n\n# Create filenames for each of the dataframes\nsave_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\nsave_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n\n# Create filepaths to save the results to\nsingle_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\"\nsingle_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\"\nprint(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\")\nprint(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")\n\n# Save the results\nsingle_run_no_compile_results_df.to_csv(single_run_no_compile_save_path)\nsingle_run_compile_results_df.to_csv(single_run_compile_save_path)\n</pre> # Make a directory for single_run results import os pytorch_2_results_dir = \"pytorch_2_results\" pytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\" os.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)  # Create filenames for each of the dataframes save_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\" save_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"  # Create filepaths to save the results to single_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\" single_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\" print(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\") print(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")  # Save the results single_run_no_compile_results_df.to_csv(single_run_no_compile_save_path) single_run_compile_results_df.to_csv(single_run_compile_save_path) <pre>[INFO] Saving non-compiled experiment 1 results to: pytorch_2_results/single_run_results/single_run_non_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n[INFO] Saving compiled experiment 2 results to: pytorch_2_results/single_run_results/single_run_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n</pre> In\u00a0[28]: Copied! <pre>def create_and_train_non_compiled_model(epochs=NUM_EPOCHS, \n                                        learning_rate=LEARNING_RATE, \n                                        disable_progress_bar=False):\n\"\"\"\n    Create and train a non-compiled PyTorch model.\n    \"\"\"\n    model, _ = create_model()\n    model.to(device)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),\n                                 lr=learning_rate)\n\n    results = train(model=model,\n                    train_dataloader=train_dataloader,\n                    test_dataloader=test_dataloader,\n                    loss_fn=loss_fn,\n                    optimizer=optimizer,\n                    epochs=epochs,\n                    device=device,\n                    disable_progress_bar=disable_progress_bar)\n    return results\n\ndef create_compiled_model():\n\"\"\"\n    Create a compiled PyTorch model and return it.\n    \"\"\"\n    model, _ = create_model()\n    model.to(device)\n    \n    compile_start_time = time.time()\n    ### New in PyTorch 2.x ###\n    compiled_model = torch.compile(model)\n    ##########################\n    compile_end_time = time.time()\n\n    compile_time = compile_end_time - compile_start_time\n\n    print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n    return compiled_model\n\ndef train_compiled_model(model=compiled_model, \n                         epochs=NUM_EPOCHS, \n                         learning_rate=LEARNING_RATE,\n                         disable_progress_bar=False):\n\"\"\"\n    Train a compiled model and return the results.\n    \"\"\"\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(compiled_model.parameters(),\n                                 lr=learning_rate)\n    \n    compile_results = train(model=model,\n                            train_dataloader=train_dataloader,\n                            test_dataloader=test_dataloader,\n                            loss_fn=loss_fn,\n                            optimizer=optimizer,\n                            epochs=epochs,\n                            device=device,\n                            disable_progress_bar=disable_progress_bar)\n    \n    return compile_results\n</pre> def create_and_train_non_compiled_model(epochs=NUM_EPOCHS,                                          learning_rate=LEARNING_RATE,                                          disable_progress_bar=False):     \"\"\"     Create and train a non-compiled PyTorch model.     \"\"\"     model, _ = create_model()     model.to(device)      loss_fn = torch.nn.CrossEntropyLoss()     optimizer = torch.optim.Adam(model.parameters(),                                  lr=learning_rate)      results = train(model=model,                     train_dataloader=train_dataloader,                     test_dataloader=test_dataloader,                     loss_fn=loss_fn,                     optimizer=optimizer,                     epochs=epochs,                     device=device,                     disable_progress_bar=disable_progress_bar)     return results  def create_compiled_model():     \"\"\"     Create a compiled PyTorch model and return it.     \"\"\"     model, _ = create_model()     model.to(device)          compile_start_time = time.time()     ### New in PyTorch 2.x ###     compiled_model = torch.compile(model)     ##########################     compile_end_time = time.time()      compile_time = compile_end_time - compile_start_time      print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")     return compiled_model  def train_compiled_model(model=compiled_model,                           epochs=NUM_EPOCHS,                           learning_rate=LEARNING_RATE,                          disable_progress_bar=False):     \"\"\"     Train a compiled model and return the results.     \"\"\"     loss_fn = torch.nn.CrossEntropyLoss()     optimizer = torch.optim.Adam(compiled_model.parameters(),                                  lr=learning_rate)          compile_results = train(model=model,                             train_dataloader=train_dataloader,                             test_dataloader=test_dataloader,                             loss_fn=loss_fn,                             optimizer=optimizer,                             epochs=epochs,                             device=device,                             disable_progress_bar=disable_progress_bar)          return compile_results In\u00a0[29]: Copied! <pre># Run non-compiled model for multiple runs\nNUM_RUNS = 3\nNUM_EPOCHS = 5\n\n# Create an empty list to store multiple run results\nnon_compile_results_multiple_runs = []\n\n# Run non-compiled model for multiple runs\nfor i in tqdm(range(NUM_RUNS)):\n    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for non-compiled model\")\n    results = create_and_train_non_compiled_model(epochs=NUM_EPOCHS, disable_progress_bar=True)\n    non_compile_results_multiple_runs.append(results)\n</pre> # Run non-compiled model for multiple runs NUM_RUNS = 3 NUM_EPOCHS = 5  # Create an empty list to store multiple run results non_compile_results_multiple_runs = []  # Run non-compiled model for multiple runs for i in tqdm(range(NUM_RUNS)):     print(f\"[INFO] Run {i+1} of {NUM_RUNS} for non-compiled model\")     results = create_and_train_non_compiled_model(epochs=NUM_EPOCHS, disable_progress_bar=True)     non_compile_results_multiple_runs.append(results) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>[INFO] Run 1 of 3 for non-compiled model\nEpoch: 1 | train_loss: 0.8242 | train_acc: 0.7136 | test_loss: 0.5486 | test_acc: 0.8124 | train_epoch_time: 185.1112 | test_epoch_time: 12.9925\nEpoch: 2 | train_loss: 0.4415 | train_acc: 0.8479 | test_loss: 0.6415 | test_acc: 0.7829 | train_epoch_time: 185.0138 | test_epoch_time: 12.9690\nEpoch: 3 | train_loss: 0.3229 | train_acc: 0.8882 | test_loss: 0.4486 | test_acc: 0.8488 | train_epoch_time: 185.0366 | test_epoch_time: 12.9433\nEpoch: 4 | train_loss: 0.2433 | train_acc: 0.9151 | test_loss: 0.4376 | test_acc: 0.8596 | train_epoch_time: 185.0900 | test_epoch_time: 12.9465\nEpoch: 5 | train_loss: 0.1785 | train_acc: 0.9379 | test_loss: 0.4305 | test_acc: 0.8641 | train_epoch_time: 185.0405 | test_epoch_time: 13.0102\n[INFO] Run 2 of 3 for non-compiled model\nEpoch: 1 | train_loss: 0.8304 | train_acc: 0.7101 | test_loss: 0.6132 | test_acc: 0.7884 | train_epoch_time: 185.0911 | test_epoch_time: 13.0429\nEpoch: 2 | train_loss: 0.4602 | train_acc: 0.8411 | test_loss: 0.6183 | test_acc: 0.7907 | train_epoch_time: 185.0738 | test_epoch_time: 12.9596\nEpoch: 3 | train_loss: 0.3283 | train_acc: 0.8869 | test_loss: 0.4309 | test_acc: 0.8534 | train_epoch_time: 185.0462 | test_epoch_time: 12.9877\nEpoch: 4 | train_loss: 0.2474 | train_acc: 0.9140 | test_loss: 0.4525 | test_acc: 0.8565 | train_epoch_time: 184.9521 | test_epoch_time: 12.9942\nEpoch: 5 | train_loss: 0.1860 | train_acc: 0.9360 | test_loss: 0.6284 | test_acc: 0.8195 | train_epoch_time: 184.9911 | test_epoch_time: 12.9369\n[INFO] Run 3 of 3 for non-compiled model\nEpoch: 1 | train_loss: 0.7915 | train_acc: 0.7246 | test_loss: 0.6102 | test_acc: 0.7894 | train_epoch_time: 184.9795 | test_epoch_time: 13.0175\nEpoch: 2 | train_loss: 0.4394 | train_acc: 0.8477 | test_loss: 0.5958 | test_acc: 0.7968 | train_epoch_time: 184.9266 | test_epoch_time: 12.9909\nEpoch: 3 | train_loss: 0.3156 | train_acc: 0.8893 | test_loss: 0.4299 | test_acc: 0.8547 | train_epoch_time: 185.1226 | test_epoch_time: 12.9396\nEpoch: 4 | train_loss: 0.2371 | train_acc: 0.9163 | test_loss: 0.4185 | test_acc: 0.8608 | train_epoch_time: 184.9447 | test_epoch_time: 12.9673\nEpoch: 5 | train_loss: 0.1739 | train_acc: 0.9389 | test_loss: 0.3797 | test_acc: 0.8805 | train_epoch_time: 184.9552 | test_epoch_time: 13.0328\n</pre> <p>Now we've got a list of results from experiment 3, let's iterate through them and create a dataframe containing all of the results.</p> <p>We'll then average the results across the 3 runs by grouping by the epoch number (the index of the dataframe) and taking the mean of the results.</p> In\u00a0[30]: Copied! <pre># Go through non_compile_results_multiple_runs and create a dataframe for each run then concatenate them together\nnon_compile_results_dfs = []\nfor result in non_compile_results_multiple_runs:\n    result_df = pd.DataFrame(result)\n    non_compile_results_dfs.append(result_df)\nnon_compile_results_multiple_runs_df = pd.concat(non_compile_results_dfs)\n\n# Get the averages across the multiple runs\nnon_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean()\nnon_compile_results_multiple_runs_df\n</pre> # Go through non_compile_results_multiple_runs and create a dataframe for each run then concatenate them together non_compile_results_dfs = [] for result in non_compile_results_multiple_runs:     result_df = pd.DataFrame(result)     non_compile_results_dfs.append(result_df) non_compile_results_multiple_runs_df = pd.concat(non_compile_results_dfs)  # Get the averages across the multiple runs non_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean() non_compile_results_multiple_runs_df Out[30]: train_loss train_acc test_loss test_acc train_epoch_time test_epoch_time 0 0.815352 0.716103 0.590690 0.796710 185.060622 13.017663 1 0.447013 0.845567 0.618526 0.790150 185.004740 12.973144 2 0.322255 0.888117 0.436471 0.852321 185.068499 12.956863 3 0.242587 0.915120 0.436207 0.858946 184.995601 12.969341 4 0.179439 0.937612 0.479547 0.854727 184.995575 12.993280 <p>Wonderful!</p> <p>We can inspect these later, let's move onto experiment 4.</p> In\u00a0[31]: Copied! <pre># Create compiled model\ncompiled_model = create_compiled_model()\n\n# Create an empty list to store compiled model results\ncompiled_results_multiple_runs = []\n\n# Run compiled model for multiple runs\nfor i in tqdm(range(NUM_RUNS)):\n    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for compiled model\")\n    # Train the compiled model (note: the model will only be compiled once and then re-used for subsequent runs)\n    results = train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS, disable_progress_bar=True)\n    compiled_results_multiple_runs.append(results)\n</pre> # Create compiled model compiled_model = create_compiled_model()  # Create an empty list to store compiled model results compiled_results_multiple_runs = []  # Run compiled model for multiple runs for i in tqdm(range(NUM_RUNS)):     print(f\"[INFO] Run {i+1} of {NUM_RUNS} for compiled model\")     # Train the compiled model (note: the model will only be compiled once and then re-used for subsequent runs)     results = train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS, disable_progress_bar=True)     compiled_results_multiple_runs.append(results) <pre>Time to compile: 0.001275777816772461 | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>[INFO] Run 1 of 3 for compiled model\nEpoch: 1 | train_loss: 0.8026 | train_acc: 0.7192 | test_loss: 0.6995 | test_acc: 0.7650 | train_epoch_time: 194.3336 | test_epoch_time: 20.6106\nEpoch: 2 | train_loss: 0.4440 | train_acc: 0.8483 | test_loss: 0.5565 | test_acc: 0.8089 | train_epoch_time: 169.3882 | test_epoch_time: 10.8076\nEpoch: 3 | train_loss: 0.3208 | train_acc: 0.8896 | test_loss: 0.4164 | test_acc: 0.8620 | train_epoch_time: 169.9283 | test_epoch_time: 10.8361\nEpoch: 4 | train_loss: 0.2329 | train_acc: 0.9197 | test_loss: 0.3635 | test_acc: 0.8792 | train_epoch_time: 169.8744 | test_epoch_time: 10.9050\nEpoch: 5 | train_loss: 0.1803 | train_acc: 0.9369 | test_loss: 0.4387 | test_acc: 0.8587 | train_epoch_time: 169.6391 | test_epoch_time: 10.8240\n[INFO] Run 2 of 3 for compiled model\nEpoch: 1 | train_loss: 0.1875 | train_acc: 0.9347 | test_loss: 0.4187 | test_acc: 0.8714 | train_epoch_time: 169.4814 | test_epoch_time: 10.8180\nEpoch: 2 | train_loss: 0.1288 | train_acc: 0.9550 | test_loss: 0.4333 | test_acc: 0.8698 | train_epoch_time: 169.4503 | test_epoch_time: 10.8263\nEpoch: 3 | train_loss: 0.0950 | train_acc: 0.9672 | test_loss: 0.4867 | test_acc: 0.8650 | train_epoch_time: 169.6038 | test_epoch_time: 10.8199\nEpoch: 4 | train_loss: 0.0943 | train_acc: 0.9675 | test_loss: 0.3714 | test_acc: 0.8966 | train_epoch_time: 169.5757 | test_epoch_time: 10.8221\nEpoch: 5 | train_loss: 0.0537 | train_acc: 0.9821 | test_loss: 0.5002 | test_acc: 0.8701 | train_epoch_time: 169.5253 | test_epoch_time: 10.8426\n[INFO] Run 3 of 3 for compiled model\nEpoch: 1 | train_loss: 0.0705 | train_acc: 0.9751 | test_loss: 0.4333 | test_acc: 0.8839 | train_epoch_time: 169.4846 | test_epoch_time: 10.9057\nEpoch: 2 | train_loss: 0.0595 | train_acc: 0.9802 | test_loss: 0.4341 | test_acc: 0.8904 | train_epoch_time: 169.6055 | test_epoch_time: 10.8804\nEpoch: 3 | train_loss: 0.0405 | train_acc: 0.9859 | test_loss: 0.4478 | test_acc: 0.8901 | train_epoch_time: 169.5788 | test_epoch_time: 10.8449\nEpoch: 4 | train_loss: 0.0365 | train_acc: 0.9873 | test_loss: 0.5382 | test_acc: 0.8765 | train_epoch_time: 169.6732 | test_epoch_time: 10.9873\nEpoch: 5 | train_loss: 0.0422 | train_acc: 0.9854 | test_loss: 0.5057 | test_acc: 0.8832 | train_epoch_time: 169.6618 | test_epoch_time: 10.8969\n</pre> <p>Experiment 4 done!</p> <p>Now let's put the results together into a dataframe and take the mean across each of the runs (we'll do this by grouping by the epoch number, which is the index number of the dataframe).</p> In\u00a0[32]: Copied! <pre># Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together\ncompile_results_dfs = []\nfor result in compiled_results_multiple_runs:\n    result_df = pd.DataFrame(result)\n    compile_results_dfs.append(result_df)\ncompile_results_multiple_runs_df = pd.concat(compile_results_dfs)\n\n# Get the averages across the multiple runs\ncompile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number\ncompile_results_multiple_runs_df\n</pre> # Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together compile_results_dfs = [] for result in compiled_results_multiple_runs:     result_df = pd.DataFrame(result)     compile_results_dfs.append(result_df) compile_results_multiple_runs_df = pd.concat(compile_results_dfs)  # Get the averages across the multiple runs compile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number compile_results_multiple_runs_df Out[32]: train_loss train_acc test_loss test_acc train_epoch_time test_epoch_time 0 0.353548 0.876332 0.517181 0.840124 177.766548 14.111428 1 0.210781 0.927845 0.474630 0.856375 169.481367 10.838063 2 0.152098 0.947577 0.450293 0.872396 169.703638 10.833619 3 0.121230 0.958177 0.424376 0.884065 169.707751 10.904810 4 0.092080 0.968116 0.481520 0.870649 169.608708 10.854486 In\u00a0[33]: Copied! <pre># Create a directory to save the multi-run figure to \nos.makedirs(\"pytorch_2_results/figures\", exist_ok=True)\n\n# Create a path to save the figure for multiple runs\nsave_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n\n# Plot the mean epoch times for experiment 3 and 4\nplot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df, \n                      compiled_results=compile_results_multiple_runs_df, \n                      multi_runs=True, \n                      num_runs=NUM_RUNS, \n                      save_path=save_path_multi_run, \n                      save=True)\n</pre> # Create a directory to save the multi-run figure to  os.makedirs(\"pytorch_2_results/figures\", exist_ok=True)  # Create a path to save the figure for multiple runs save_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"  # Plot the mean epoch times for experiment 3 and 4 plot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df,                        compiled_results=compile_results_multiple_runs_df,                        multi_runs=True,                        num_runs=NUM_RUNS,                        save_path=save_path_multi_run,                        save=True) <pre>Mean train epoch time difference: -7.443% (negative means faster)\nMean test epoch time difference: -11.351% (negative means faster)\n[INFO] Plot saved to pytorch_2_results/figures/multi_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n</pre> <p>Nice!</p> <p>Looks like the compiled model edges out the non-compiled model across multiple runs.</p> <p>This is likely because on a single run (with a low amount of epochs), the compiling of the model takes quite a bit of time for the first epoch to run.</p> <p>However, when the model has already been compiled and starts training for longer, the speedups from the behind the scenes optimizations start to show.</p> <p>A possible extension would be to let the model train for a longer time, say 100 epochs, and see how the results compare.</p> In\u00a0[34]: Copied! <pre># Make a directory for multi_run results\nimport os\npytorch_2_results_dir = \"pytorch_2_results\"\npytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\"\nos.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)\n\n# Create filenames for each of the dataframes\nsave_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\nsave_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n\n# Create filepaths to save the results to\nmulti_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\"\nmulti_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\"\nprint(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\")\nprint(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")\n\n# Save the results\nnon_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path)\ncompile_results_multiple_runs_df.to_csv(multi_run_compile_save_path)\n</pre> # Make a directory for multi_run results import os pytorch_2_results_dir = \"pytorch_2_results\" pytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\" os.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)  # Create filenames for each of the dataframes save_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\" save_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"  # Create filepaths to save the results to multi_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\" multi_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\" print(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\") print(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")  # Save the results non_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path) compile_results_multiple_runs_df.to_csv(multi_run_compile_save_path) <pre>[INFO] Saving experiment 3 non-compiled results to: pytorch_2_results/multi_run_results/single_run_non_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n[INFO] Saving experiment 4 compiled results to: pytorch_2_results/multi_run_results/single_run_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n</pre>"},{"location":"pytorch_2_intro/#a-quick-pytorch-20-tutorial","title":"A Quick PyTorch 2.0 Tutorial\u00b6","text":""},{"location":"pytorch_2_intro/#30-second-intro","title":"30-second intro\u00b6","text":"<p>PyTorch 2.0 is out!</p> <p>With the main improvement being speed.</p> <p>This comes via a single backwards-compatible line.</p> <pre>torch.compile()\n</pre> <p>In other words, after you create your model, you can pass it to <code>torch.compile()</code> and in turn expect speedups in training and inference on newer GPUs (e.g. NVIDIA RTX 40 series, A100, H100, the newer the GPU the more noticeable the speedups).</p> <p>Note: There are plenty more upgrades within PyTorch 2.0 than just <code>torch.compile()</code> but since it's the main one, it's what we're going to focus on. For a full list of changes, see the PyTorch 2.0 release notes.</p>"},{"location":"pytorch_2_intro/#will-my-old-pytorch-code-still-work","title":"Will my old PyTorch code still work?\u00b6","text":"<p>Yes, PyTorch 2.0 is backwards-compatible. The changes are mostly additive (new features).</p> <p>That means if you already know PyTorch, such as via the learnpytorch.io course, you can start using PyTorch 2.0 straight away. And your old PyTorch code will still work.</p>"},{"location":"pytorch_2_intro/#quick-code-examples","title":"Quick code examples\u00b6","text":""},{"location":"pytorch_2_intro/#before-pytorch-20","title":"Before PyTorch 2.0\u00b6","text":""},{"location":"pytorch_2_intro/#after-pytorch-20","title":"After PyTorch 2.0\u00b6","text":""},{"location":"pytorch_2_intro/#speedups","title":"Speedups\u00b6","text":"<p>Ok so the focus of PyTorch 2.0 is speed, how much faster is it actually?</p> <p>The PyTorch team ran tests across 163 open-source models from Hugging Face Transformers, timm (PyTorch Image Models) and TorchBench (a curated set of popular code bases from across GitHub).</p> <p>This is important because unless PyTorch 2.0 is faster on models people actually use, it\u2019s not faster.</p> <p>Using a mixture of AMP (automatic mixed precision or float16) training and float32 precision (higher precision requires more compute) the PyTorch team found that <code>torch.compile()</code> provides an average speedup of 43% in training on a NVIDIA A100 GPU.</p> <p>Or 38% on timm, 76% on TorchBench and 52% on Hugging Face Transformers.</p> <p>PyTorch 2.0 speedups across various models from different locations. Source:* PyTorch 2.0 announcement post.*</p>"},{"location":"pytorch_2_intro/#3-minute-overview","title":"3-minute overview\u00b6","text":"<p>Note: The following is adapted from A Quick Introduction to PyTorch 2.0 on mrdbourke.com, there's also an accompanying video explainer on YouTube.</p> <p>What's happening behind the scenes of <code>torch.compile()</code>?</p> <p><code>torch.compile()</code> is designed to \"just work\" but there are a few technologies behind it:</p> <ul> <li>TorchDynamo</li> <li>AOTAutograd</li> <li>PrimTorch</li> <li>TorchInductor</li> </ul> <p>The PyTorch 2.0 getting started notes explain these in more detail but from a high level the two main improvements <code>torch.compile()</code> offers are:</p> <ul> <li>Fusion (or operator fusion)</li> <li>Graph capture (or graph tracing)</li> </ul>"},{"location":"pytorch_2_intro/#fusion","title":"Fusion\u00b6","text":"<p>Fusion, also known as operator fusion is one of the best ways to make deep learning models go brrrrrr (brrrrrr is the sound your GPUs fan make when your models are training).</p> <p>Operator fusion condenses (like Dragon Ball Z) many operations into one (or many to less).</p> <p>Why?</p> <p>Modern GPUs have so much compute power they are often not compute limited, as in, the main bottleneck to training models is how fast can you get data from your CPU to your GPU. This is known as bandwidth or memory bandwidth.</p> <p>You want to reduce your bandwidth costs as much as possible.</p> <p>And feed the data hungry GPUs with as much data as possible.</p> <p>So instead of performing an operation on a piece of data and then saving the result to memory (increased bandwidth costs), you chain together as many operations as possible via fusion.</p> <p>A rough analogy would be using a blender to make a smoothie.</p> <p>Most blenders are good at blending things (like GPUs are good at performing matrix multiplications).</p> <p>Using a blender without operator fusion would be like adding each ingredient one by one and blending each time a new ingredient is added. Not only is this insane, it increases your bandwidth cost.</p> <p>The actual blending is fast each time (like GPU computations generally are) but you lose a bunch of time adding each ingredient one by one.</p> <p>Using a blender with operator fusion is akin to using a blender by adding all the ingredients at the start (operator fusion) and then performing the blend once.</p> <p>You lose a little time adding at the start but you gain all of the lost memory bandwidth time back.</p>"},{"location":"pytorch_2_intro/#graph-capture","title":"Graph capture\u00b6","text":"<p>Graph capture I\u2019m less confident explaining.</p> <p>But the way I think about is that graph capture or graph tracing is:</p> <ul> <li>Going through a series of operations that need to happen, such as the operations in a neural network.</li> <li>And capturing or tracing what needs to happen ahead of time.</li> </ul> <p>Computing without graph capture is like going to a new area and following GPS directions turn by turn.</p> <p>As a good human driver, you can follow the turns quite easily but you still have to think about each turn you take.</p> <p>This is the equivalent to PyTorch having to look up what each operation does as it does it.</p> <p>As in, to perform an addition, it has to look up what an addition does before it can perform it.</p> <p>It does this quickly but there\u2019s still non-zero overhead.</p> <p>Example of graph capture, mapping out the steps in a neural network and then capturing every operation that needs to happen ahead of time.</p> <p>Computing with graph capture is like driving through your own neighbourhood.</p> <p>You barely think about what turns to make.</p> <p>Sometimes you get out of the car and realise you can\u2019t remember the last 5 minutes of the drive.</p> <p>Your brain was functioning on autopilot, minimal overhead.</p> <p>However, it took you some time upfront to remember how to drive to your house.</p> <p>This is a caveat of graph capture, it takes a little time upfront to memorize the operations that need to happen but subsequent computations should be faster.</p> <p>Of course, this is a quick high-level overview of what\u2019s happening behind the scenes of torch.compile()but it's how I understand it.</p> <p>For more on fusion and graph tracing, I\u2019d recommend Horace He\u2019s Making Deep Learning Go Brrrr From First Principles blog post.</p>"},{"location":"pytorch_2_intro/#things-to-note","title":"Things to note\u00b6","text":"<p>Since PyTorch 2.0 was just released, there are a few limitations with some of the features.</p> <p>One of the main ones being with exporting models.</p> <p>There are a few caveats when using the PyTorch 2.0 features, such as not being about to export to mobile devices when using the <code>torch.compile()</code> default options. However, there are work arounds to this and improved exporting is on the PyTorch 2.x roadmap. Source:* PyTorch 2.0 announcement post.*</p> <p>However, these will likely be fixed in future releases.</p> <p>Another main limitation is that because the features of PyTorch 2.0 are designed for newer hardware, old GPUs and desktop-class GPUs (e.g. NVIDIA RTX 30 series) will likely see less speedups than newer hardware.</p>"},{"location":"pytorch_2_intro/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Since many of the upgrades in PyTorch 2.0 are speed focused and happen behind the scenes (e.g. PyTorch takes care of them for you), in this notebook we're going to run a compartive speed test.</p> <p>Namely we'll make two of the same models, one using the default PyTorch setup and the other using the new <code>torch.compile()</code> setup and we'll train them on the same dataset.</p> <ol> <li>Model 1 - no <code>torch.compile()</code>.</li> <li>Model 2 - <code>torch.compile()</code>.</li> </ol> <p>We'll then compare the training/testing times of both models for single run and multiple runs.</p> Experiment Model Data Epochs Batch size Image size <code>torch.compile()</code> 1 (single run) ResNet50 CIFAR10 5 128 224 No 2 (single run) ResNet50 CIFAR10 5 128 224 Yes 3 (multi-run) ResNet50 CIFAR10 3x5 128 224 No 4 (multi-run) ResNet50 CIFAR10 3x5 128 224 Yes <p>We've chosen ResNet50 and CIFAR10 here for ease of access or use, however, you could substitute any model/dataset you like.</p> <p>The biggest speedups I've noticed with PyTorch 2.0 are when the GPU computes on as much data as possible (e.g. larger batch size/image size/data size/model size).</p> <p>Note: Depending on the size of your GPU, you may have to lower the batch size (or image size) to fit the model on your GPU. For example, if you're using a GPU with 8GB of memory or less, you may have to lower the batch size to 64 or 32.</p>"},{"location":"pytorch_2_intro/#0-getting-setup","title":"0. Getting setup\u00b6","text":"<p>To get setup we'll first check for PyTorch 2.x+ and install it if it's not available.</p> <p>You can see how to install PyTorch 2.x on your own system in the PyTorch documentation.</p> <p>Note: If you're running on Google Colab, you'll need to setup a GPU: runtime -&gt; change runtime type -&gt; hardware accelerator. The best speedups are on newer NVIDIA/AMD GPUs (this is because PyTorch 2.0 leverages newer GPU hardware) such as the NVIDIA A100 and above. This tutorial focuses on NVIDIA GPUs.</p>"},{"location":"pytorch_2_intro/#1-get-gpu-info","title":"1. Get GPU info\u00b6","text":"<p>Time to get GPU info.</p> <p>Why?</p> <p>Many of the speedups PyTorch 2.0 offers are best experienced on newer NVIDIA GPUs (we're focused on NVIDIA GPUs for now).</p> <p>This is because PyTorch 2.0 takes advantage of the new hardware on newer GPUs.</p> <p>How do you tell what's a newer GPU?</p> <p>Generally, a newer GPU will have a compute capability score of 8.0 or higher.</p> <p>You can see a list of NVIDIA GPU compute capability scores on NVIDIA's developer page.</p> <p>Here are some scores of NVIDIA GPUs released in 2020 or later:</p> NVIDIA GPU Compute capability score GPU Type Release year Architecture RTX 4090 8.9 Desktop-class 2022 Ada Lovelace RTX 4080 8.9 Desktop-class 2022 Ada Lovelace RTX 4070 Ti 8.9 Desktop-class 2022 Ada Lovelace RTX 3090 8.6 Desktop-class 2020 Ampere RTX 3080 8.6 Desktop-class 2020 Ampere RTX 3070 8.6 Desktop-class 2020 Ampere RTX 3060 Ti 8.6 Desktop-class 2020 Ampere H100 9.0 Datacenter-class 2022 Hopper A100 8.0 Datacenter-class 2020 Ampere A10 8.6 Datacenter-class 2021 Ampere <p>GPUs with a compute capability score of 8.0 or above are likely to see the biggest speedups.</p> <p>And GPUs which are datacenter-class (e.g. A100, A10, H100) are likely to see more significant speedups than desktop-class GPUs (e.g. RTX 3090, RTX 3080, RTX 3070, RTX 3060 Ti).</p> <p>We can check the compute capbility score of our GPU using <code>torch.cuda.get_device_capability()</code>.</p> <p>This will output a tuple of <code>(major, minor)</code> compute capability scores, for example, <code>(8, 0)</code> for the A100.</p> <p>We'll also get some other details about our GPU such as the name and other info using <code>nvidia-smi</code>.</p> <p>Resource: For an in-depth comparison of many different NVIDIA GPUs and their speeds, costs and tradeoffs, I'd recommend reading Tim Dettmers' Which GPU for deep learning? blog post.</p>"},{"location":"pytorch_2_intro/#11-globally-set-devices","title":"1.1 Globally set devices\u00b6","text":"<p>One of my favourite new features in PyTorch 2.x is being able to set the default device type via:</p> <ul> <li>Context manager</li> <li>Globally</li> </ul> <p>Previously, you could only set the default device type via:</p> <ul> <li><code>tensor.to(device)</code></li> </ul> <p>Let's see these two new device settings in action.</p>"},{"location":"pytorch_2_intro/#2-setting-up-the-experiments","title":"2. Setting up the experiments\u00b6","text":"<p>Okay, time to measure speed!</p> <p>To keep things simple, as we discussed we're going to run a series of four experiments, all with:</p> <ul> <li>Model: ResNet50 (from TorchVision)</li> <li>Data: CIFAR10 (from TorchVision)</li> <li>Epochs: 5 (single run) and 3x5 (multiple runs)</li> <li>Batch size: 128</li> <li>Image size: 224</li> </ul> <p>Each experiment will be run with and without <code>torch.compile()</code>.</p> <p>Why the single and multiple runs?</p> <p>Because we can measure speedups via a single run, however, we'll also want to run the tests multiple times to get an average (just to make sure the results from a single run weren't a fluke or something went wrong).</p> <p>Note: Depending on the amount of memory your GPU has, you may have to lower the batch size or the image size. This tutorial is focused on using an NVIDIA A100 GPU with 40GB of memory, the amount of memory on this GPU means it can handle a larger batch size. As of April 2023, NVIDIA A100 GPUs are available via Google Colab Pro.</p> <p>Let's start by importing <code>torch</code> and <code>torchvision</code> and setting the target device.</p>"},{"location":"pytorch_2_intro/#21-create-model-and-transforms","title":"2.1 Create model and transforms\u00b6","text":"<p>Let's now create our model and transforms.</p> <p>We'll use the same setup to create the model and transforms we covered in 06. PyTorch Transfer Learning section 2.2.</p> <p>In essence, we'll create the model and transforms for the model using the <code>torchvision.models</code> API.</p> <p>We can get the weights and transforms for ResNet50 using the following:</p> <ul> <li><code>model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2</code> (this requires <code>torchvision</code> 0.14 or later).</li> <li><code>transforms = model_weights.transforms()</code> (once we have the weights, we can get the appropriate transforms for the model).</li> </ul> <p>Note: We'll count the model's parameters to see how big of a model we're working with. The more parameters in a model, the larger GPU memory you'll need to train it. However, the more parameters your model has, the more GPU memory it uses, the larger relative speedup you'll often see. Meaning, a larger model may take longer to train in total, however, on a relative basis because it's using more GPU power, it could be faster than a smaller model. As in, a model with 10M parameters may take only 5x longer to train than a model with 1M parameters (10x the size but only 5x the training time).</p>"},{"location":"pytorch_2_intro/#22-speedups-are-most-noticeable-when-a-large-portion-of-the-gpu-is-being-used","title":"2.2 Speedups are most noticeable when a large portion of the GPU is being used\u00b6","text":"<p>Since modern GPUs are so fast at performing operations, you will often notice the majority of relative speedups when as much data as possible is on the GPU.</p> <p>This can be achieved by:</p> <ul> <li>Increasing the batch size - More samples per batch means more samples on the GPU, for example, using a batch size of 256 instead of 32.</li> <li>Increasing data size - For example, using larger image size, 224x224 instead of 32x32. A larger data size means that more tensor operations will be happening on the GPU.</li> <li>Increasing model size - For example, using a larger model such as ResNet101 instead of ResNet50. A larger model means that more tensor operations will be happening on the GPU.</li> <li>Decreasing data transfer - For example, setting up all your tensors to be on GPU memory, this minizes the amount of data transfer between the CPU and GPU.</li> </ul> <p>All of these result in more data being on the GPU.</p> <p>You may be thinking, \"but doesn't this mean that the GPU will be slower because it has to do more work?\"</p> <p>This is correct, operations may take longer when using more data on the GPU, however, they benefit from parallelism (many operations happening at once).</p> <p>This means that although more operations are happening, the GPU is performing as many of them as possible simultaneously.</p> <p>So while you may see speedups with smaller datasets, models, batch sizes and data sizes, however, you will tend to see the biggest relative speedups with increasing scale.</p>"},{"location":"pytorch_2_intro/#23-checking-the-memory-limits-of-our-gpu","title":"2.3 Checking the memory limits of our GPU\u00b6","text":"<p>To take advantage of speedups at scale, let's check how much memory our GPU has.</p> <p>If your GPU has less memory, you may need to decrease the batch size or image size (less potential for speedups).</p> <p>We can check the memory available on our GPU using <code>torch.cuda.mem_get_info()</code>.</p> <p>This will return a tuple of <code>(total_free_gpu_memory, total_gpu_memory)</code>.</p> <p>Where:</p> <ul> <li><code>total_free_gpu_memory</code> is the amount of memory currently not being used on the GPU in bytes.</li> <li><code>total_gpu_memory</code> is the total amount of memory available on the GPU in bytes.</li> </ul>"},{"location":"pytorch_2_intro/#24-more-potential-speedups-with-tf32","title":"2.4  More potential speedups with TF32\u00b6","text":"<p>TF32 stands for TensorFloat-32, a data format which is a combination of 16-bit and 32-bit floating point numbers.</p> <p>You can read more about how it works on NVIDIA's blog.</p> <p>The main thing you should know is that it allows you to perform faster matrix multiplications on GPUs with the Ampere architecture and above (a compute capability score of 8.0+).</p> <p>Although it's not specific to PyTorch 2.0, since we're talking about newer GPUs, it's worth mentioning.</p> <p>If you're using a GPU with a compute capability score of 8.0 or above, you can enable TF32 by setting <code>torch.backends.cuda.matmul.allow_tf32 = True</code> (this defaults to <code>False</code>).</p> <p>Let's write a check that sets it automatically for us based on our GPUs compute capability score.</p> <p>Note: TensorFloat32 is disabled by default (set to <code>False</code>) in PyTorch versions 1.12 onwards. This is because it may cause inconsistent results across different devices. Although this issue is not noticed for all use cases, it's worth knowing.</p>"},{"location":"pytorch_2_intro/#25-preparing-datasets","title":"2.5 Preparing datasets\u00b6","text":"<p>Computing setup done!</p> <p>Let's now create our datasets.</p> <p>To keep things simple, we'll use CIFAR10 since it's readily available in <code>torchvision</code>.</p> <p>Some info about CIFAR10 the CIFAR10 website:</p> <ul> <li>CIFAR10 is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.</li> <li>There are 50,000 training images and 10,000 test images.</li> <li>The dataset contains 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.</li> </ul> <p>Although the original dataset consists of 32x32 images, we'll use the <code>transforms</code> we created earlier to resize them to 224x224 (larger images provide more information and will take up more memory on the GPU).</p>"},{"location":"pytorch_2_intro/#26-create-dataloaders","title":"2.6 Create DataLoaders\u00b6","text":"<p>Generally GPUs aren't the bottleneck of machine learning code.</p> <p>Data loading is the main bottleneck.</p> <p>As in, the transfer speed from CPU to GPU.</p> <p>As we're discussed before you want to get your data to the GPU as fast as possible.</p> <p>Let's create our <code>DataLoaders</code> using <code>torch.utils.data.DataLoader</code>.</p> <p>We'll set their <code>batch_size</code> to the <code>BATCH_SIZE</code> we created earlier.</p> <p>And the <code>num_workers</code> parameter to be the number of CPU cores we have available with <code>os.cpu_count()</code>.</p> <p>Note: You may want to experiment with different values for <code>num_workers</code> to see what works best for your specific GPU and CPU setup. In my experience, more is better but some people have found this generally caps out at <code>4 * number_of_gpus_you_have</code>, for example, <code>num_workers = 4 * 1</code> for 1 GPU.</p>"},{"location":"pytorch_2_intro/#27-create-training-and-testing-loops","title":"2.7 Create training and testing loops\u00b6","text":"<p>Dataloaders ready!</p> <p>Let's now create some training and testing loops.</p> <p>These will be the same training and testing loops we created in 05. PyTorch Going Modular with some slight modifications.</p> <p>Since we're focused on measuring speed, we're going to add a timing component to each loop to measure how long each takes to complete.</p> <p>We'll do this by measuring the start and end time of each training and testing epoch with Python's <code>time.time()</code> and tracking it in a dictionary.</p> <p>Note: One thing I found when experimenting with PyTorch 2.0 is that <code>torch.inference_mode()</code> produced errors in the testing loop. So I've changed it to be <code>torch.no_grad()</code> which offers similar functionality but is an older method than <code>torch.inference_mode()</code>. If you find that <code>torch.inference_mode()</code> works for you, please let me know on GitHub and I'll update this notebook.</p>"},{"location":"pytorch_2_intro/#3-time-models-across-single-run","title":"3. Time models across single run\u00b6","text":"<p>Training and testing functions ready!</p> <p>Time to start training/evaluating and timing our model.</p> <p>We'll start with the first experiment.</p>"},{"location":"pytorch_2_intro/#31-experiment-1-single-run-no-compile","title":"3.1 Experiment 1 - Single run, no compile\u00b6","text":"<p>For experiment 1, we'll use the following parameters:</p> Experiment Model Data Epochs Batch size Image size <code>torch.compile()</code> 1 (single run) ResNet50 CIFAR10 5 128 224 No <p>We'll set the number of epochs to <code>5</code> and use a learning rate of <code>0.003</code> throughout (you can experiment with different learning rates for better results but we're focused on speed).</p>"},{"location":"pytorch_2_intro/#32-experiment-2-single-run-with-compile","title":"3.2 Experiment 2 - Single run, with compile\u00b6","text":"<p>Now we'll do the same experiment but this time we'll use <code>torch.compile()</code>.</p> Experiment Model Data Epochs Batch size Image size <code>torch.compile()</code> 2 (single run) ResNet50 CIFAR10 5 128 224 Yes <p>Note: Depending on the speed of your GPU, the following code can take a little while to run. For example, it took around 16 minutes on my local NVIDIA TITAN RTX and around 7 minutes on a NVIDIA A100 GPU on Google Colab Pro.</p>"},{"location":"pytorch_2_intro/#33-compare-the-results-of-experiment-1-and-2","title":"3.3 Compare the results of experiment 1 and 2\u00b6","text":"<p>Nice!</p> <p>We've got two trained models:</p> <ol> <li>One without <code>torch.compile()</code>.</li> <li>One with <code>torch.compile()</code>.</li> </ol> <p>Let's compare the results of each experiment.</p> <p>To do so, we'll first create dataframes of the results of each.</p> <p>Then we'll plot the results of each experiment on a bar chart.</p>"},{"location":"pytorch_2_intro/#34-save-single-run-results-to-file-with-gpu-details","title":"3.4 Save single run results to file with GPU details\u00b6","text":"<p>We can save the raw data of our results to file too by exporting the dataframes as CSVs.</p> <p>We'll first create a directory for storing results.</p> <p>Then we'll create filepaths to save each of the target dataframes to before exporting them.</p>"},{"location":"pytorch_2_intro/#4-time-models-across-multiple-runs","title":"4. Time models across multiple runs\u00b6","text":"<p>Now we've tested our model with a single run with <code>torch.compile()</code> on and off, let's do the same for multiple runs.</p> <p>We're going to start by creating three functions for experiments 3 and 4.</p> <ol> <li>Experiment 3: <code>create_and_train_non_compiled_model()</code> - this function will be similar to the workflow we've used for the single runs. We'll put the model creation (via <code>create_model()</code>) and training in a single function so we can call it multiple times (for multiple runs) and measure the time of each run.</li> <li>Experiment 4: <code>create_compiled_model()</code> - this function will be similar to the <code>create_model()</code> function above, however, it will create a normal PyTorch model and then call <code>torch.compile()</code> on it and return it.</li> <li>Experiment 4: <code>train_compiled_model()</code> - this function will take in a compiled model and train it in the same way we've been training our models for single runs.</li> </ol> <p>Why separate functions 2 and 3 (<code>create_compiled_model()</code> and <code>train_compiled_model()</code>) for experiment 4?</p> <p>Because calling <code>torch.compile()</code> on model means that for the first few runs, the model will be \"warming up\" as PyTorch calculates a bunch of optimization steps behind the scenes.</p> <p>So in practice, you'll generally want to compile up front once and then train/perform inference with an already compiled model.</p>"},{"location":"pytorch_2_intro/#41-experiment-3-multiple-runs-no-compile","title":"4.1 Experiment 3 - Multiple runs, no compile\u00b6","text":"<p>Functions ready for experiment 3 and 4!</p> <p>Let's start with experiment 3.</p> Experiment Model Data Epochs Batch size Image size <code>torch.compile()</code> 3 (multi-run) ResNet50 CIFAR10 3x5 128 224 No <p>We'll set the number of runs to 3 and the number of epochs to 5.</p> <p>We'll create an empty list to store the results and append the results of each run to it after each run.</p> <p>Note: Running the following code can take quite a while depending on the speed of your GPU, for me, it took 20 minutes on a NVIDIA A100 on Google Colab Pro and around 49 minutes on a NVIDIA TITAN RTX.</p>"},{"location":"pytorch_2_intro/#42-experiment-4-multiple-runs-with-compile","title":"4.2 Experiment 4 - Multiple runs, with compile\u00b6","text":"<p>Time for experiment 4.</p> <p>Running a compiled model for multiple runs.</p> Experiment Model Data Epochs Batch size Image size <code>torch.compile()</code> 4 (multi-run) ResNet50 CIFAR10 3x5 128 224 Yes <p>We can do this by using the <code>create_compiled_model()</code> and <code>train_compiled_model()</code> functions we created earlier.</p> <p>We'll start by creating the compiled model first and then training it for 3 runs.</p> <p>We're not worried about the results of the model (loss and accuracy) as much as how long it takes.</p> <p>The reason why we compile it once at the start is that PyTorch only needs to run the optimization steps once (this can take some time) and then it can reuse them for the rest of the runs.</p> <p>We'll also create an empty list just like before to store our model's results over a series of runs.</p> <p>Note: Running the following code can take quite a while depending on the speed of your GPU, for me, it took 18 minutes on a NVIDIA A100 on Google Colab Pro and around 45 minutes on a NVIDIA TITAN RTX.</p>"},{"location":"pytorch_2_intro/#43-compare-results-of-experiment-3-and-4","title":"4.3 Compare results of experiment 3 and 4\u00b6","text":"<p>Multi-run experiments done!</p> <p>Let's inspect the results.</p> <p>We can do so with our <code>plot_mean_epoch_times()</code> function we created before.</p> <p>This time we'll set the <code>multi_runs</code> parameter to <code>True</code> so that our plots reflect the fact we're plotting the results of multiple runs.</p> <p>We'll make sure we've got a directory to save the figure to as well.</p>"},{"location":"pytorch_2_intro/#44-save-multi-run-results-to-file-with-gpu-details","title":"4.4 Save multi run results to file with GPU details\u00b6","text":"<p>Let's also save our results dataframes for experiments 3 and 4 to file to in case we'd like to inspect them later or compare them to other kinds of models.</p>"},{"location":"pytorch_2_intro/#5-possible-improvements-and-extensions","title":"5. Possible improvements and extensions\u00b6","text":"<p>We've explored the fundamentals of <code>torch.compile()</code> and wrote code for several experiments to test how it performs.</p> <p>But there's still more we could do.</p> <p>As we've discussed, many of the speedups in PyTorch 2.0 and <code>torch.compile()</code> come from using newer GPUs (e.g. A100 and above) and using as much of the GPU as possible (larger batch sizes, larger model sizes).</p> <p>For even more speedups, I'd recommend researching/trying the following:</p> <ul> <li>More powerful CPUs - I have a sneaking suspicion that Google Colab instances are limited to 2 CPU cores, speedup numbers could be improved with more CPUs. This could be tracked via the PyTorch Profiler (a tool to find what processes take what time).</li> <li>Using mixed precision training - newer GPUs have the ability to handle difference precision types (e.g. <code>torch.float16</code> and <code>torch.bfloat16</code>) which enable faster training and inference. I'd suspect you'll see an even larger speedup than we've seen here by using mixed precision training. For more on this, see the PyTorch documentation for automatic mixed precision (also called AMP) with PyTorch.</li> <li>Transformer based models may see more relative speedups than convolutional models - PyTorch 2.0 includes a stable release for accelerated transformer models (models which use the attention mechanism). The main speedups come from an improved implementation of <code>scaled_dot_product_attention()</code> which automatically selects the best version of attention to use based on the hardware you're computing on. You can see more in the dedicated PyTorch tutorial.</li> <li>Train for longer - As previously discussed, the speedups from <code>torch.compile()</code> are likely to be more noticeable when training for longer. A great exercise would be to train over a longer number of epochs, potentially on a different dataset with a different model (e.g. a transformer) and see how the speedups compare.</li> </ul>"},{"location":"pytorch_2_intro/#6-resources-to-learn-more","title":"6. Resources to learn more\u00b6","text":"<p>I've found the following resources to be helpful learning about PyTorch 2.0 and it's upcoming features.</p> <ul> <li>PyTorch 2.0 launch blog post.</li> <li>PyTorch 2.0 release notes (blog post).<ul> <li>As well as the GitHub release notes (lots of info here!).</li> </ul> </li> <li>PyTorch default device context manager docs.</li> <li>PyTorch 2.0 video introduction on YouTube (created by yours truly).</li> <li>See a tip by Sebastian Raschka to improve <code>torch.compile()</code> by performing an example batch first (warm-up the model) before continuing with further training (this explains the increased speedups with multiple runs).</li> </ul>"},{"location":"pytorch_cheatsheet/","title":"PyTorch Cheatsheet","text":"In\u00a0[1]: Copied! <pre>import torch\n\n# Check the version\nprint(f\"PyTorch version: {torch.__version__}\")\n</pre> import torch  # Check the version print(f\"PyTorch version: {torch.__version__}\") <pre>PyTorch version: 1.13.1\n</pre> In\u00a0[2]: Copied! <pre># Can also import the common abbreviation \"nn\" for \"Neural Networks\"\nfrom torch import nn\n\n# Almost everything in PyTorch is called a \"Module\" (you build neural networks by stacking together Modules)\nthis_is_a_module = nn.Linear(in_features=1,\n                             out_features=1)\nprint(type(this_is_a_module))\n</pre> # Can also import the common abbreviation \"nn\" for \"Neural Networks\" from torch import nn  # Almost everything in PyTorch is called a \"Module\" (you build neural networks by stacking together Modules) this_is_a_module = nn.Linear(in_features=1,                              out_features=1) print(type(this_is_a_module)) <pre>&lt;class 'torch.nn.modules.linear.Linear'&gt;\n</pre> In\u00a0[3]: Copied! <pre># Import PyTorch Dataset (you can store your data here) and DataLoader (you can load your data here)\nfrom torch.utils.data import Dataset, DataLoader\n</pre> # Import PyTorch Dataset (you can store your data here) and DataLoader (you can load your data here) from torch.utils.data import Dataset, DataLoader In\u00a0[4]: Copied! <pre># Create a single number tensor (scalar)\nscalar = torch.tensor(7)\n</pre> # Create a single number tensor (scalar) scalar = torch.tensor(7) In\u00a0[5]: Copied! <pre># Create a random tensor\nrandom_tensor = torch.rand(size=(3, 4)) # this will create a tensor of size 3x4 but you can manipulate the shape how you want\n</pre> # Create a random tensor random_tensor = torch.rand(size=(3, 4)) # this will create a tensor of size 3x4 but you can manipulate the shape how you want In\u00a0[6]: Copied! <pre># Multiply two random tensors\nrandom_tensor_1 = torch.rand(size=(3, 4))\nrandom_tensor_2 = torch.rand(size=(3, 4))\nrandom_tensor_3 = random_tensor_1 * random_tensor_2 # PyTorch has support for most math operators in Python (+, *, -, /)\n</pre> # Multiply two random tensors random_tensor_1 = torch.rand(size=(3, 4)) random_tensor_2 = torch.rand(size=(3, 4)) random_tensor_3 = random_tensor_1 * random_tensor_2 # PyTorch has support for most math operators in Python (+, *, -, /) In\u00a0[7]: Copied! <pre># Base computer vision library\nimport torchvision\n\n# Other components of TorchVision (premade datasets, pretrained models and image transforms)\nfrom torchvision import datasets, models, transforms\n</pre> # Base computer vision library import torchvision  # Other components of TorchVision (premade datasets, pretrained models and image transforms) from torchvision import datasets, models, transforms  In\u00a0[8]: Copied! <pre># Base text and natural language processing library\nimport torchtext\n\n# Other components of TorchText (premade datasets, pretrained models and text transforms)\nfrom torchtext import datasets, models, transforms\n</pre> # Base text and natural language processing library import torchtext  # Other components of TorchText (premade datasets, pretrained models and text transforms) from torchtext import datasets, models, transforms In\u00a0[9]: Copied! <pre># Base audio and speech processing library\nimport torchaudio\n\n# Other components of TorchAudio (premade datasets, pretrained models and text transforms)\nfrom torchaudio import datasets, models, transforms\n</pre> # Base audio and speech processing library import torchaudio  # Other components of TorchAudio (premade datasets, pretrained models and text transforms) from torchaudio import datasets, models, transforms In\u00a0[10]: Copied! <pre># # Base recommendation system library \n# import torchrec\n\n# # Other components of TorchRec\n# from torchrec import datasets, models\n</pre> # # Base recommendation system library  # import torchrec  # # Other components of TorchRec # from torchrec import datasets, models In\u00a0[11]: Copied! <pre># Setup device-agnostic code \nif torch.cuda.is_available():\n    device = \"cuda\" # NVIDIA GPU\nelif torch.backends.mps.is_available():\n    device = \"mps\" # Apple GPU\nelse:\n    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n\nprint(f\"Using device: {device}\")\n</pre> # Setup device-agnostic code  if torch.cuda.is_available():     device = \"cuda\" # NVIDIA GPU elif torch.backends.mps.is_available():     device = \"mps\" # Apple GPU else:     device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available  print(f\"Using device: {device}\") <pre>Using device: mps\n</pre> In\u00a0[12]: Copied! <pre># Create a tensor \nx = torch.tensor([1, 2, 3]) \nprint(x.device) # defaults to CPU \n\n# Send tensor to target device\nx = x.to(device)\nprint(x.device)\n</pre> # Create a tensor  x = torch.tensor([1, 2, 3])  print(x.device) # defaults to CPU   # Send tensor to target device x = x.to(device) print(x.device)  <pre>cpu\nmps:0\n</pre> In\u00a0[13]: Copied! <pre>import torch\n\n# Set the random seed (you can set this to any number you like, it will \"flavour\"\n# the randomness with that number.\ntorch.manual_seed(42)\n\n# Create two random tensors\nrandom_tensor_A = torch.rand(3, 4)\n\ntorch.manual_seed(42) # set the seed again (try commenting this out and see what happens)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"Tensor A:\\n{random_tensor_A}\\n\")\nprint(f\"Tensor B:\\n{random_tensor_B}\\n\")\nprint(f\"Does Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n</pre> import torch  # Set the random seed (you can set this to any number you like, it will \"flavour\" # the randomness with that number. torch.manual_seed(42)  # Create two random tensors random_tensor_A = torch.rand(3, 4)  torch.manual_seed(42) # set the seed again (try commenting this out and see what happens) random_tensor_B = torch.rand(3, 4)  print(f\"Tensor A:\\n{random_tensor_A}\\n\") print(f\"Tensor B:\\n{random_tensor_B}\\n\") print(f\"Does Tensor A equal Tensor B? (anywhere)\") random_tensor_A == random_tensor_B <pre>Tensor A:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor B:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n</pre> Out[13]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>You can also set the random seed on the GPU (CUDA devices).</p> In\u00a0[14]: Copied! <pre># Set random seed on GPU\ntorch.cuda.manual_seed(42)\n</pre> # Set random seed on GPU torch.cuda.manual_seed(42) In\u00a0[15]: Copied! <pre>from torch import nn\n</pre> from torch import nn In\u00a0[16]: Copied! <pre># Create a linear layer with 10 in features and out features\nlinear_layer = nn.Linear(in_features=10,\n                         out_features=10)\n</pre> # Create a linear layer with 10 in features and out features linear_layer = nn.Linear(in_features=10,                          out_features=10) In\u00a0[17]: Copied! <pre># Create an Identity layer\nidentity_layer = nn.Identity()\n</pre> # Create an Identity layer identity_layer = nn.Identity() In\u00a0[18]: Copied! <pre># Create a Conv1d layer (often used for text with a singular dimension)\nconv1d = nn.Conv1d(in_channels=1,\n                   out_channels=10,\n                   kernel_size=3)\n</pre> # Create a Conv1d layer (often used for text with a singular dimension) conv1d = nn.Conv1d(in_channels=1,                    out_channels=10,                    kernel_size=3) In\u00a0[19]: Copied! <pre># Create a Conv2d layer (often used for images with Height x Width dimensions)\nconv2d = nn.Conv2d(in_channels=3, # 3 channels for color images (red, green, blue)\n                   out_channels=10,\n                   kernel_size=3)\n</pre> # Create a Conv2d layer (often used for images with Height x Width dimensions) conv2d = nn.Conv2d(in_channels=3, # 3 channels for color images (red, green, blue)                    out_channels=10,                    kernel_size=3)                    In\u00a0[20]: Copied! <pre># Create a Conv3d layer (often used for video with Height x Width x Time dimensions)\nconv3d = nn.Conv3d(in_channels=3,\n                   out_channels=10,\n                   kernel_size=3)\n</pre> # Create a Conv3d layer (often used for video with Height x Width x Time dimensions) conv3d = nn.Conv3d(in_channels=3,                    out_channels=10,                    kernel_size=3) In\u00a0[21]: Copied! <pre># Create a Transformer model (model based on the paper \"Attention Is All You Need\" - https://arxiv.org/abs/1706.03762)\ntransformer_model = nn.Transformer()\n</pre> # Create a Transformer model (model based on the paper \"Attention Is All You Need\" - https://arxiv.org/abs/1706.03762) transformer_model = nn.Transformer() In\u00a0[22]: Copied! <pre># Create a single Transformer encoder cell\ntransformer_encoder = nn.TransformerEncoderLayer(d_model=768, # embedding dimension\n                                                 nhead=12) # number of attention heads\n</pre> # Create a single Transformer encoder cell transformer_encoder = nn.TransformerEncoderLayer(d_model=768, # embedding dimension                                                  nhead=12) # number of attention heads In\u00a0[23]: Copied! <pre># Stack together Transformer encoder cells\ntransformer_encoder_stack = nn.TransformerEncoder(encoder_layer=transformer_encoder, # from above\n                                                  num_layers=6) # 6 Transformer encoders stacked on top of each other\n</pre> # Stack together Transformer encoder cells transformer_encoder_stack = nn.TransformerEncoder(encoder_layer=transformer_encoder, # from above                                                   num_layers=6) # 6 Transformer encoders stacked on top of each other In\u00a0[24]: Copied! <pre># Create a single Transformer decoder cell\ntransformer_decoder = nn.TransformerDecoderLayer(d_model=768,\n                                                 nhead=12)\n</pre> # Create a single Transformer decoder cell transformer_decoder = nn.TransformerDecoderLayer(d_model=768,                                                  nhead=12) In\u00a0[25]: Copied! <pre># Stack together Transformer decoder cells\ntransformer_decoder_stack = nn.TransformerDecoder(decoder_layer=transformer_decoder, # from above\n                                                  num_layers=6) # 6 Transformer decoders stacked on top of each other\n</pre> # Stack together Transformer decoder cells transformer_decoder_stack = nn.TransformerDecoder(decoder_layer=transformer_decoder, # from above                                                   num_layers=6) # 6 Transformer decoders stacked on top of each other In\u00a0[26]: Copied! <pre># Create a single LSTM cell\nlstm_cell = nn.LSTMCell(input_size=10, # can adjust as necessary\n                        hidden_size=10) # can adjust as necessary\n</pre> # Create a single LSTM cell lstm_cell = nn.LSTMCell(input_size=10, # can adjust as necessary                         hidden_size=10) # can adjust as necessary In\u00a0[27]: Copied! <pre># Stack together LSTM cells\nlstm_stack = nn.LSTM(input_size=10,\n                     hidden_size=10,\n                     num_layers=3) # 3 single LSTM cells stacked on top of each other\n</pre> # Stack together LSTM cells lstm_stack = nn.LSTM(input_size=10,                      hidden_size=10,                      num_layers=3) # 3 single LSTM cells stacked on top of each other In\u00a0[28]: Copied! <pre># Create a single GRU cell\ngru_cell = nn.GRUCell(input_size=10, # can adjust as necessary\n                      hidden_size=10) # can adjust as necessary\n</pre> # Create a single GRU cell gru_cell = nn.GRUCell(input_size=10, # can adjust as necessary                       hidden_size=10) # can adjust as necessary In\u00a0[29]: Copied! <pre># Stack together GRU cells\ngru_stack = nn.GRU(input_size=10, \n                   hidden_size=10,\n                   num_layers=3) # 3 single GRU cells stacked on top of each other\n</pre> # Stack together GRU cells gru_stack = nn.GRU(input_size=10,                     hidden_size=10,                    num_layers=3) # 3 single GRU cells stacked on top of each other  In\u00a0[30]: Copied! <pre># ReLU\nrelu = nn.ReLU()\n\n# Sigmoid\nsigmoid = nn.Sigmoid()\n\n# Softmax\nsoftmax = nn.Softmax()\n</pre> # ReLU relu = nn.ReLU()  # Sigmoid sigmoid = nn.Sigmoid()  # Softmax softmax = nn.Softmax() In\u00a0[31]: Copied! <pre># L1Loss\nloss_fn = nn.L1Loss() # also known as MAE or mean absolute error\n\n# MSELoss\nloss_fn = nn.MSELoss() # also known as MSE or mean squared error\n\n# Binary cross entropy (for binary classification problems)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Cross entropy (for multi-class classification problems)\nloss_fn = nn.CrossEntropyLoss()\n</pre> # L1Loss loss_fn = nn.L1Loss() # also known as MAE or mean absolute error  # MSELoss loss_fn = nn.MSELoss() # also known as MSE or mean squared error  # Binary cross entropy (for binary classification problems) loss_fn = nn.BCEWithLogitsLoss()  # Cross entropy (for multi-class classification problems) loss_fn = nn.CrossEntropyLoss() In\u00a0[32]: Copied! <pre># Create a baseline model\nmodel = nn.Transformer()\n\n# SGD (stochastic gradient descent)\noptimizer = torch.optim.SGD(lr=0.1, # set the learning rate (required)\n                            params=model.parameters()) # tell the optimizer what parameters to optimize\n</pre> # Create a baseline model model = nn.Transformer()  # SGD (stochastic gradient descent) optimizer = torch.optim.SGD(lr=0.1, # set the learning rate (required)                             params=model.parameters()) # tell the optimizer what parameters to optimize In\u00a0[33]: Copied! <pre># Create a baseline model\nmodel = nn.Transformer()\n\n# Adam optimizer\noptimizer = torch.optim.Adam(lr=0.001, # set the learning rate (required)\n                             params=model.parameters()) # tell the optimizer what parameters to optimize\n</pre> # Create a baseline model model = nn.Transformer()  # Adam optimizer optimizer = torch.optim.Adam(lr=0.001, # set the learning rate (required)                              params=model.parameters()) # tell the optimizer what parameters to optimize In\u00a0[34]: Copied! <pre># Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1) # data\ny = weight * X + bias # labels (want model to learn from data to predict these)\n\nX[:10], y[:10]\n</pre> # Create *known* parameters weight = 0.7 bias = 0.3  # Create data start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) # data y = weight * X + bias # labels (want model to learn from data to predict these)  X[:10], y[:10] Out[34]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> In\u00a0[35]: Copied! <pre># Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Create train/test split train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[35]: <pre>(40, 40, 10, 10)</pre> In\u00a0[36]: Copied! <pre>from torch import nn\n\n# Option 1 - subclass torch.nn.Module\nclass LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\nmodel_0 = LinearRegressionModel()\nmodel_0, model_0.state_dict()\n</pre> from torch import nn  # Option 1 - subclass torch.nn.Module class LinearRegressionModel(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  model_0 = LinearRegressionModel() model_0, model_0.state_dict() Out[36]: <pre>(LinearRegressionModel(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.5025]])),\n              ('linear_layer.bias', tensor([-0.0722]))]))</pre> <p>Now let's create the same model as above but using <code>torch.nn.Sequential</code>.</p> In\u00a0[37]: Copied! <pre>from torch import nn\n\n# Option 2 - use torch.nn.Sequential\nmodel_1 = torch.nn.Sequential(\n    nn.Linear(in_features=1,\n              out_features=1))\n\nmodel_1, model_1.state_dict()\n</pre> from torch import nn  # Option 2 - use torch.nn.Sequential model_1 = torch.nn.Sequential(     nn.Linear(in_features=1,               out_features=1))  model_1, model_1.state_dict() Out[37]: <pre>(Sequential(\n   (0): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('0.weight', tensor([[0.9905]])), ('0.bias', tensor([0.9053]))]))</pre> In\u00a0[38]: Copied! <pre># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Create loss function loss_fn = nn.L1Loss()  # Create optimizer optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) In\u00a0[40]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, an error will happen (not all data on target device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\n# Put model on the available device\n# With this, an error will happen (the model is not on target device)\nmodel_1 = model_1.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Set the number of epochs  epochs = 1000   # Put data on the available device # Without this, an error will happen (not all data on target device) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  # Put model on the available device # With this, an error will happen (the model is not on target device) model_1 = model_1.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <pre>Epoch: 0 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 100 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 200 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 300 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 400 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 500 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 600 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 700 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 800 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 900 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\n</pre>"},{"location":"pytorch_cheatsheet/#pytorch-cheatsheet","title":"PyTorch Cheatsheet\u00b6","text":"<p>Some of the most commonly used commands/setups in PyTorch.</p> <p>Note: One of the best ways to get help for PyTorch specific functions and use cases is to  search \"pytorch how to make a convolutional neural network\" or \"pytorch transformer layers\" or \"pytorch loss functions\". I do this regularly.</p>"},{"location":"pytorch_cheatsheet/#imports","title":"Imports\u00b6","text":"<p>You can install PyTorch on various platforms via the PyTorch installation page.</p>"},{"location":"pytorch_cheatsheet/#data-imports","title":"Data imports\u00b6","text":"<p>Since most of machine learning is finding patterns in data, it's good to know how to work with datasets in PyTorch.</p>"},{"location":"pytorch_cheatsheet/#creating-tensors","title":"Creating Tensors\u00b6","text":"<p>One of the main use cases of PyTorch is for accelerated deep learning computing.</p> <p>And deep learning usually involves the manipulation of large tensors (big, multi-dimensional collections of numbers).</p> <p>PyTorch has a number of methods to create tensors.</p> <p>Note: For a more extensive overview of creating tensors with PyTorch, see 00. PyTorch Fundamentals.</p>"},{"location":"pytorch_cheatsheet/#domain-libraries","title":"Domain Libraries\u00b6","text":"<p>Depending on the specific problem you're working on, PyTorch has several domain libraries.</p> <ul> <li>TorchVision \u2014 PyTorch\u2019s resident computer vision library.</li> <li>TorchText \u2014 PyTorch\u2019s in-built domain library for text.</li> <li>TorchAudio \u2014 PyTorch\u2019s domain library for everything audio.</li> <li>TorchRec \u2014 PyTorch\u2019s newest in-built domain library for powering recommendation engines with deep learning.</li> </ul>"},{"location":"pytorch_cheatsheet/#computer-vision","title":"Computer Vision\u00b6","text":"<p>Note: For an in-depth overview of computer vision in PyTorch, see 03. PyTorch Computer Vision.</p>"},{"location":"pytorch_cheatsheet/#text-and-natural-language-processing-nlp","title":"Text and Natural Language Processing (NLP)\u00b6","text":""},{"location":"pytorch_cheatsheet/#audio-and-speech","title":"Audio and Speech\u00b6","text":""},{"location":"pytorch_cheatsheet/#recommendation-systems","title":"Recommendation systems\u00b6","text":"<p>Note: This library is currently in beta release, see the GitHub page for installation.</p>"},{"location":"pytorch_cheatsheet/#device-agnostic-code-using-pytorch-on-cpu-gpu-or-mps","title":"Device-agnostic code (using PyTorch on CPU, GPU or MPS)\u00b6","text":"<p>Much of deep learning involves computing on tensors.</p> <p>Computing on tensors generally happens much faster on GPUs (graphics processing units, typically from NVIDIA) than CPUs (computer processing units).</p> <p>MPS stands for \"Metal Performance Shader\" which is Apple's GPU (M1, M1 Pro, M2 etc).</p> <p>It is advised to perform training on the fastest piece of hardware you have available, which will generally be: NVIDIA GPU (<code>\"cuda\"</code>) &gt; MPS device (<code>\"mps\"</code>) &gt; CPU (<code>\"cpu\"</code>).</p> <ul> <li>For more on seeing how to get PyTorch to run on NVIDIA GPU (with CUDA), see 00. PyTorch Fundamentals section 2: getting PyTorch to run on the GPU.</li> <li>For more on running PyTorch using an MPS backend (running PyTorch on Mac GPUs) see the PyTorch documentaiton.</li> </ul> <p>Note: It is advised to setup device-agnostic code at the start of your workflow.</p>"},{"location":"pytorch_cheatsheet/#sending-a-tensor-to-target-device","title":"Sending a tensor to target device\u00b6","text":"<p>You can move objects (models and tensors) in PyTorch to different devices via the <code>.to(\"device_name\")</code> method.</p>"},{"location":"pytorch_cheatsheet/#setting-random-seeds","title":"Setting random seeds\u00b6","text":"<p>A lot of machine learning and deep learning involves taking random numbers in tensors and then shaping those random numbers to find/represent patterns in real data.</p> <p>However, sometimes you'll want \"reproducible\" randomness.</p> <p>To do so, you can set the random seeds, see Reproducibility (trying to take the random out of random) for more.</p>"},{"location":"pytorch_cheatsheet/#neural-networks","title":"Neural Networks\u00b6","text":"<p>PyTorch has a very comprehensive library of pre-built neural network components (many of these are referred to as \"modules\" in the PyTorch ecosystem).</p> <p>At a fundamental level neural networks are stacks of layers. Each of these layers performs some kind of operation on an input and produces an output.</p> <p>How these layers stack together will depend on the problem you're working on.</p> <p>One of the most active areas of research in machine learnin is how to stack neural network layers together (and the best answer to this is constantly changing).</p> <p>The vast majority of neural network components in PyTorch are contained within the <code>torch.nn</code> package (<code>nn</code> is short for neural networks).</p>"},{"location":"pytorch_cheatsheet/#linear-layers","title":"Linear layers\u00b6","text":"<p>PyTorch has several in-built linear layers.</p>"},{"location":"pytorch_cheatsheet/#convolutional-layers-for-making-convolutional-neural-networks-or-cnns","title":"Convolutional Layers (for making Convolutional Neural Networks or CNN's)\u00b6","text":"<p>PyTorch has several in-built convolutional layers.</p> <p>Naming of convolutional layers usually follows <code>torch.nn.ConvXd</code> where <code>X</code> can be a value of <code>1</code>, <code>2</code> or <code>3</code>.</p> <p>The <code>X</code> value represents the number of dimensions the convolution will operate over, for example, <code>1</code> for singular dimension text, <code>2</code> for two dimension images (height x width) and <code>3</code> for 3D objects such as video (video is considered a series of images with a time dimension, height x width x time).</p> <p>Note: You can see more on building convolutional neural networks for computer vision with PyTorch in 03. PyTorch Computer Vision section 7.2: building a convolutional neural network (CNN).</p>"},{"location":"pytorch_cheatsheet/#transformer-layers-for-making-transformer-models","title":"Transformer Layers (for making Transformer models)\u00b6","text":"<p>PyTorch has in-built Transformer layers as described in the paper Attention Is All You Need.</p> <p>Using in-built PyTorch Transformer layers has the benefit of potential speedups thanks to PyTorch's BetterTransformer.</p> <p>Note: You can see the use of PyTorch's in-built Transformer layers to build a Vision Transformer in 08. PyTorch Paper Replicating.</p>"},{"location":"pytorch_cheatsheet/#recurrent-layers-for-making-recurrent-neural-networks-or-rnns","title":"Recurrent Layers (for making Recurrent Neural Networks or RNN's)\u00b6","text":"<p>PyTorch has in-built support for Recurrent Neural Network layers such as long short-term memory (LSTM) and gated recurrent unit (GRU).</p>"},{"location":"pytorch_cheatsheet/#activation-functions","title":"Activation Functions\u00b6","text":"<p>Activation functions often go between layers in a neural network to add non-linear (non-straight) capabilities to linear (straight) functions.</p> <p>In essence, a neural network is often comprised of a large amount of linear and non-linear functions.</p> <p>PyTorch has several non-linear activation functions built into <code>torch.nn</code>.</p> <p>Some of the most common are:</p> <ul> <li><code>nn.ReLU</code> - also known as rectified linear unit.</li> <li><code>nn.Sigmoid</code> - also known as the sigmoid function.</li> <li><code>nn.Softmax</code> - also known as the softmax function.</li> </ul> <p>Note: See 02. PyTorch Neural Network Classification section 6: non-linearity, the missing piece for more.</p>"},{"location":"pytorch_cheatsheet/#loss-functions","title":"Loss Functions\u00b6","text":"<p>A loss function measures how wrong your model is. As in, how far are its predictions off where they should be.</p> <p>Ideally, with training, data and an optimization function, this loss value goes as low as possible.</p> <p>Loss functions in PyTorch (and deep learning in general) are also often referred to as: criterion, cost function.</p> <p>PyTorch has several loss functions built into <code>torch.nn</code>.</p> <p>And some of the most common are:</p> <ul> <li><code>nn.L1Loss</code> - also referred to as MAE or mean absolute error (this loss is often used for regression problems or predicting a number such as the price of houses).</li> <li><code>nn.MSELoss</code> - also referred to as L2Loss or mean squared error (this loss is often used for regression problems or predicting a number such as the price of houses).</li> <li><code>nn.BCEWithLogitsLoss</code> - also known as binary cross entropy this loss function is often used for binary classification probelms (classifying something as one thing or another).</li> <li><code>nn.CrossEntropyLoss</code> - this loss function is often used for multi-class classification problems (classifying something as one thing or another).</li> </ul>"},{"location":"pytorch_cheatsheet/#optimizers","title":"Optimizers\u00b6","text":"<p>An optimizer's job is to change the neural network weights in such a way that it reduces the loss function value.</p> <p>PyTorch has several optimization functions built into the <code>torch.optim</code> module.</p> <p>Two of the main optimizer functions include:</p> <ul> <li><code>torch.optim.SGD(lr=0.1, params=model.parameters())</code> - SGD also known as stochastic gradient descent (<code>lr</code> stands for \"learning rate\", the multiplier of how much to modify neural network weights at each step, small value = small adjustments, big value = big adjustments).</li> <li><code>torch.optim.Adam(lr=0.001, params=model.parameters())</code> - The Adam optimizer (<code>params</code> stands for \"model parameters\", in other words, the model parameters/weights you'd like the optimization function to optimize during training).</li> </ul>"},{"location":"pytorch_cheatsheet/#end-to-end-example-workflow","title":"End-to-end example workflow\u00b6","text":"<p>Let's put everything together in a quick end-to-end workflow.</p> <p>This workflow has been taken from 01. PyTorch Workflow Fundamentals.</p>"},{"location":"pytorch_cheatsheet/#create-data","title":"Create data\u00b6","text":""},{"location":"pytorch_cheatsheet/#create-a-model","title":"Create a model\u00b6","text":"<p>Two main ways to create a model in PyTorch:</p> <ol> <li>Subclass <code>torch.nn.Module</code> - more code but can be very flexible, models that subclass <code>torch.nn.Module</code> must implement a <code>forward()</code> method.</li> <li>Use <code>torch.nn.Sequential</code> - less code but less flexibility.</li> </ol>"},{"location":"pytorch_cheatsheet/#setup-loss-function-and-optimizer","title":"Setup loss function and optimizer\u00b6","text":""},{"location":"pytorch_cheatsheet/#create-a-trainingtesting-loop","title":"Create a training/testing loop\u00b6","text":"<p>Our goal is to reduce the loss of our model (how much our model's predictions are different to the actual data).</p> <p>If our training/testing loops are implemented right and the model is capable of learning patterns in the data, the training and test losses should go down.</p> <p>See the following for steps in a PyTorch training loop:</p> <ul> <li>PyTorch optimization loop song</li> <li>PyTorch Workflow Fundamentals Section 3: Training Loop</li> <li>PyTorch Workflow Fundamentals Section 3: Testing Loop</li> <li>PyTorch Workflow Fundamentals Section 6.3: Training</li> </ul>"},{"location":"pytorch_cheatsheet/#extras","title":"Extras\u00b6","text":"<p>The above list is not exhaustive.</p> <p>Here are some good places to find out more:</p> <ul> <li>PyTorch official cheatsheet.</li> <li>Zero to Mastery Learn PyTorch course - a comprehensive yet beginner-friendly deep dive into using PyTorch for deep learning all the way from the fundamentals to deploying a model to the real-world so other people can use it.</li> <li>PyTorch performance tuning guide - a resource from the PyTorch team on how to tune performance of PyTorch models.</li> <li>PyTorch Extra Resources - a curated list of helpful resources to extend PyTorch and learn more about the engineering side of things around deep learning.</li> <li>Effective PyTorch by vahidk - a GitHub repo with a fantastic overview of some of the main functionality in PyTorch in a straight-foward manner.</li> </ul>"},{"location":"pytorch_extra_resources/","title":"PyTorch Extra Resources","text":"<p>Despite the full Zero to Mastery PyTorch course being over 40 hours, you\u2019ll likely finish being excited to learn more.</p> <p>After all, the course is a PyTorch momentum builder.</p> <p>The following resources are collected to extend the course.</p> <p>A warning though: there\u2019s a lot here.</p> <p>Best to choose 1 or 2 resources from each section (or less) to explore more. And put the rest in your bag for later. </p> <p>Which one's the best? </p> <p>Well, if they\u2019ve made it on this list, you can consider them a quality resource.</p> <p>Most are PyTorch-specific, fitting extensions to the course but a couple are non PyTorch-specific, however, they\u2019re still valuable in the world of machine learning.</p>"},{"location":"pytorch_extra_resources/#pure-pytorch-resources","title":"\ud83d\udd25\u00a0Pure PyTorch resources","text":"<ul> <li>PyTorch blog \u2014 Stay up to date on the latest from PyTorch right from the source. I check the blog once a month or so for updates.</li> <li>PyTorch documentation \u2014 We\u2019ll have explored this plenty throughout the course but there\u2019s still a large amount we haven\u2019t touched. No trouble, explore often and when necessary.</li> <li>PyTorch Performance Tuning Guide \u2014 One of the first things you\u2019ll likely want to do after the course is to make your PyTorch models faster (training and inference), the PyTorch Performance Tuning Guide helps you do just that.</li> <li>PyTorch Recipes \u2014 PyTorch recipes is a collection of small tutorials to showcase common PyTorch features and workflows you may want to create, such as Loading Data in PyTorch and Saving and Loading models for Inference in PyTorch.</li> <li>PyTorch Ecosystem - A vast collection of tools that build on top of pure PyTorch to add specialized features for different fields, from PyTorch3D for 3D computer vision to Albumentations for fast data augmentation to TorchMetrics for model evaluation (thank you for the tip Alessandro).</li> <li>Setting up PyTorch in VSCode \u2014 VSCode is one of the most popular IDEs out there. And its PyTorch support is getting better and better. Throughout the Zero to Mastery PyTorch course, we use Google Colab because of its ease of use. But chances are you\u2019ll be developing in an IDE like VSCode soon.</li> </ul>"},{"location":"pytorch_extra_resources/#libraries-that-make-pure-pytorch-betteradd-features","title":"\ud83d\udcc8\u00a0Libraries that make pure PyTorch better/add features","text":"<p>The course focuses on pure PyTorch (using minimal external libraries) because if you know how to write plain PyTorch, you can learn to use the various extension libraries.</p> <ul> <li>fast.ai \u2014 fastai is an open-source library that takes care of many of the boring parts of building neural networks and makes creating state-of-the-art models possible with a few lines of code. Their free library, course and documentation are all world-class.</li> <li>MosaicML for more efficient model training \u2014 The faster you can train models, the faster you can figure out what works and what doesn\u2019t. MosaicML\u2019s open-source <code>Composer</code> library helps you train neural networks with PyTorch faster by implementing speedup algorithms behind the scenes which means you can get better results out of your existing PyTorch models faster. All of their code is open-source and their docs are fantastic.</li> <li>PyTorch Lightning for reducing boilerplate \u2014 PyTorch Lightning takes care of many of the steps that you often have to do by hand in vanilla PyTorch, such as writing a training and test loop, model checkpointing, logging and more. PyTorch Lightning builds on top of PyTorch to allow you to make PyTorch models with less code.</li> </ul> <p></p> <p>Libraries that extend/make pure PyTorch better.</p>"},{"location":"pytorch_extra_resources/#books-for-pytorch","title":"\ud83d\udcd6\u00a0Books for PyTorch","text":"<ul> <li>Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python by Sebastian Raschka \u2014 A fantastic introduction to machine learning and deep learning. Starting with traditional machine learning algorithms using Scikit-Learn for problems with structured data (tabular or rows and columns or Excel-style) and then switching to how to use PyTorch for deep learning on unstructured data (such as computer vision and natural language processing).</li> <li>PyTorch Step-by-Step series by Daniel Voigt Godoy \u2014 Where the Zero to Mastery PyTorch course works from a code-first perspective, the Step-by-Step series covers PyTorch and deep learning from a concept-first perspective with code examples to go along. With three editions, Fundamentals, Computer Vision and Sequences (NLP), the step-by-step series is one of my favourite resources for learning PyTorch from the ground up.</li> <li>Dive into Deep Learning book \u2014 Possibly one of the most comprehensive resources on the internet for deep learning concepts along with code examples in PyTorch, TensorFlow and Gluon. And all for free! For example, take a look at the author\u2019s explanation of the Vision Transformer we cover in 08. PyTorch Paper Replicating.</li> <li>Bonus: The fast.ai course (available free online) also comes as a freely available online book, Deep Learning for Coders with fastai &amp; PyTorch.</li> </ul> <p>Textbooks to learn more about PyTorch as well as deep learning in general.</p>"},{"location":"pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering","title":"\ud83c\udfd7\u00a0Resources for Machine Learning and Deep Learning Engineering","text":"<p>Machine Learning Engineering (also referred to as MLOps or ML operations) is the practice of getting the models you create into the hands of others. This may mean via a public app or working behind the scenes to make business decisions.</p> <p>The following resources will help you learn more about the steps around deploying a machine learning model.</p> <ul> <li>Designing Machine Learning Systems book by Chip Huyen \u2014 If you want to build an ML system, it\u2019d be good to know how others have done it. Chip\u2019s book focuses less on building a single machine learning model (though there\u2019s plenty of content on that in the book) but rather building a cohesive ML system. It covers everything from data engineering to model building to model deployment (online and offline) to model monitoring. Even better, it\u2019s a joy to read, you can tell the book is written by a writer (Chip has previously authored several books).</li> <li>Made With ML by Goku Mohandas \u2014 Whenever I want to learn or reference something to do with MLOps, I go to madewithml.com/mlops and see if there\u2019s a lesson on it. Made with ML not only teaches you the  fundamentals of many different ML models but goes through how to build an end-to-end ML system with plenty of code and tooling examples.</li> <li>The Machine Learning Engineering book by Andriy Burkov \u2014 Even though this book is available to read online for free, I bought it as soon as it came out. I\u2019ve used it as a reference and to learn more about ML engineering so much it\u2019s basically always on my desk/within arms reach. Burkov does an excellent job at getting to the point and referencing further materials when necessary.</li> <li>Full Stack Deep Learning course \u2014 I first did this course in 2021. And it\u2019s continued to evolve to cover the latest and greatest tools in the field. It\u2019ll teach you how to plan a project to solve an ML problem, how to source or create data, how to troubleshoot an ML project when it goes wrong and most of all, how to build ML-powered products.</li> </ul> <p></p> <p>Resources to improve your machine learning engineering skills (all of the steps that go around building a machine learning model).</p>"},{"location":"pytorch_extra_resources/#where-to-find-datasets","title":"\ud83d\uddc3\u00a0Where to find datasets","text":"<p>Machine learning projects begin with data. </p> <p>No data, no ML. </p> <p>The following resources are some of the best for finding open-source and often ready-to-use datasets on a wide range of topics and problem domains.</p> <ul> <li>Paperswithcode Datasets \u2014 Search for the most used and common machine learning benchmark datasets, understand what they contain, where they came from and where they can be found. You can often also see the current best-performing model on each dataset.</li> <li>HuggingFace Datasets \u2014 Not just a resource to find datasets across a wide range of problem domains but also a library to download and start using them within a few lines of code.</li> <li>Kaggle Datasets \u2014 Find all kinds of datasets that usually accompany Kaggle Competitions, many of which come straight out of industry.</li> <li>Google Dataset search \u2014 Just like searching Google but specifically for datasets.</li> </ul> <p>These should be plenty to get started, however, for your own specific problems you\u2019ll likely want to build your own dataset.</p> <p></p> <p>Places to find existing and open-source datasets for a variety of problem spaces.</p>"},{"location":"pytorch_extra_resources/#tools-for-deep-learning-domains","title":"Tools for Deep Learning Domains","text":"<p>The following resources are focused on libraries and pretrained models for specific problem domains such as computer vision and recommendation engines/systems.</p>"},{"location":"pytorch_extra_resources/#computer-vision","title":"\ud83d\ude0e\u00a0Computer Vision","text":"<p>We cover computer vision in 03. PyTorch Computer Vision but as a quick recap, computer vision is the art of getting computers to see. </p> <p>If your data is visual, images, x-rays, production line video or even hand-written documents, it may be a computer vision problem.</p> <ul> <li>TorchVision \u2014 PyTorch\u2019s resident computer vision library. Find plenty of methods for loading vision data as well as plenty of pretrained computer vision models to use for your own problems.</li> <li>timm (Torch Image Models) library \u2014 One of the most comprehensive computer vision libraries and resources for pretrained computer vision models. Almost all new research in that uses PyTorch for computer vision leverages the <code>timm</code> library in some way.</li> <li>Yolov5 for object detection \u2014 If you\u2019re looking to build an object detection model in PyTorch, the <code>yolov5</code> GitHub repository might be the quickest way to get started.</li> <li>VISSL (Vision Self-Supervised Learning) library \u2014 Self-supervised learning is the art of getting data to learn patterns in itself. Rather than providing labels for different classes and learning a representation like that, self-supervised learning tries to replicate similar results without labels. VISSL provides an easy to use way to get started using self-supervised learning computer vision models with PyTorch.</li> </ul>"},{"location":"pytorch_extra_resources/#natural-language-processing-nlp","title":"\ud83d\udcda\u00a0Natural Language Processing (NLP)","text":"<p>Natural language processing involves finding patterns in text. </p> <p>For example, you might want to extract important entities in support tickets or classify a document into different categories.</p> <p>If your problem involves a large of amount of text, you\u2019ll want to look into the following resources.</p> <ul> <li>TorchText \u2014 PyTorch\u2019s in-built domain library for text. Like TorchVision, it contains plenty of pre-built methods for loading data and a healthy collection of pretrained models you can adapt to your own problems.</li> <li>HuggingFace Transformers library \u2014 The HuggingFace Transformers library has more stars on GitHub than the PyTorch library itself. And there\u2019s a reason. Not that HuggingFace Transformers is better than PyTorch but because it\u2019s the best at what it does: provide data loaders and pretrained state-of-the-art models for NLP (and a whole bunch more).</li> <li>Bonus: To learn more about how to HuggingFace Transformers library and all of the pieces around it, the HuggingFace team offer a free online course.</li> </ul>"},{"location":"pytorch_extra_resources/#speech-and-audio","title":"\ud83c\udfa4\u00a0Speech and Audio","text":"<p>If your problem deals with audio files or speech data, such as trying to classify a sound or transcribe speech into text, you\u2019ll want to look into the following resources.</p> <ul> <li>TorchAudio \u2014 PyTorch\u2019s domain library for everything audio. Find in-built methods for preparing data and pre-built model architectures for finding patterns in audio data.</li> <li>SpeechBrain \u2014 An open-source library built on top of PyTorch to handle speech problems such as recognition (turning speech into text), speech enhancement, speech processing, text-to-speech and more. You can try out many of their models on the HuggingFace Hub.</li> </ul>"},{"location":"pytorch_extra_resources/#recommendation-engines","title":"\u2753Recommendation Engines","text":"<p>The internet is powered by recommendations. YouTube recommends videos, Netflix recommends movies and TV shows, Amazon recommends products, Medium recommends articles.</p> <p>If you\u2019re building an online store or online marketplace, chances are you\u2019ll want to start recommending things to your customers.</p> <p>For that, you\u2019ll want to look into building a recommendation engine. </p> <ul> <li>TorchRec \u2014 PyTorch\u2019s newest in-built domain library for powering recommendation engines with deep learning. TorchRec comes with recommendation datasets and models ready to try and use. Though if a custom recommendation egnine isn\u2019t up to par with what you\u2019re after (or too much work), many cloud vendors offer recommendation engine services.</li> </ul>"},{"location":"pytorch_extra_resources/#time-series","title":"\u23f3\u00a0Time Series","text":"<p>If your data has a time component and you\u2019d like to leverage patterns from the past to predict the future, such as, predicting the price of Bitcoin next year (don\u2019t try this, stock forecasting is BS) or a more reasonable problem of predicting electricity demand for a city next week, you\u2019ll want to look into time series libraries.</p> <p>Both of these libraries don\u2019t necessarily use PyTorch, however, since time series is such a common problem, I\u2019ve included them here.</p> <ul> <li>Salesforce Merlion \u2014 Turn your time series data into intelligence by using Merlion\u2019s data loaders, pre-built models, AutoML (automated machine learning) hyperparameter tuning and more for time series forecasting and time series anomaly detection all inspired by practical use cases.</li> <li>Facebook Kats \u2014 Facebook\u2019s entire business depends on prediction: when\u2019s the best time to place an advertisement? So you can bet they\u2019re invested heavily in their time series prediction software. Kats (Kit to Analyze Time Series data) is their open-source library for time series forecasting, detection and data processing.</li> </ul>"},{"location":"pytorch_extra_resources/#how-to-get-a-job","title":"\ud83d\udc69\u200d\ud83d\udcbb\u00a0How to get a job","text":"<p>Once you\u2019ve finished an ML course, it\u2019s likely you\u2019ll want to use your ML skills.</p> <p>And even better, get paid for them.</p> <p>The following resources are good guides on what to do to get one.</p> <ul> <li>\"How can a beginner data scientist like me gain experience?\" by Daniel Bourke \u2014 I get the question of \u201chow do I get experience?\u201d often because many different job requirements state \u201cexperience needed\u201d. Well, it turns out one of the best ways to get experience (and a job) is to: start the job before you have it.</li> <li>You Don\u2019t Really Need Another MOOC by Eugene Yan \u2014 MOOC stands for massive online open course (or something similar). MOOCs are beautiful. They enable people all over the world at their own pace. However, it can be tempting to just continually do MOOCs over and over again thinking \u201cif I just do one more, I\u2019ll be ready\u201d. The truth is, a few is enough, the returns of a MOOC quickly start to trail off. Instead, go off the trail, start to build, start to create, start to learn skills that can\u2019t be taught. Showcase those skills to get a job.</li> <li>Bonus: For the most thorough resource on the internet for machine learning interviews, check out Chip Huyen\u2019s free Introduction to Machine Learning Interviews book.</li> </ul>"},{"location":"pytorch_most_common_errors/","title":"The Three Most Common Errors in PyTorch","text":"In\u00a0[1]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") <pre>PyTorch version: 1.12.1+cu113\n</pre> In\u00a0[2]: Copied! <pre># Create two tensors\ntensor_1 = torch.rand(3, 4)\ntensor_2 = torch.rand(3, 4)\n\n# Check the shapes\nprint(tensor_1.shape)\nprint(tensor_2.shape)\n</pre> # Create two tensors tensor_1 = torch.rand(3, 4) tensor_2 = torch.rand(3, 4)  # Check the shapes print(tensor_1.shape) print(tensor_2.shape) <pre>torch.Size([3, 4])\ntorch.Size([3, 4])\n</pre> <p>Notice both tensors have the same shape.</p> <p>Let's try to perform a matrix multiplication on them.</p> <p>Note: The matrix multiplication operation is different to a standard multiplication operation.</p> <p>With our current tensors, the standard multiplication operation (<code>*</code> or <code>torch.mul()</code>) will work where as the matrix multiplication operation (<code>@</code> or <code>torch.matmul()</code>) will error.</p> <p>See 00. PyTorch Fundamentals: Matrix Multiplication for a breakdown of what happens in matrix multiplication.</p> In\u00a0[3]: Copied! <pre># Standard multiplication, the following lines perform the same operation (will work)\ntensor_3 = tensor_1 * tensor_2 # can do standard multiplication with \"*\"\ntensor_4 = torch.mul(tensor_1, tensor_2) # can also do standard multiplicaton with \"torch.mul()\" \n\n# Check for equality \ntensor_3 == tensor_4\n</pre> # Standard multiplication, the following lines perform the same operation (will work) tensor_3 = tensor_1 * tensor_2 # can do standard multiplication with \"*\" tensor_4 = torch.mul(tensor_1, tensor_2) # can also do standard multiplicaton with \"torch.mul()\"   # Check for equality  tensor_3 == tensor_4 Out[3]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>Wonderful! Looks like standard multiplication works with our current tensor shapes.</p> <p>Let's try matrix multiplication.</p> In\u00a0[4]: Copied! <pre># Try matrix multiplication (won't work)\ntensor_5 = tensor_1 @ tensor_2 # could also do \"torch.matmul(tensor_1, tensor_2)\"\n</pre> # Try matrix multiplication (won't work) tensor_5 = tensor_1 @ tensor_2 # could also do \"torch.matmul(tensor_1, tensor_2)\" <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [4], in &lt;cell line: 2&gt;()\n      1 # Try matrix multiplication (won't work)\n----&gt; 2 tensor_5 = tensor_1 @ tensor_2\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)</pre> <p>Oh no!</p> <p>We get an error similar to the following:</p> <pre><code>RuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-2ca2c90dbb42&gt; in &lt;module&gt;\n      1 # Try matrix multiplication (won't work)\n----&gt; 2 tensor_5 = tensor_1 @ tensor_2\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)\n</code></pre> <p>This is a shape error, our two tensors (matrices) can't be matrix multiplied because their shapes are incompatible.</p> <p>Why?</p> <p>This is because matrix multiplication has specific rules:</p> <ol> <li>The inner dimensions must match:</li> </ol> <ul> <li><code>(3, 4) @ (3, 4)</code> won't work</li> <li><code>(4, 3) @ (3, 4)</code> will work</li> <li><code>(3, 4) @ (4, 3)</code> will work</li> </ul> <ol> <li>The resulting matrix has the shape of the outer dimensions:</li> </ol> <ul> <li><code>(4, 3) @ (3, 4)</code> -&gt; <code>(4, 4)</code></li> <li><code>(3, 4) @ (4, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>So how do we fix it?</p> <p>This where either a transpose or a reshape comes in.</p> <p>And in the case of neural networks, it's more generally a transpose operation.</p> <ul> <li>Transpose - The transpose (<code>torch.transpose()</code>) operation swaps the dimensions of a given tensor.<ul> <li>Note: You can also use the shortcut of <code>tensor.T</code> to perform a transpose.</li> </ul> </li> <li>Reshape - The reshape (<code>torch.reshape()</code>) operation returns a tensor with the same number of original elements but in a different specified shape.</li> </ul> <p>Let's see this in action.</p> In\u00a0[5]: Copied! <pre># Perform a transpose on tensor_1 and then perform matrix multiplication \ntensor_6 = tensor_1.T @ tensor_2\nprint(f\"Shape of input tensors: {tensor_1.T.shape} and {tensor_2.shape}\")\nprint(f\"Shape of output tensor: {tensor_6.shape}\")\n</pre> # Perform a transpose on tensor_1 and then perform matrix multiplication  tensor_6 = tensor_1.T @ tensor_2 print(f\"Shape of input tensors: {tensor_1.T.shape} and {tensor_2.shape}\") print(f\"Shape of output tensor: {tensor_6.shape}\") <pre>Shape of input tensors: torch.Size([4, 3]) and torch.Size([3, 4])\nShape of output tensor: torch.Size([4, 4])\n</pre> <p>No errors!</p> <p>See how the input shape of <code>tensor_1</code> changed from <code>(3, 4)</code> to <code>(4, 3)</code> thanks to the transpose (<code>tensor_1.T</code>).</p> <p>And because of this, rule 1 of matrix multiplication, the inner dimensions must match was satisfied.</p> <p>Finally, the output shape satisfied rule 2 of matrix multiplication, the resulting matrix has the shape of the outer dimensions.</p> <p>In our case, <code>tensor_6</code> has a shape of <code>(4, 4)</code>.</p> <p>Let's do the same operation except now we'll transpose <code>tensor_2</code> instead of <code>tensor_1</code>.</p> In\u00a0[6]: Copied! <pre># Perform a transpose on tensor_2 and then perform matrix multiplication\ntensor_7 = tensor_1 @ tensor_2.T\nprint(f\"Shape of input tensors: {tensor_1.shape} and {tensor_2.T.shape}\")\nprint(f\"Shape of output tensor: {tensor_7.shape}\")\n</pre> # Perform a transpose on tensor_2 and then perform matrix multiplication tensor_7 = tensor_1 @ tensor_2.T print(f\"Shape of input tensors: {tensor_1.shape} and {tensor_2.T.shape}\") print(f\"Shape of output tensor: {tensor_7.shape}\") <pre>Shape of input tensors: torch.Size([3, 4]) and torch.Size([4, 3])\nShape of output tensor: torch.Size([3, 3])\n</pre> <p>Woohoo!</p> <p>No errors again!</p> <p>See how rule 1 and rule 2 of matrix multiplication were satisfied again.</p> <p>Except this time because we transposed <code>tensor_2</code>, the resulting output tensor shape is <code>(3, 3)</code>.</p> <p>The good news is most of the time, when you build neural networks with PyTorch, the library takes care of most of the matrix multiplication operations you'll need to perform for you.</p> <p>With that being said, let's build a neural network with PyTorch and see where shape errors might occur.</p> In\u00a0[7]: Copied! <pre>import torchvision\nfrom torchvision import datasets, transforms\n\n# Setup training data\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=transforms.ToTensor()\n)\n</pre> import torchvision from torchvision import datasets, transforms  # Setup training data train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Setup testing data test_data = datasets.FashionMNIST(     root=\"data\",     train=False, # get test data     download=True,     transform=transforms.ToTensor() ) <pre>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>  0%|          | 0/26421880 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>  0%|          | 0/29515 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>  0%|          | 0/4422102 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>  0%|          | 0/5148 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\n</pre> <p>Now let's get some details about the first training sample, the label as well as the class names and number of classes.</p> In\u00a0[8]: Copied! <pre># See first training sample\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape} -&gt; [batch, height, width]\") \nprint(f\"Label: {label}\") # label is an int rather than a tensor (it has no shape attribute)\n</pre> # See first training sample image, label = train_data[0] print(f\"Image shape: {image.shape} -&gt; [batch, height, width]\")  print(f\"Label: {label}\") # label is an int rather than a tensor (it has no shape attribute) <pre>Image shape: torch.Size([1, 28, 28]) -&gt; [batch, height, width]\nLabel: 9\n</pre> <p>Our image has a shape of <code>[1, 28, 28]</code> or <code>[batch_size, height, width]</code>.</p> In\u00a0[9]: Copied! <pre># See class names and number of classes\nclass_names = train_data.classes\nnum_classes = len(class_names)\nclass_names, num_classes\n</pre> # See class names and number of classes class_names = train_data.classes num_classes = len(class_names) class_names, num_classes Out[9]: <pre>(['T-shirt/top',\n  'Trouser',\n  'Pullover',\n  'Dress',\n  'Coat',\n  'Sandal',\n  'Shirt',\n  'Sneaker',\n  'Bag',\n  'Ankle boot'],\n 10)</pre> In\u00a0[10]: Copied! <pre># Plot a sample\nimport matplotlib.pyplot as plt\nplt.imshow(image.squeeze(), cmap=\"gray\") # plot image as grayscale\nplt.axis(False)\nplt.title(class_names[label]);\n</pre> # Plot a sample import matplotlib.pyplot as plt plt.imshow(image.squeeze(), cmap=\"gray\") # plot image as grayscale plt.axis(False) plt.title(class_names[label]); In\u00a0[11]: Copied! <pre>from torch import nn\n\n# Create a two layer neural network\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=10)\n)\n\n# Pass the image through the model (this will error)\nmodel_0(image)\n</pre> from torch import nn  # Create a two layer neural network model_0 = nn.Sequential(     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=10) )  # Pass the image through the model (this will error) model_0(image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [11], in &lt;cell line: 10&gt;()\n      4 model_0 = nn.Sequential(\n      5     nn.Linear(in_features=10, out_features=10),\n      6     nn.Linear(in_features=10, out_features=10)\n      7 )\n      9 # Pass the image through the model (this will error)\n---&gt; 10 model_0(image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)</pre> <p>Running the above code we get another shape error!</p> <p>Something similar to:</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)\n</code></pre> <p>The key is in the final line <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)</code>.</p> <p>This is telling us there's something wrong with our data shapes.</p> <p>Because behind the scenes, <code>nn.Linear()</code> is attempting to do a matrix multiplication.</p> <p>How do we fix this?</p> <p>There are several different options depending on what kind of layer(s) you're using.</p> <p>But since we're using <code>nn.Linear()</code> layers, let's focus on that.</p> <p><code>nn.Linear()</code> likes to accept data as a single-dimension vector .</p> <p>For example, instead of an input <code>image</code> shape of <code>[1, 28, 28]</code>, it would prefer <code>[1, 784]</code> (<code>784 = 28*28</code>).</p> <p>In other words, it likes all of the information to be flattened into a single dimension.</p> <p>We can achieve this flattening using PyTorch's <code>nn.Flatten()</code>.</p> <p>Let's see it happen.</p> In\u00a0[12]: Copied! <pre># Create a flatten layer\nflatten = nn.Flatten()\n\n# Pass the image through the flatten layer\nflattened_image = flatten(image)\n\n# Print out the image shape before and after \nprint(f\"Before flatten shape: {image.shape} -&gt; [batch, height, width]\")\nprint(f\"After flatten shape: {flattened_image.shape} -&gt; [batch, height*width]\")\n</pre> # Create a flatten layer flatten = nn.Flatten()  # Pass the image through the flatten layer flattened_image = flatten(image)  # Print out the image shape before and after  print(f\"Before flatten shape: {image.shape} -&gt; [batch, height, width]\") print(f\"After flatten shape: {flattened_image.shape} -&gt; [batch, height*width]\") <pre>Before flatten shape: torch.Size([1, 28, 28]) -&gt; [batch, height, width]\nAfter flatten shape: torch.Size([1, 784]) -&gt; [batch, height*width]\n</pre> <p>Wonderful, image data flattened!</p> <p>Now let's try adding the <code>nn.Flatten()</code> layer to our existing model.</p> In\u00a0[13]: Copied! <pre># Replicate model_0 except add a nn.Flatten() layer to begin with \nmodel_1 = nn.Sequential(\n    nn.Flatten(), # &lt;-- NEW: add nn.Flatten() layer\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=10)\n)\n\n# Pass the image through the model\nmodel_1(image)\n</pre> # Replicate model_0 except add a nn.Flatten() layer to begin with  model_1 = nn.Sequential(     nn.Flatten(), # &lt;-- NEW: add nn.Flatten() layer     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=10) )  # Pass the image through the model model_1(image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [13], in &lt;cell line: 9&gt;()\n      2 model_1 = nn.Sequential(\n      3     nn.Flatten(), # &lt;-- NEW: add nn.Flatten() layer\n      4     nn.Linear(in_features=10, out_features=10),\n      5     nn.Linear(in_features=10, out_features=10)\n      6 )\n      8 # Pass the image through the model\n----&gt; 9 model_1(image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)</pre> <p>Oh no!</p> <p>Another error...</p> <p>Something like:</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)\n</code></pre> <p>Again, the key information is in the bottom line.</p> <p><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)</code></p> <p>Hmm, we know the <code>(1x784)</code> must be coming from our input data (<code>image</code>) since we flattened it from <code>(1, 28, 28)</code> -&gt; <code>(1, 784)</code>.</p> <p>How about the <code>(10x10)</code>?</p> <p>These values come from the parameters we set in our <code>nn.Linear()</code> layers, <code>in_features=10</code> and <code>out_features=10</code> or <code>nn.Linear(in_features=10, out_features=10)</code>.</p> <p>What was the first rule of matrix multiplication again?</p> <ol> <li>The inner dimensions must match.</li> </ol> <p>Right!</p> <p>So what happens if we change <code>in_features=10</code> to <code>in_features=784</code> in the first layer?</p> <p>Let's find out!</p> In\u00a0[14]: Copied! <pre># Flatten the input as well as make sure the first layer can accept the flattened input shape\nmodel_2 = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(in_features=784, out_features=10), # &lt;-- NEW: change in_features=10 to in_features=784\n    nn.Linear(in_features=10, out_features=10)\n)\n\n# Pass the image through the model\nmodel_2(image)\n</pre> # Flatten the input as well as make sure the first layer can accept the flattened input shape model_2 = nn.Sequential(     nn.Flatten(),     nn.Linear(in_features=784, out_features=10), # &lt;-- NEW: change in_features=10 to in_features=784     nn.Linear(in_features=10, out_features=10) )  # Pass the image through the model model_2(image) Out[14]: <pre>tensor([[-0.2045,  0.2677, -0.0713, -0.3096, -0.0586,  0.3153, -0.3413,  0.2031,\n          0.4421,  0.1715]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>It worked!</p> <p>We got an output from our model!</p> <p>The output might not mean much for now but at least we know all of the shapes line up and data can flow all the through our model.</p> <p>The <code>nn.Flatten()</code> layer turned our input image from <code>(1, 28, 28)</code> to <code>(1, 784)</code> and our first <code>nn.Linear(in_features=784, out_features=10)</code> layer could accept it as input.</p> In\u00a0[15]: Copied! <pre># Create a model with incorrect input and output shapes between layers\nmodel_3 = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(in_features=784, out_features=10), # out_features=10 \n    nn.Linear(in_features=5, out_features=10) # &lt;-- NEW: in_features does not match the out_features of the previous layer\n)\n\n# Pass the image through the model (this will error)\nmodel_3(image)\n</pre> # Create a model with incorrect input and output shapes between layers model_3 = nn.Sequential(     nn.Flatten(),     nn.Linear(in_features=784, out_features=10), # out_features=10      nn.Linear(in_features=5, out_features=10) # &lt;-- NEW: in_features does not match the out_features of the previous layer )  # Pass the image through the model (this will error) model_3(image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [15], in &lt;cell line: 9&gt;()\n      2 model_3 = nn.Sequential(\n      3     nn.Flatten(),\n      4     nn.Linear(in_features=784, out_features=10), # out_features=10 \n      5     nn.Linear(in_features=5, out_features=10) # &lt;-- NEW: in_features does not match the out_features of the previous layer\n      6 )\n      8 # Pass the image through the model (this will error)\n----&gt; 9 model_3(image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)</pre> <p>Running the model above we get the following error:</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)\n</code></pre> <p>Once again, we've broken rule 1 of matrix multiplication, the inner dimensions must match.</p> <p>Our first <code>nn.Linear()</code> layer outputs a shape of <code>(1, 10)</code> but our second <code>nn.Linear()</code> layer is expecting a shape of <code>(1, 5)</code>.</p> <p>How could we fix this?</p> <p>Well, we could set <code>in_features=10</code> for the second <code>nn.Linear()</code> manually, or we could try one of the newer features of PyTorch, \"lazy\" layers.</p> In\u00a0[16]: Copied! <pre># Try nn.LazyLinear() as the second layer\nmodel_4 = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(in_features=784, out_features=10),\n    nn.LazyLinear(out_features=10) # &lt;-- NEW: no in_features parameter as this is inferred from the previous layer's output\n)\n\n# Pass the image through the model\nmodel_4(image)\n</pre> # Try nn.LazyLinear() as the second layer model_4 = nn.Sequential(     nn.Flatten(),     nn.Linear(in_features=784, out_features=10),     nn.LazyLinear(out_features=10) # &lt;-- NEW: no in_features parameter as this is inferred from the previous layer's output )  # Pass the image through the model model_4(image) <pre>/home/daniel/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n</pre> Out[16]: <pre>tensor([[ 0.4282,  0.2492, -0.2045, -0.4943, -0.1639,  0.1166,  0.3828, -0.1283,\n         -0.1771, -0.2277]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>It works (though there may be a warning depending on the version of PyTorch you're using, if so, don't worry, it's just to say the <code>Lazy</code> layers are still in development)!</p> <p>How about we try replacing all the <code>nn.Linear()</code> layers with <code>nn.LazyLinear()</code> layers?</p> <p>Then we'll only have to set the <code>out_features</code> values for each.</p> In\u00a0[18]: Copied! <pre># Replace all nn.Linear() layers with nn.LazyLinear()\nmodel_5 = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=10),\n    nn.LazyLinear(out_features=10) # &lt;-- NEW \n)\n\n# Pass the image through the model\nmodel_5(image)\n</pre> # Replace all nn.Linear() layers with nn.LazyLinear() model_5 = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=10),     nn.LazyLinear(out_features=10) # &lt;-- NEW  )  # Pass the image through the model model_5(image) Out[18]: <pre>tensor([[ 0.1375, -0.2175, -0.1054,  0.1424, -0.1406, -0.1180, -0.0896, -0.4285,\n         -0.0077, -0.3188]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>Nice!</p> <p>It worked again, our image was able to flow through the network without any issues.</p> <p>Note: The above examples only deal with one type of layer in PyTorch, <code>nn.Linear()</code>, however, the principles of lining up input and output shapes with each layer is a constant throughout all neural networks and different types of data.</p> <p>Layers like <code>nn.Conv2d()</code>, used in convolutional neural networks (CNNs) can even accept inputs without the use of <code>nn.Flatten()</code>. You can see more on this in 03. PyTorch Computer Vision section 7: Building a CNN.</p> In\u00a0[19]: Copied! <pre>import torch\n\n# Set device to \"cuda\" if it's available otherwise default to \"cpu\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Current device: {device}\")\n</pre> import torch  # Set device to \"cuda\" if it's available otherwise default to \"cpu\" device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Current device: {device}\") <pre>Current device: cuda\n</pre> <p>Now let's create a model with the same layers as <code>model_5</code>.</p> <p>In PyTorch, models and tensors are created on the CPU by default.</p> <p>We can test this by checking the <code>device</code> attribute of the model we create.</p> In\u00a0[20]: Copied! <pre>from torch import nn\n\n# Create a model (similar to model_5 above)\nmodel_6 = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=10), \n    nn.LazyLinear(out_features=10)\n)\n\n# All models and tensors are created on the CPU by default (unless explicitly set otherwise)\nprint(f\"Model is on device: {next(model_6.parameters()).device}\")\n</pre> from torch import nn  # Create a model (similar to model_5 above) model_6 = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=10),      nn.LazyLinear(out_features=10) )  # All models and tensors are created on the CPU by default (unless explicitly set otherwise) print(f\"Model is on device: {next(model_6.parameters()).device}\") <pre>Model is on device: cpu\n</pre> In\u00a0[21]: Copied! <pre>from torch.utils.data import DataLoader, RandomSampler\n\n# Only sample 10% of the data\ntrain_sampler = RandomSampler(train_data, \n                              num_samples=int(0.1*len(train_data)))\n\ntest_sampler = RandomSampler(test_data, \n                             num_samples=int(0.1*len(test_data)))\n\nprint(f\"Number of random training samples selected: {len(train_sampler)}/{len(train_data)}\")\nprint(f\"Number of random testing samples selected: {len(test_sampler)}/{len(test_data)}\")\n\n# Create DataLoaders and turn data into batches\nBATCH_SIZE = 32\ntrain_dataloader = DataLoader(dataset=train_data,\n                              batch_size=BATCH_SIZE,\n                              sampler=train_sampler)\n\ntest_dataloader = DataLoader(dataset=test_data,\n                             batch_size=BATCH_SIZE,\n                             sampler=test_sampler)\n\nprint(f\"Number of batches in train_dataloader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\nprint(f\"Number of batches in test_dataloader: {len(test_dataloader)} batch of size {BATCH_SIZE}\")\n\n# Create loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(lr=0.01, \n                            params=model_6.parameters())\n</pre> from torch.utils.data import DataLoader, RandomSampler  # Only sample 10% of the data train_sampler = RandomSampler(train_data,                                num_samples=int(0.1*len(train_data)))  test_sampler = RandomSampler(test_data,                               num_samples=int(0.1*len(test_data)))  print(f\"Number of random training samples selected: {len(train_sampler)}/{len(train_data)}\") print(f\"Number of random testing samples selected: {len(test_sampler)}/{len(test_data)}\")  # Create DataLoaders and turn data into batches BATCH_SIZE = 32 train_dataloader = DataLoader(dataset=train_data,                               batch_size=BATCH_SIZE,                               sampler=train_sampler)  test_dataloader = DataLoader(dataset=test_data,                              batch_size=BATCH_SIZE,                              sampler=test_sampler)  print(f\"Number of batches in train_dataloader: {len(train_dataloader)} batches of size {BATCH_SIZE}\") print(f\"Number of batches in test_dataloader: {len(test_dataloader)} batch of size {BATCH_SIZE}\")  # Create loss function loss_fn = nn.CrossEntropyLoss()  # Create optimizer optimizer = torch.optim.SGD(lr=0.01,                              params=model_6.parameters()) <pre>Number of random training samples selected: 6000/60000\nNumber of random testing samples selected: 1000/10000\nNumber of batches in train_dataloader: 188 batches of size 32\nNumber of batches in test_dataloader: 32 batch of size 32\n</pre> In\u00a0[22]: Copied! <pre>from tqdm.auto import tqdm\n\n# Set the number of epochs\nepochs = 5\n\n# Train the model\nfor epoch in tqdm(range(epochs)):\n\n    # Set loss to 0 every epoch\n    train_loss = 0\n\n    # Get images (X) and labels (y)\n    for X, y in train_dataloader:\n\n        # Forward pass\n        y_pred = model_6(X)\n\n        # Calculate the loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n\n        # Optimizer zero grad\n        optimizer.zero_grad()\n\n        # Loss backward\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n  \n    # Print loss in the epoch loop only\n    print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")\n</pre> from tqdm.auto import tqdm  # Set the number of epochs epochs = 5  # Train the model for epoch in tqdm(range(epochs)):      # Set loss to 0 every epoch     train_loss = 0      # Get images (X) and labels (y)     for X, y in train_dataloader:          # Forward pass         y_pred = model_6(X)          # Calculate the loss         loss = loss_fn(y_pred, y)         train_loss += loss          # Optimizer zero grad         optimizer.zero_grad()          # Loss backward         loss.backward()          # Optimizer step         optimizer.step()        # Print loss in the epoch loop only     print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0 | Training loss: 334.65\nEpoch: 1 | Training loss: 215.44\nEpoch: 2 | Training loss: 171.15\nEpoch: 3 | Training loss: 154.72\nEpoch: 4 | Training loss: 142.22\n</pre> <p>Nice! Looks like our training loop is working!</p> <p>Our model's loss is going down (the lower the loss the better).</p> In\u00a0[23]: Copied! <pre># Send model_6 to the target device (\"cuda\")\nmodel_6.to(device)\n\n# Print out what device the model is on\nprint(f\"Model is on device: {next(model_6.parameters()).device}\")\n</pre> # Send model_6 to the target device (\"cuda\") model_6.to(device)  # Print out what device the model is on print(f\"Model is on device: {next(model_6.parameters()).device}\") <pre>Model is on device: cuda:0\n</pre> <p>Our <code>model_6</code> is on the <code>\"cuda:0\"</code> (where <code>0</code> is the index of the device, in case there was more than one GPU) device.</p> <p>Now let's run the same training loop code as above and see what happens.</p> <p>Can you guess?</p> In\u00a0[24]: Copied! <pre>from tqdm.auto import tqdm\n\n# Set the number of epochs\nepochs = 5\n\n# Train the model\nfor epoch in tqdm(range(epochs)):\n\n  # Set loss to 0 every epoch\n  train_loss = 0\n\n  # Get images (X) and labels (y)\n  for X, y in train_dataloader:\n\n    # Forward pass\n    y_pred = model_6(X) # model is on GPU, data is on CPU (will error)\n\n    # Calculate the loss\n    loss = loss_fn(y_pred, y)\n    train_loss += loss\n    \n    # Optimizer zero grad\n    optimizer.zero_grad()\n\n    # Loss backward\n    loss.backward()\n\n    # Optimizer step\n    optimizer.step()\n  \n  # Print loss in the epoch loop only\n  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")\n</pre> from tqdm.auto import tqdm  # Set the number of epochs epochs = 5  # Train the model for epoch in tqdm(range(epochs)):    # Set loss to 0 every epoch   train_loss = 0    # Get images (X) and labels (y)   for X, y in train_dataloader:      # Forward pass     y_pred = model_6(X) # model is on GPU, data is on CPU (will error)      # Calculate the loss     loss = loss_fn(y_pred, y)     train_loss += loss          # Optimizer zero grad     optimizer.zero_grad()      # Loss backward     loss.backward()      # Optimizer step     optimizer.step()      # Print loss in the epoch loop only   print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [24], in &lt;cell line: 7&gt;()\n     12 # Get images (X) and labels (y)\n     13 for X, y in train_dataloader:\n     14 \n     15   # Forward pass\n---&gt; 16   y_pred = model_6(X) # model is on GPU, data is on CPU (will error)\n     18   # Calculate the loss\n     19   loss = loss_fn(y_pred, y)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre> <p>Whoops!</p> <p>Looks like we go a device error:</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n</code></pre> <p>We can see the error states <code>Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</code>.</p> <p>In essence, our model is on the <code>cuda:0</code> device but our data tensors (<code>X</code> and <code>y</code>) are still on the <code>cpu</code> device.</p> <p>But PyTorch expects all tensors to be on the same device.</p> In\u00a0[25]: Copied! <pre># Send the model to the target device (we don't need to do this again but we will for completeness)\nmodel_6.to(device)\n\n# Set the number of epochs\nepochs = 5\n\n# Train the model\nfor epoch in tqdm(range(epochs)):\n\n  # Set loss to 0 every epoch\n  train_loss = 0\n\n  # Get images (X) and labels (y)\n  for X, y in train_dataloader:\n\n    # Put target data on target device  &lt;-- NEW\n    X, y = X.to(device), y.to(device) # &lt;-- NEW: send data to target device\n\n    # Forward pass\n    y_pred = model_6(X)\n\n    # Calculate the loss\n    loss = loss_fn(y_pred, y)\n    train_loss += loss\n    \n    # Optimizer zero grad\n    optimizer.zero_grad()\n\n    # Loss backward\n    loss.backward()\n\n    # Optimizer step\n    optimizer.step()\n  \n  # Print loss in the epoch loop only\n  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")\n</pre> # Send the model to the target device (we don't need to do this again but we will for completeness) model_6.to(device)  # Set the number of epochs epochs = 5  # Train the model for epoch in tqdm(range(epochs)):    # Set loss to 0 every epoch   train_loss = 0    # Get images (X) and labels (y)   for X, y in train_dataloader:      # Put target data on target device  &lt;-- NEW     X, y = X.to(device), y.to(device) # &lt;-- NEW: send data to target device      # Forward pass     y_pred = model_6(X)      # Calculate the loss     loss = loss_fn(y_pred, y)     train_loss += loss          # Optimizer zero grad     optimizer.zero_grad()      # Loss backward     loss.backward()      # Optimizer step     optimizer.step()      # Print loss in the epoch loop only   print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0 | Training loss: 134.76\nEpoch: 1 | Training loss: 127.76\nEpoch: 2 | Training loss: 120.85\nEpoch: 3 | Training loss: 120.50\nEpoch: 4 | Training loss: 116.29\n</pre> <p>Excellent!</p> <p>Our training loop completes just as before because now both our model and data tensors are on the same device.</p> <p>Note: Libraries like HuggingFace Accelerate are a fantastic way to train your PyTorch models with minimal explicit device setting (they discover the best device to use and set things up for you).</p> <p>You could also write functions to ensure your training code happens all on the same device, see 05. PyTorch Going Modular section 4: Creating training functions for more.</p> In\u00a0[26]: Copied! <pre># Get a single sample from the test dataset\ntest_image, test_label = test_data.data[0], test_data.targets[0]\nprint(f\"Test image shape: {test_image.shape}\")\nprint(f\"Test image label: {test_label}\")\n</pre> # Get a single sample from the test dataset test_image, test_label = test_data.data[0], test_data.targets[0] print(f\"Test image shape: {test_image.shape}\") print(f\"Test image label: {test_label}\") <pre>Test image shape: torch.Size([28, 28])\nTest image label: 9\n</pre> In\u00a0[27]: Copied! <pre># Plot test image\nimport matplotlib.pyplot as plt\nplt.imshow(test_image, cmap=\"gray\")\nplt.axis(False)\nplt.title(class_names[test_label]);\n</pre> # Plot test image import matplotlib.pyplot as plt plt.imshow(test_image, cmap=\"gray\") plt.axis(False) plt.title(class_names[test_label]); <p>Looking good!</p> <p>Now let's try to make a prediction on it by passing it to our <code>model_6</code>.</p> In\u00a0[28]: Copied! <pre># Pass the test image through model_6 to make a prediction\nmodel_6(test_image)\n</pre> # Pass the test image through model_6 to make a prediction model_6(test_image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [28], in &lt;cell line: 2&gt;()\n      1 # Pass the test image through model_6 to make a prediction\n----&gt; 2 model_6(test_image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre> <p>Dam!</p> <p>We get another device error.</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n</code></pre> <p>This is because our <code>model_6</code> is on the GPU (<code>\"cuda\"</code>), however, our <code>test_image</code> is on the CPU (in PyTorch, all tensors are on the CPU by default).</p> <p>Let's send the <code>test_image</code> to the target <code>device</code> and then try the prediction again.</p> In\u00a0[30]: Copied! <pre># Send test_image to target device\nmodel_6(test_image.to(device))\n</pre> # Send test_image to target device model_6(test_image.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [30], in &lt;cell line: 2&gt;()\n      1 # Send test_image to target device\n----&gt; 2 model_6(test_image.to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)</pre> <p>Oh no! Another error...</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)\n</code></pre> <p>This time it's a shape error.</p> <p>We've seen these before.</p> <p>What's going on with our <code>test_image</code> shape?</p> <p>Perhaps it's because our model was trained on images that had a batch dimension?</p> <p>And our current <code>test_image</code> doesn't have a batch dimension?</p> <p>Here's another helpful rule of thumb to remember: trained models like to predict on data in the same format and shape that they were trained on.</p> <p>This means if our model was trained on images with a batch dimension, it'll tend to like to predict on images with a batch dimension, even if the batch dimension is only 1 (a single sample).</p> <p>And if our model was trained on data in the format <code>torch.float32</code> (or another format), it'll like to predict on data in that same format (we'll see this later on).</p> <p>We can add a single batch dimension to our <code>test_image</code> using the <code>torch.unsqueeze()</code> method.</p> In\u00a0[31]: Copied! <pre># Changing the input size to be the same as what the model was trained on\noriginal_input_shape = test_image.shape\nupdated_input_shape = test_image.unsqueeze(dim=0).shape # adding a batch dimension on the \"0th\" dimension\n\n# Print out shapes of original tensor and updated tensor\nprint(f\"Original input data shape: {original_input_shape} -&gt; [height, width]\")\nprint(f\"Updated input data shape (with added batch dimension): {updated_input_shape} -&gt; [batch, height, width]\")\n</pre> # Changing the input size to be the same as what the model was trained on original_input_shape = test_image.shape updated_input_shape = test_image.unsqueeze(dim=0).shape # adding a batch dimension on the \"0th\" dimension  # Print out shapes of original tensor and updated tensor print(f\"Original input data shape: {original_input_shape} -&gt; [height, width]\") print(f\"Updated input data shape (with added batch dimension): {updated_input_shape} -&gt; [batch, height, width]\") <pre>Original input data shape: torch.Size([28, 28]) -&gt; [height, width]\nUpdated input data shape (with added batch dimension): torch.Size([1, 28, 28]) -&gt; [batch, height, width]\n</pre> <p>Nice!</p> <p>We've found a way to add a batch dimension to our <code>test_image</code>.</p> <p>Let's try make a prediction on it again.</p> In\u00a0[32]: Copied! <pre># Make prediction on test image with additional batch size dimension and with it on the target device\nmodel_6(test_image.unsqueeze(dim=0).to(device))\n</pre> # Make prediction on test image with additional batch size dimension and with it on the target device model_6(test_image.unsqueeze(dim=0).to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [32], in &lt;cell line: 2&gt;()\n      1 # Make prediction on test image with additional batch size dimension and with it on the target device\n----&gt; 2 model_6(test_image.unsqueeze(dim=0).to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: expected scalar type Float but found Byte</pre> <p>What?</p> <p>Another error!</p> <p>This time it's a datatype error:</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: expected scalar type Float but found Byte\n</code></pre> <p>We've stumbled upon the third most common error in PyTorch, datatype errors.</p> <p>Let's figure out how to fix them in the next section.</p> In\u00a0[33]: Copied! <pre># Get a single sample from the train_dataloader and print the dtype\ntrain_image_batch, train_label_batch = next(iter(train_dataloader))\ntrain_image_single, train_label_single = train_image_batch[0], train_label_batch[0]\n\n# Print the datatype of the train_image_single\nprint(f\"Datatype of training data: {train_image_single.dtype}\")\n</pre> # Get a single sample from the train_dataloader and print the dtype train_image_batch, train_label_batch = next(iter(train_dataloader)) train_image_single, train_label_single = train_image_batch[0], train_label_batch[0]  # Print the datatype of the train_image_single print(f\"Datatype of training data: {train_image_single.dtype}\") <pre>Datatype of training data: torch.float32\n</pre> <p>There we go! We confirmed our training data samples are in <code>torch.float32</code>.</p> <p>So it makes sense that our <code>model_6</code> wants to predict on this datatype.</p> <p>But how did our training data get in that datatype?</p> <p>It happened back in section 1.3 when we downloaded the Fashion MNIST dataset and used the <code>transform</code> parameter of <code>torchvision.transforms.ToTensor()</code>.</p> <p>This <code>transform</code> converts whatever data is passed to it into a <code>torch.Tensor</code> with the default datatype <code>torch.float32</code>.</p> <p>So another rule of thumb: when making predictions, whatever transforms you performed on the training data, you should also perform on the testing data.</p> In\u00a0[34]: Copied! <pre># Print out the original datatype of test_image\nprint(f\"Original datatype: {test_image.unsqueeze(dim=0).dtype}\")\n\n# Change the datatype of test_image and see the change\nprint(f\"Changing the datatype: {test_image.unsqueeze(dim=0).type(torch.float32).dtype}\")\n</pre> # Print out the original datatype of test_image print(f\"Original datatype: {test_image.unsqueeze(dim=0).dtype}\")  # Change the datatype of test_image and see the change print(f\"Changing the datatype: {test_image.unsqueeze(dim=0).type(torch.float32).dtype}\") <pre>Original datatype: torch.uint8\nChanging the datatype: torch.float32\n</pre> In\u00a0[35]: Copied! <pre># Make a prediction with model_6 on the transformed test_image\npred_on_gpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n                      .type(torch.float32) # convert the datatype to torch.float32\n                      .to(device)) # send the tensor to the target device\npred_on_gpu\n</pre> # Make a prediction with model_6 on the transformed test_image pred_on_gpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension                       .type(torch.float32) # convert the datatype to torch.float32                       .to(device)) # send the tensor to the target device pred_on_gpu Out[35]: <pre>tensor([[ -963.8352, -1658.8182,  -735.9952, -1285.2964,  -550.3845,   949.4190,\n          -538.1960,  1123.0616,   552.7371,  1413.8110]], device='cuda:0',\n       grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>Woohoo!!!</p> <p>A fair few steps but our <code>model_6</code> successfully makes a prediction on <code>test_image</code>.</p> <p>Since <code>test_image</code> is on the CPU by default, we could also put the model back on the CPU using the <code>.cpu()</code> method and make the same prediction on the CPU device instead of the GPU device.</p> In\u00a0[36]: Copied! <pre># Put model back on CPU\nmodel_6.cpu()\n \n# Make a prediction on the CPU device (no need to put test_image on the CPU as it's already there)\npred_on_cpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n                      .type(torch.float32)) # convert the datatype to torch.float32 \npred_on_cpu\n</pre> # Put model back on CPU model_6.cpu()   # Make a prediction on the CPU device (no need to put test_image on the CPU as it's already there) pred_on_cpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension                       .type(torch.float32)) # convert the datatype to torch.float32  pred_on_cpu Out[36]: <pre>tensor([[ -963.8351, -1658.8182,  -735.9953, -1285.2964,  -550.3845,   949.4189,\n          -538.1960,  1123.0615,   552.7371,  1413.8110]],\n       grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>And again the prediction works!</p> <p>Is it correct?</p> <p>We can check by the taking the model's raw outputs and converting them from <code>raw logits -&gt; prediction probabilities -&gt; prediction label</code> (see 02. PyTorch Neural Network Classification section 3.1 for more on this conversion).</p> In\u00a0[37]: Copied! <pre># Convert raw logits to prediction probabilities\npred_probs = torch.softmax(pred_on_cpu, dim=1)\n\n# Convert prediction probabilities to prediction label\npred_label = torch.argmax(pred_probs, dim=1)\n\n# Check if it's correct\nprint(f\"Test label: {test_label}\")\nprint(f\"Pred label: {pred_label}\")\nprint(f\"Is the prediction correct? {pred_label.item() == test_label}\")\n</pre> # Convert raw logits to prediction probabilities pred_probs = torch.softmax(pred_on_cpu, dim=1)  # Convert prediction probabilities to prediction label pred_label = torch.argmax(pred_probs, dim=1)  # Check if it's correct print(f\"Test label: {test_label}\") print(f\"Pred label: {pred_label}\") print(f\"Is the prediction correct? {pred_label.item() == test_label}\") <pre>Test label: 9\nPred label: tensor([9])\nIs the prediction correct? True\n</pre> <p>There can a fair few steps involved when making predictions on a test or custom sample.</p> <p>So one of the ways to prevent repeating all of these steps is to turn them into a function.</p> <p>There's an example of this in 04. PyTorch Custom Datasets section 11.3: Building a function to predict on custom images.</p>"},{"location":"pytorch_most_common_errors/#the-three-most-common-errors-in-pytorch","title":"The Three Most Common Errors in PyTorch\u00b6","text":"<p>PyTorch is one of the largest machine learning libraries available.</p> <p>So it's likely you'll run into various errors when using it.</p> <p>Because of the various maintenance and checks performed by the creators, it's rare the error will be because of the library itself.</p> <p>This means the majority of the errors you run into will be user errors.</p> <p>More specifically, you wrote the wrong code.</p> <p>Don't be offended, this happens to every programmer.</p> <p>Of the user errors you run into, chances are they'll be one of the following:</p> <ol> <li>Shape errors - You're trying to perform an operation on matrices/tensors with shapes that don't line up. For example, your data's shape is <code>[1, 28, 28]</code> but your first layer takes an input of <code>[10]</code>.</li> <li>Device errors - Your model is on a different device to your data. For example your model is on the GPU (e.g. <code>\"cuda\"</code>) and your data is on the CPU (e.g. <code>\"cpu\"</code>).</li> <li>Datatype errors - Your data is one datatype (e.g. <code>torch.float32</code>), however the operation you're trying to perform requires another datatype (e.g. <code>torch.int64</code>).</li> </ol> <p>Notice the recurring theme here.</p> <p>There's some kind of mismatch between your shape(s), device(s) and/or datatype(s).</p> <p>This notebook/blog post goes through examples of each of the above errors and how to fix them.</p> <p>It won't prevent you from making them in the future but it will make you aware enough to perhaps reduce them and even more important, know how to solve them.</p> <p>Note: All of the following examples have been adapted from learnpytorch.io which is the book version of the Zero to Mastery: PyTorch for Deep Learning video course.</p>"},{"location":"pytorch_most_common_errors/#1-shape-errors-in-pytorch","title":"1. Shape errors in PyTorch\u00b6","text":""},{"location":"pytorch_most_common_errors/#11-matrix-multiplication-shape-errors","title":"1.1 Matrix multiplication shape errors\u00b6","text":"<p>PyTorch is one of the best frameworks to build neural network models with.</p> <p>And one of the fundamental operations of a neural network is matrix multiplication.</p> <p>However, matrix multiplication comes with very specific rules.</p> <p>If these rules aren't adhered to, you'll get an infamous shape error.</p> <pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)\n</code></pre> <p>Let's start with a brief example.</p> <p>Note: Although it's called \"matrix multiplication\" almost every form of data in PyTorch comes in the form of a tensor. Where a tensor is an n-dimensional array (n can be any number). So while I use the terminology \"matrix multiplication\", this extends to \"tensor multiplication\" as well. See 00. PyTorch Fundamentals: Introduction to Tensors for more on the difference between matrices and tensors.</p>"},{"location":"pytorch_most_common_errors/#12-pytorch-neural-network-shape-errors","title":"1.2 PyTorch neural network shape errors\u00b6","text":"<p>We've seen how shape errors can occur when working with matrix multiplication (or matrix multiplying tensors).</p> <p>Now let's build a neural network with PyTorch and see where shape errors can occur.</p> <p>A shape error will occur in neural network in any of the following situations:</p> <ul> <li>Incorrect input shape - your data is in a certain shape but the model's first layer expects a different shape.</li> <li>Incorrect input and output shapes between layers - one of the layers of your model outputs a certain shape but the following layer expects a different shape as input.</li> <li>No batch size dimension in input data when trying to make a prediction - your model was trained on samples with a batch dimension, so when you try to predict on a single sample without a batch dimension, an error occurs.</li> </ul> <p>To showcase these shape errors, let's build a simple neural network (the errors are the same regardless of the size of your network) to try and find patterns in the Fashion MNIST dataset (black and white images of 10 different classes of clothing).</p> <p>Note: The following examples focus specifically on shape errors rather than building the best neural network. You can see a fully working example of this problem in 03. PyTorch Computer Vision.</p>"},{"location":"pytorch_most_common_errors/#13-downloading-a-dataset","title":"1.3 Downloading a dataset\u00b6","text":"<p>To begin, we'll get the Fashion MNIST dataset from <code>torchvision.datasets</code>.</p>"},{"location":"pytorch_most_common_errors/#14-building-a-series-of-neural-networks-with-different-shape-errors","title":"1.4 Building a series of neural networks with different shape errors\u00b6","text":"<p>Our problem is: build a neural network capable of finding patterns in grayscale images of clothing.</p> <p>This statement could go very deep since \"what neural network is the best?\" is one of the main research problems in machine learning as a whole.</p> <p>But let's start as simple as possible to showcase different error types.</p> <p>We'll build several two layer neural networks with PyTorch each to showcase a different error:</p> Model number Layers Error showcase 0 2 x <code>nn.Linear()</code> with 10 hidden units Incorrect input shape 1 Same as model 1 + 1 x <code>nn.Flatten()</code> Incorrect input shape (still) 2 1 x <code>nn.Flatten()</code>, 1 x <code>nn.Linear()</code> with correct input shape and 1 x <code>nn.Linear()</code> with 10 hidden units None (input shape is correct) 3 Same as model 2 but with different shapes between <code>nn.Linear()</code> layers Incorrect shapes between layers 4 Same as model 3 but with last layer replaced with <code>nn.LazyLinear()</code> None (shows how <code>nn.LazyX()</code> layers can infer correct shape) 5 Same as model 4 but with all <code>nn.Linear()</code> replaced with <code>nn.LazyLinear()</code> None (shows how <code>nn.LazyX()</code> layers can infer correct shape)"},{"location":"pytorch_most_common_errors/#15-incorrect-input-layer-shapes","title":"1.5 Incorrect input layer shapes\u00b6","text":"<p>We'll start with a two layer network with <code>nn.Linear()</code> layers with 10 hidden units in each.</p> <p>Note: See 01. PyTorch Workflow section 6: Putting it all together for what happens inside <code>nn.Linear()</code>.</p> <p>And then we'll pass our <code>image</code> through it and see what happens.</p>"},{"location":"pytorch_most_common_errors/#16-incorrect-hidden-layer-input-and-output-shapes","title":"1.6 Incorrect hidden layer input and output shapes\u00b6","text":"<p>What happens if our input layer(s) had the correct shapes but there was a mismatch between the interconnected layer(s)?</p> <p>As in, our first <code>nn.Linear()</code> had <code>out_features=10</code> but the next <code>nn.Linear()</code> had <code>in_features=5</code>.</p> <p>This is an example of incorrect input and output shapes between layers.</p>"},{"location":"pytorch_most_common_errors/#17-pytorch-lazy-layers-automatically-inferring-the-input-shape","title":"1.7 PyTorch lazy layers (automatically inferring the input shape)\u00b6","text":"<p>Lazy layers in PyTorch often come in the form of <code>nn.LazyX</code> where <code>X</code> is an existing non-lazy form of the layer.</p> <p>For example, the lazy equilvalent of <code>nn.Linear()</code> is <code>nn.LazyLinear()</code>.</p> <p>The main feature of a <code>Lazy</code> layer is to infer what the <code>in_features</code> or input shape from the previous layer should be.</p> <p>Note: As of November 2022, <code>Lazy</code> layers in PyTorch are still experimental and subject to change, however their usage shouldn't differ too dramatically from what's below.</p> <p>For example, if the previous layer has <code>out_features=10</code>, the subsequent <code>Lazy</code> layer should infer that <code>in_features=10</code>.</p> <p>Let's test it out.</p>"},{"location":"pytorch_most_common_errors/#2-device-errors-in-pytorch","title":"2. Device errors in PyTorch\u00b6","text":"<p>One of the main benefits of PyTorch is the in-built ability for doing computations on a GPU (graphics processing unit).</p> <p>GPUs can often perform operations, specifically matrix multiplications (which make up the most of neural networks) much faster than CPUs (central processing units).</p> <p>If you're using vanilla PyTorch (no other external libraries), PyTorch requires you to explicitly set which device you're computing on.</p> <p>For example, to send your model to a target <code>device</code>, you would use the <code>to()</code> method, such as <code>model.to(device)</code>.</p> <p>And similarly for data <code>some_dataset.to(device)</code>.</p> <p>Device errors occur when your model/data are on different devices.</p> <p>Such as when you've sent your model to the target GPU device but your data is still on the CPU.</p>"},{"location":"pytorch_most_common_errors/#21-setting-the-target-device","title":"2.1 Setting the target device\u00b6","text":"<p>Let's set our current device to <code>\"cuda\"</code> if it's available.</p> <p>Note: See 00. PyTorch Fundamentals: Running Tensors on GPUs for more information about how to get access to a GPU and set it up with PyTorch.</p>"},{"location":"pytorch_most_common_errors/#22-preparing-data-for-modelling","title":"2.2 Preparing data for modelling\u00b6","text":"<p>To prepare our data for modelling, let's create some PyTorch <code>DataLoader</code>'s.</p> <p>To make things quicker, we'll use an instance of <code>torch.utils.data.RandomSampler</code> to randomly select 10% of the training and testing samples (we're not interested in the best performing model as much as we are in showcasing potential errors).</p> <p>We'll also setup a loss function of <code>torch.nn.CrossEntropyLoss()</code> as well as an optimizer of <code>torch.optim.SGD(lr=0.01)</code>.</p> <p>Note: For more information on preparing data, loss functions and optimizers for training a PyTorch model, see 01. PyTorch Workflow Fundamentals section 3: Training a model.</p>"},{"location":"pytorch_most_common_errors/#23-training-a-model-on-the-cpu","title":"2.3 Training a model on the CPU\u00b6","text":"<p>Data ready, model ready, let's train!</p> <p>We'll use a standard PyTorch training loop to do five epochs of training with <code>model_6</code> going over 10% of the data.</p> <p>Don't worry too much here about the loss being as low as it could be as we're more focused on making sure there aren't any errors than having the lowest possible loss.</p> <p>Note: For more information on the steps in a PyTorch training loop, see 01. PyTorch Workflow section 3: PyTorch training loop.</p>"},{"location":"pytorch_most_common_errors/#24-attempting-to-train-a-model-on-the-gpu-with-errors","title":"2.4 Attempting to train a model on the GPU (with errors)\u00b6","text":"<p>Now let's send our <code>model_6</code> to the target <code>device</code> (in our case, this is a <code>\"cuda\"</code> GPU).</p>"},{"location":"pytorch_most_common_errors/#25-training-a-model-on-the-gpu-without-errors","title":"2.5 Training a model on the GPU (without errors)\u00b6","text":"<p>Let's fix this error but sending our data tensors (<code>X</code> and <code>y</code>) to the target <code>device</code> as well.</p> <p>We can do so using <code>X.to(device)</code> and <code>y.to(device)</code>.</p>"},{"location":"pytorch_most_common_errors/#26-device-errors-when-making-predictions","title":"2.6 Device errors when making predictions\u00b6","text":"<p>We've seen device errors whilst training but the same error can occur during testing or inference (making predictions).</p> <p>The whole idea of training a model on some data is to use it to make predictions on unseen data.</p> <p>Let's take our trained <code>model_6</code> and use it to make a prediction on a sample from the test dataset.</p>"},{"location":"pytorch_most_common_errors/#3-datatype-errors-in-pytorch","title":"3. Datatype errors in PyTorch\u00b6","text":"<p>Recall the rule of thumb: trained models like to predict on data that's in the same shape and format that they were trained on.</p> <p>It looks like our model expects a <code>Float</code> datatype but our <code>test_image</code> is in <code>Byte</code> datatype.</p> <p>We can tell this by the last line in the previous error:</p> <pre><code>RuntimeError: expected scalar type Float but found Byte\n</code></pre> <p>Why is this?</p> <p>It's because our <code>model_6</code> was trained on data samples in the format of <code>Float</code>, specifically, <code>torch.float32</code>.</p> <p>How do we know this?</p> <p>Well, <code>torch.float32</code> is the default for many tensors in PyTorch, unless explicitly set otherwise.</p> <p>But let's do a check to make sure.</p>"},{"location":"pytorch_most_common_errors/#31-checking-the-datatype-of-the-data-the-model-was-trained-on","title":"3.1 Checking the datatype of the data the model was trained on\u00b6","text":"<p>We can check the datatype of the data our model was trained on by looking at the <code>dtype</code> attribute of a sample from our <code>train_dataloader</code>.</p>"},{"location":"pytorch_most_common_errors/#32-changing-the-datatype-of-a-tensor","title":"3.2 Changing the datatype of a tensor\u00b6","text":"<p>In our case, we could create a standalone transform to transform our test data but we can also change the datatype of a target tensor with <code>tensor.type(some_type_here)</code>, for example, <code>tensor_1.type(torch.float32)</code>.</p> <p>Let's try it out.</p>"},{"location":"pytorch_most_common_errors/#33-making-predictions-on-a-test-image-and-making-sure-its-in-the-right-format","title":"3.3 Making predictions on a test image and making sure it's in the right format\u00b6","text":"<p>Alright, it looks like we've got all of the pieces of the puzzle ready, shape, device and datatype, let's try and make a prediction!</p> <p>Note: Remember a model likes to make predictions on data in the same (or similar) format to what it was trained on (shape, device and datatype).</p>"},{"location":"pytorch_most_common_errors/#putting-it-all-together","title":"Putting it all together\u00b6","text":"<p>We've been hands on with three of the main errors you'll come across when building neural networks with PyTorch:</p> <ol> <li>Shape errors - there are mismatches between the data you're working with the neural network you're building to find patterns in or there are mismatches between the connecting layers of your neural network.</li> <li>Device errors - your model and data are on different devices, PyTorch expects all tensors and objects to be on the same device.</li> <li>Datatype errors - you're trying to compute on one datatype when your model expects another datatype.</li> </ol> <p>And we've seen how and why they occur and then how to fix them:</p> <ul> <li>Your model wants to make predictions on same kind of data it was trained on (shape, device and datatype).</li> <li>Your model and data should be on same device for training and testing.</li> <li>You can take care of many of these issues by creating reusable functions that define <code>device</code> and datatype, such as in 04. PyTorch Going Modular section 4: Creating training and testing functions.</li> </ul> <p>Knowing about these errors won't prevent you from making them in the future but it will give you an idea of where to go to fix them.</p> <p>For more in-depth examples of these errors, including making them and fixing in a hands-on manner, check out the Zero to Mastery: PyTorch for Deep Learning course.</p>"}]}