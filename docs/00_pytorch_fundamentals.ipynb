{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "> **원본:** 이 노트북은 [Daniel Bourke](https://github.com/mrdbourke)의 [Learn PyTorch for Deep Learning](https://github.com/mrdbourke/pytorch-deep-learning) 자료를 한국어로 번역한 것입니다. 원본 저장소: https://github.com/mrdbourke/pytorch-deep-learning\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/00_pytorch_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSNK7duj5SeU"
   },
   "source": [
    "# 00. PyTorch 기초\n",
    "\n",
    "## PyTorch란 무엇인가?\n",
    "\n",
    "[PyTorch](https://pytorch.org/)는 오픈 소스 머신러닝 및 딥러닝 프레임워크입니다.\n",
    "\n",
    "## PyTorch는 무엇에 사용될 수 있는가?\n",
    "\n",
    "PyTorch를 사용하면 Python 코드를 사용하여 데이터를 조작하고 처리하며 머신러닝 알고리즘을 작성할 수 있습니다.\n",
    "\n",
    "## PyTorch를 누가 사용하는가?\n",
    "\n",
    "[메타(Facebook)](https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/), 테슬라, 마이크로소프트와 같은 세계 최대 기술 회사들과 [OpenAI가 PyTorch를 사용](https://openai.com/blog/openai-pytorch/)하는 것과 같은 인공지능 연구 회사들이 연구를 강화하고 제품에 머신러닝을 도입하기 위해 PyTorch를 사용합니다.\n",
    "\n",
    "![pytorch being used across industry and research](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-being-used-across-research-and-industry.png)\n",
    "\n",
    "예를 들어, Andrej Karpathy(테슬라 AI 책임자)는 테슬라가 자율주행 컴퓨터 비전 모델을 구동하기 위해 PyTorch를 어떻게 사용하는지에 대해 여러 번의 강연([PyTorch DevCon 2019](https://youtu.be/oBklltKXtDE), [Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M?t=2904))을 했습니다.\n",
    "\n",
    "PyTorch는 또한 [트랙터에서 컴퓨터 비전을 구동](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)하는 것과 같은 농업과 같은 다른 산업에서도 사용됩니다.\n",
    "\n",
    "## PyTorch를 사용하는 이유는?\n",
    "\n",
    "머신러닝 연구자들은 PyTorch 사용을 좋아합니다. 2022년 2월 기준으로, PyTorch는 머신러닝 연구 논문과 관련 코드 저장소를 추적하는 웹사이트인 [Papers With Code에서 가장 많이 사용되는 딥러닝 프레임워크](https://paperswithcode.com/trends)입니다.\n",
    "\n",
    "PyTorch는 또한 GPU 가속(코드를 더 빠르게 실행)과 같은 많은 것들을 뒤에서 처리하는 데 도움이 됩니다.\n",
    "\n",
    "따라서 데이터 조작과 알고리즘 작성에 집중할 수 있고, PyTorch가 빠르게 실행되도록 보장합니다.\n",
    "\n",
    "그리고 테슬라와 메타(Facebook)와 같은 회사들이 수백 개의 애플리케이션을 구동하고, 수천 대의 자동차를 운전하며, 수십억 명의 사람들에게 콘텐츠를 전달하는 모델을 구축하는 데 사용한다면, 개발 측면에서도 명백히 능력이 있다는 것입니다.\n",
    "\n",
    "## 이 모듈에서 다룰 내용\n",
    "\n",
    "이 과정은 서로 다른 섹션(노트북)으로 나뉩니다.\n",
    "\n",
    "각 노트북은 PyTorch 내의 중요한 아이디어와 개념을 다룹니다.\n",
    "\n",
    "후속 노트북들은 이전 노트북의 지식을 바탕으로 구축됩니다(번호는 00, 01, 02로 시작하여 끝까지 갑니다).\n",
    "\n",
    "이 노트북은 머신러닝과 딥러닝의 기본 구성 요소인 텐서를 다룹니다.\n",
    "\n",
    "구체적으로 다음을 다룹니다:\n",
    "\n",
    "| **주제** | **내용** |\n",
    "| ----- | ----- |\n",
    "| **텐서 소개** | 텐서는 모든 머신러닝과 딥러닝의 기본 구성 요소입니다. |\n",
    "| **텐서 생성** | 텐서는 거의 모든 종류의 데이터(이미지, 단어, 숫자 표)를 나타낼 수 있습니다. |\n",
    "| **텐서에서 정보 가져오기** | 텐서에 정보를 넣을 수 있다면, 그것을 꺼내고 싶을 것입니다. |\n",
    "| **텐서 조작** | 머신러닝 알고리즘(신경망과 같은)은 더하기, 곱하기, 결합과 같은 다양한 방법으로 텐서를 조작하는 것을 포함합니다. | \n",
    "| **텐서 shape 다루기** | 머신러닝에서 가장 흔한 문제 중 하나는 shape 불일치를 다루는 것입니다(잘못된 shape의 텐서를 다른 텐서와 혼합하려고 시도). |\n",
    "| **텐서 인덱싱** | Python 리스트나 NumPy 배열에 인덱싱해봤다면, 텐서도 매우 유사하지만 훨씬 더 많은 차원을 가질 수 있습니다. |\n",
    "| **PyTorch 텐서와 NumPy 혼합** | PyTorch는 텐서([`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html))로 작업하고, NumPy는 배열([`np.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html))을 선호합니다. 때로는 이것들을 혼합하고 매칭하고 싶을 것입니다. | \n",
    "| **재현성** | 머신러닝은 매우 실험적이며 많은 *무작위성*을 사용하여 작동하므로, 때로는 그 *무작위성*이 그렇게 무작위가 아니기를 원할 것입니다. |\n",
    "| **GPU에서 텐서 실행** | GPU(그래픽 처리 장치)는 코드를 더 빠르게 만들고, PyTorch는 GPU에서 코드를 쉽게 실행할 수 있게 합니다. |\n",
    "\n",
    "## 어디서 도움을 받을 수 있나요?\n",
    "\n",
    "이 과정의 모든 자료는 [GitHub](https://github.com/mrdbourke/pytorch-deep-learning)에 있습니다.\n",
    "\n",
    "문제가 생기면 [Discussions 페이지](https://github.com/mrdbourke/pytorch-deep-learning/discussions)에 질문을 남길 수도 있습니다.\n",
    "\n",
    "[PyTorch 개발자 포럼](https://discuss.pytorch.org/)도 있습니다. PyTorch의 모든 것에 대한 매우 유용한 장소입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v3iRCRUTGeu"
   },
   "source": [
    "## PyTorch 가져오기\n",
    "\n",
    "> **참고:** 이 노트북의 코드를 실행하기 전에 [PyTorch 설정 단계](https://pytorch.org/get-started/locally/)를 거쳐야 합니다. \n",
    ">\n",
    "> 하지만 **Google Colab에서 실행 중이라면**, 모든 것이 작동해야 합니다(Google Colab에는 PyTorch와 다른 라이브러리들이 설치되어 있습니다).\n",
    "\n",
    "PyTorch를 가져오고 사용하고 있는 버전을 확인하는 것부터 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1VxEOik46Y4i",
    "outputId": "f3141076-29bc-4600-c1c3-1586b1fe2292"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.10.0+cu111'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SqvI4S9TGew"
   },
   "source": [
    "훌륭합니다. PyTorch 1.10.0(2021년 12월 기준)이 설치되어 있는 것 같습니다. 이는 이 자료를 따라가고 있다면 PyTorch 1.10.0과의 대부분의 호환성을 볼 수 있다는 의미이지만, 버전 번호가 훨씬 높다면 일부 불일치를 발견할 수 있습니다.\n",
    "\n",
    "문제가 있다면 GitHub Discussions 페이지에 게시해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-33BKR16iWc"
   },
   "source": [
    "## 텐서 소개\n",
    "\n",
    "이제 PyTorch를 가져왔으니, 텐서에 대해 배울 시간입니다.\n",
    "\n",
    "텐서는 머신러닝의 기본 구성 요소입니다.\n",
    "\n",
    "그들의 역할은 데이터를 수치적 방식으로 나타내는 것입니다.\n",
    "\n",
    "예를 들어, 이미지를 `[3, 224, 224]` shape의 텐서로 나타낼 수 있는데, 이는 `[색상_채널, 높이, 너비]`를 의미합니다. 즉, 이미지가 `3`개의 색상 채널(빨강, 초록, 파랑)과 `224`픽셀의 높이, `224`픽셀의 너비를 가집니다.\n",
    "\n",
    "![example of going from an input image to a tensor representation of the image, image gets broken down into 3 colour channels as well as numbers to represent the height and width](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-tensor-shape-example-of-image.png)\n",
    "\n",
    "텐서 용어(텐서를 설명하는 데 사용되는 언어)에서, 텐서는 `색상_채널`, `높이`, `너비`에 대해 각각 하나씩 세 개의 차원을 가집니다.\n",
    "\n",
    "하지만 너무 앞서 나가고 있습니다.\n",
    "\n",
    "텐서를 코딩하여 더 많이 배워보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFF0N2TU7S7Q"
   },
   "source": [
    "### 텐서 생성\n",
    "\n",
    "PyTorch는 텐서를 좋아합니다. [`torch.Tensor`](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor) 클래스에 전용된 전체 문서 페이지가 있을 정도입니다.\n",
    "\n",
    "첫 번째 숙제는 [`torch.Tensor` 문서](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor)를 10분간 읽어보는 것입니다. 하지만 나중에 해도 됩니다.\n",
    "\n",
    "코딩해보겠습니다.\n",
    "\n",
    "첫 번째로 생성할 것은 **스칼라**입니다.\n",
    "\n",
    "스칼라는 단일 숫자이며, 텐서 용어로는 0차원 텐서입니다.\n",
    "\n",
    "> **참고:** 이것이 이 과정의 경향입니다. 특정 코드 작성에 집중하겠습니다. 하지만 종종 PyTorch 문서를 읽고 익숙해지는 연습을 설정할 것입니다. 결국 이 과정을 마친 후에는 더 배우고 싶어할 것이고, 문서는 자주 찾게 될 곳이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUDgG2zk7Us5",
    "outputId": "0ac22bd2-16bc-4307-f312-31ae89d6c375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqSuhW7rTGey"
   },
   "source": [
    "위에서 `tensor(7)`이 출력된 것을 보셨나요?\n",
    "\n",
    "이는 `scalar`가 단일 숫자임에도 불구하고 `torch.Tensor` 타입이라는 의미입니다.\n",
    "\n",
    "`ndim` 속성을 사용하여 텐서의 차원을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV98Yz868bav",
    "outputId": "502a625e-ff3c-4fc4-b523-f7634ea82128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO2YW_QGTGez"
   },
   "source": [
    "텐서에서 숫자를 가져오고 싶다면 어떨까요?\n",
    "\n",
    "즉, `torch.Tensor`에서 Python 정수로 변환하는 것입니다.\n",
    "\n",
    "이를 위해 `item()` 메서드를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k4cyKumPfbE",
    "outputId": "1f6a7916-0c7c-403f-8ebd-875454a94470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Python number within a tensor (only works with one-element tensors)\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYs7ulrATGe0"
   },
   "source": [
    "좋습니다. 이제 **벡터**를 살펴보겠습니다.\n",
    "\n",
    "벡터는 단일 차원 텐서이지만 많은 숫자를 포함할 수 있습니다.\n",
    "\n",
    "즉, 집의 `[침실, 화장실]`을 설명하기 위해 벡터 `[3, 2]`를 가질 수 있습니다. 또는 집의 `[침실, 화장실, 주차장]`을 설명하기 위해 `[3, 2, 2]`를 가질 수 있습니다.\n",
    "\n",
    "여기서 중요한 경향은 벡터가 나타낼 수 있는 것이 유연하다는 것입니다(텐서도 마찬가지입니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IZF6ASs8QH9",
    "outputId": "e556ed2a-e58a-440f-b103-0f06c91bc75c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXxRUUW2TGe1"
   },
   "source": [
    "훌륭합니다. `vector`가 이제 제가 가장 좋아하는 숫자인 7을 두 개 포함하고 있습니다.\n",
    "\n",
    "몇 개의 차원을 가질 것이라고 생각하시나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03hm3VVv8kr4",
    "outputId": "2035bb26-0189-4b28-fa02-34220d44677f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of dimensions of vector\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0VYvSGbTGe1"
   },
   "source": [
    "흠, 이상합니다. `vector`는 두 개의 숫자를 포함하고 있지만 단일 차원만 가집니다.\n",
    "\n",
    "비밀을 알려드리겠습니다.\n",
    "\n",
    "PyTorch에서 텐서가 가진 차원의 개수는 외부의 대괄호 개수(`[`)로 알 수 있으며, 한 쪽만 세면 됩니다.\n",
    "\n",
    "`vector`는 몇 개의 대괄호를 가지고 있나요?\n",
    "\n",
    "텐서의 또 다른 중요한 개념은 `shape` 속성입니다. shape는 내부 요소들이 어떻게 배열되어 있는지 알려줍니다.\n",
    "\n",
    "`vector`의 shape를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zREV1bDTGe2",
    "outputId": "2a6e7ceb-7eb2-422b-b006-2c6e4825272f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of vector\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aWKppNyTGe2"
   },
   "source": [
    "위에서 `torch.Size([2])`가 반환되었는데, 이는 우리의 벡터가 `[2]`의 shape를 가진다는 의미입니다. 이는 대괄호 안에 넣은 두 개의 요소(`[7, 7]`) 때문입니다.\n",
    "\n",
    "이제 **행렬**을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5iNwCYL8QO9",
    "outputId": "88fc63a7-4130-4c7a-a574-c61e85d2e99e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "MATRIX = torch.tensor([[7, 8], \n",
    "                       [9, 10]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3U1bCdjTGe3"
   },
   "source": [
    "와! 더 많은 숫자들입니다! 행렬은 벡터만큼 유연하지만, 추가 차원을 가집니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LREUbeb8r8j",
    "outputId": "636246b0-b109-472a-c6d5-8601a9e08654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions\n",
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhXXgq-dTGe3"
   },
   "source": [
    "`MATRIX`는 두 개의 차원을 가집니다(한 쪽의 외부 대괄호 개수를 세어보셨나요?).\n",
    "\n",
    "어떤 `shape`를 가질 것이라고 생각하시나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TL26I31TGe3",
    "outputId": "f05ec0b6-0bc1-4381-9474-56cbe6c67139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvLpUvrKTGe4"
   },
   "source": [
    "`MATRIX`가 두 요소 깊이와 두 요소 너비이기 때문에 `torch.Size([2, 2])` 출력을 얻습니다.\n",
    "\n",
    "**텐서**를 생성해보는 것은 어떨까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEMDQr188QWW",
    "outputId": "4230e6bd-1844-4210-eea8-245bb8b8b265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmJKkXD7TGe4"
   },
   "source": [
    "와! 정말 멋진 텐서입니다.\n",
    "\n",
    "텐서가 거의 모든 것을 나타낼 수 있다는 점을 강조하고 싶습니다.\n",
    "\n",
    "방금 생성한 것은 스테이크와 아몬드 버터 가게의 판매 숫자일 수 있습니다(제가 가장 좋아하는 음식 두 가지).\n",
    "\n",
    "![a simple tensor in google sheets showing day of week, steak sales and almond butter sales](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_simple_tensor.png)\n",
    "\n",
    "몇 개의 차원을 가질 것이라고 생각하시나요? (힌트: 대괄호 세기 요령을 사용하세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dhuEsjS8QcT",
    "outputId": "7a45df1b-fc32-4cc5-e330-527c6ef7ba5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions for TENSOR\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln9dys5VTGe4"
   },
   "source": [
    "그리고 shape는 어떨까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdVv4iNRTGe5",
    "outputId": "d8ac706c-020b-4926-b145-d44e41f35e90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of TENSOR\n",
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxk8GU7oTGe5"
   },
   "source": [
    "좋습니다. `torch.Size([1, 3, 3])`를 출력합니다.\n",
    "\n",
    "차원은 외부에서 내부로 갑니다.\n",
    "\n",
    "이는 3×3의 1차원이 있다는 의미입니다.\n",
    "\n",
    "![example of different tensor dimensions](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n",
    "\n",
    "> **참고:** `scalar`와 `vector`에는 소문자를, `MATRIX`와 `TENSOR`에는 대문자를 사용한 것을 눈치채셨을 것입니다. 이는 의도적인 것입니다. 실제로는 스칼라와 벡터가 `y`나 `a`와 같은 소문자로 표시되는 것을 자주 볼 수 있습니다. 그리고 행렬과 텐서는 `X`나 `W`와 같은 대문자로 표시됩니다.\n",
    ">\n",
    "> 행렬과 텐서라는 이름이 서로 바꿔 사용되는 것도 눈치채셨을 것입니다. 이는 일반적입니다. PyTorch에서는 종종 `torch.Tensor`를 다루기 때문에(따라서 텐서라는 이름), 하지만 내부의 shape와 차원이 실제로 무엇인지를 결정합니다.\n",
    "\n",
    "요약해보겠습니다.\n",
    "\n",
    "| 이름 | 무엇인가? | 차원 수 | 소문자 또는 대문자 (일반적/예시) |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **scalar** | 단일 숫자 | 0 | 소문자 (`a`) | \n",
    "| **vector** | 방향이 있는 숫자(예: 방향이 있는 풍속)이지만 다른 많은 숫자들도 가질 수 있음 | 1 | 소문자 (`y`) |\n",
    "| **matrix** | 숫자의 2차원 배열 | 2 | 대문자 (`Q`) |\n",
    "| **tensor** | 숫자의 n차원 배열 | 임의의 수, 0차원 텐서는 스칼라, 1차원 텐서는 벡터 | 대문자 (`X`) | \n",
    "\n",
    "![scalar vector matrix tensor and what they look like](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dms7G4nkTGe5"
   },
   "source": [
    "### 무작위 텐서\n",
    "\n",
    "텐서가 어떤 형태의 데이터를 나타낸다는 것을 확인했습니다.\n",
    "\n",
    "그리고 신경망과 같은 머신러닝 모델은 텐서 내에서 조작하고 패턴을 찾습니다.\n",
    "\n",
    "하지만 PyTorch로 머신러닝 모델을 구축할 때는 텐서를 수동으로 생성하는 경우는 드뭅니다(지금까지 해온 것처럼).\n",
    "\n",
    "대신, 머신러닝 모델은 종종 큰 무작위 숫자 텐서로 시작하여 데이터를 통해 작업하면서 더 잘 나타내도록 이러한 무작위 숫자들을 조정합니다.\n",
    "\n",
    "본질적으로:\n",
    "\n",
    "`무작위 숫자로 시작 -> 데이터 보기 -> 무작위 숫자 업데이트 -> 데이터 보기 -> 무작위 숫자 업데이트...`\n",
    "\n",
    "데이터 과학자로서, 머신러닝 모델이 시작하는 방법(초기화), 데이터를 보는 방법(표현), 무작위 숫자를 업데이트하는 방법(최적화)을 정의할 수 있습니다.\n",
    "\n",
    "이러한 단계들을 나중에 직접 다뤄보겠습니다.\n",
    "\n",
    "지금은 무작위 숫자의 텐서를 생성하는 방법을 살펴보겠습니다.\n",
    "\n",
    "[`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html)를 사용하고 `size` 매개변수를 전달하여 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOJEtDx--GnK",
    "outputId": "2680d44b-e31c-4ab1-d5b1-c0cd76706a0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4090, 0.2527, 0.8699, 0.2002],\n",
       "         [0.8421, 0.1428, 0.1431, 0.0111],\n",
       "         [0.2281, 0.0345, 0.6734, 0.3866]]), torch.float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor of size (3, 4)\n",
    "random_tensor = torch.rand(size=(3, 4))\n",
    "random_tensor, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wB1c_cXTGe5"
   },
   "source": [
    "`torch.rand()`의 유연성은 `size`를 원하는 대로 조정할 수 있다는 것입니다.\n",
    "\n",
    "예를 들어, 일반적인 이미지 shape인 `[224, 224, 3]`(`[높이, 너비, 색상_채널]`)의 무작위 텐서를 원한다고 가정해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMF_NUp3Ym__",
    "outputId": "8346b853-0b1e-481a-d9ee-a410ee21bab0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor of size (224, 224, 3)\n",
    "random_image_size_tensor = torch.rand(size=(224, 224, 3))\n",
    "random_image_size_tensor.shape, random_image_size_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MQNTY0eTGe6"
   },
   "source": [
    "### 0과 1\n",
    "\n",
    "때로는 텐서를 0이나 1로 채우고 싶을 것입니다.\n",
    "\n",
    "이는 마스킹에서 자주 발생합니다(모델이 학습하지 않도록 하나의 텐서의 일부 값을 0으로 마스킹하는 것처럼).\n",
    "\n",
    "[`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html)로 0으로 가득 찬 텐서를 생성해보겠습니다.\n",
    "\n",
    "다시 한번, `size` 매개변수가 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCzhd0hl9Vp6",
    "outputId": "9c8ec87f-d8c9-4751-a13e-6a5e986daaa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]), torch.float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all zeros\n",
    "zeros = torch.zeros(size=(3, 4))\n",
    "zeros, zeros.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDQBZJRUZWTN"
   },
   "source": [
    "모든 1로 가득 찬 텐서를 생성하는 것도 동일하게 할 수 있지만, 대신 [`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html)를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRe6sSXiTGe6",
    "outputId": "3f45b0b8-7f65-423d-c664-f5b5f7866fd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]), torch.float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all ones\n",
    "ones = torch.ones(size=(3, 4))\n",
    "ones, ones.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hib1NYrSarL2"
   },
   "source": [
    "### 범위 생성 및 유사한 텐서\n",
    "\n",
    "때로는 1부터 10까지 또는 0부터 100까지와 같은 숫자 범위를 원할 수 있습니다.\n",
    "\n",
    "`torch.arange(start, end, step)`를 사용하여 할 수 있습니다.\n",
    "\n",
    "여기서:\n",
    "* `start` = 범위의 시작 (예: 0)\n",
    "* `end` = 범위의 끝 (예: 10)\n",
    "* `step` = 각 값 사이의 단계 수 (예: 1)\n",
    "\n",
    "> **참고:** Python에서는 `range()`를 사용하여 범위를 생성할 수 있습니다. 하지만 PyTorch에서는 `torch.range()`가 더 이상 사용되지 않으며 향후 오류가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IqUs81d9W4W",
    "outputId": "2a6f0c08-052e-4b36-b4eb-6a537239026f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use torch.arange(), torch.range() is deprecated \n",
    "zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n",
    "\n",
    "# Create a range of values 0 to 10\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-bXf0Ugbh-D"
   },
   "source": [
    "때로는 다른 텐서와 동일한 shape를 가진 특정 유형의 텐서를 원할 수 있습니다.\n",
    "\n",
    "예를 들어, 이전 텐서와 동일한 shape를 가진 모든 0으로 구성된 텐서입니다.\n",
    "\n",
    "이를 위해 [`torch.zeros_like(input)`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) 또는 [`torch.ones_like(input)`](https://pytorch.org/docs/1.9.1/generated/torch.ones_like.html)을 사용할 수 있습니다. 이들은 각각 `input`과 동일한 shape로 0 또는 1로 채워진 텐서를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvXwUut5BhHq",
    "outputId": "096b2f8e-8c21-4ace-97b9-c36b92b2fe77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also create a tensor of zeros similar to another tensor\n",
    "ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huKZ6QlYTGe7"
   },
   "source": [
    "### 텐서 데이터 타입\n",
    "\n",
    "PyTorch에는 [많은 다양한 텐서 데이터 타입](https://pytorch.org/docs/stable/tensors.html#data-types)이 있습니다.\n",
    "\n",
    "일부는 CPU 전용이고 일부는 GPU에 더 적합합니다.\n",
    "\n",
    "어떤 것이 어떤 것인지 아는 데 시간이 걸릴 수 있습니다.\n",
    "\n",
    "일반적으로 어디서든 `torch.cuda`를 보면 텐서가 GPU용으로 사용되고 있습니다(Nvidia GPU가 CUDA라는 컴퓨팅 도구를 사용하기 때문입니다).\n",
    "\n",
    "가장 일반적인 유형(그리고 일반적으로 기본값)은 `torch.float32` 또는 `torch.float`입니다.\n",
    "\n",
    "이것을 \"32비트 부동소수점\"이라고 합니다.\n",
    "\n",
    "하지만 16비트 부동소수점(`torch.float16` 또는 `torch.half`)과 64비트 부동소수점(`torch.float64` 또는 `torch.double`)도 있습니다.\n",
    "\n",
    "그리고 더 혼란스럽게도 8비트, 16비트, 32비트, 64비트 정수도 있습니다.\n",
    "\n",
    "그리고 더 많습니다!\n",
    "\n",
    "> **참고:** 정수는 `7`과 같은 평평한 둥근 숫자이고, float는 `7.0`과 같은 소수점을 가집니다.\n",
    "\n",
    "이 모든 것의 이유는 **컴퓨팅의 정밀도**와 관련이 있습니다.\n",
    "\n",
    "정밀도는 숫자를 설명하는 데 사용되는 세부사항의 양입니다.\n",
    "\n",
    "정밀도 값(8, 16, 32)이 높을수록 숫자를 표현하는 데 사용되는 세부사항과 데이터가 더 많습니다.\n",
    "\n",
    "이는 딥러닝과 수치 컴퓨팅에서 중요합니다. 너무 많은 연산을 수행하기 때문에 계산해야 하는 세부사항이 많을수록 사용해야 하는 컴퓨팅이 더 많아지기 때문입니다.\n",
    "\n",
    "따라서 낮은 정밀도 데이터 타입은 일반적으로 계산이 더 빠르지만 정확도와 같은 평가 지표에서 일부 성능을 희생합니다(계산은 더 빠르지만 덜 정확함).\n",
    "\n",
    "> **자료:** \n",
    "  * [사용 가능한 모든 텐서 데이터 타입 목록을 위한 PyTorch 문서](https://pytorch.org/docs/stable/tensors.html#data-types)를 참조하세요.\n",
    "  * [컴퓨팅에서 정밀도가 무엇인지에 대한 개요를 위한 Wikipedia 페이지](https://en.wikipedia.org/wiki/Precision_(computer_science))를 읽어보세요.\n",
    "\n",
    "특정 데이터 타입으로 텐서를 생성하는 방법을 살펴보겠습니다. `dtype` 매개변수를 사용하여 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3MoGnpw9XaF",
    "outputId": "61070939-8c52-4ac6-bed7-e64b3ce24615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default datatype for tensors is float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n",
    "                               device=None, # defaults to None, which uses the default tensor type\n",
    "                               requires_grad=False) # if True, operations perfromed on the tensor are recorded \n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhP8kzDfe_ty"
   },
   "source": [
    "shape 문제(텐서 shape가 맞지 않음) 외에도, PyTorch에서 마주칠 수 있는 다른 가장 흔한 문제 두 가지는 데이터 타입과 디바이스 문제입니다.\n",
    "\n",
    "예를 들어, 텐서 중 하나는 `torch.float32`이고 다른 하나는 `torch.float16`입니다(PyTorch는 종종 텐서가 동일한 형식이기를 선호합니다).\n",
    "\n",
    "또는 텐서 중 하나는 CPU에 있고 다른 하나는 GPU에 있습니다(PyTorch는 텐서 간 계산이 동일한 디바이스에서 이루어지기를 선호합니다).\n",
    "\n",
    "이러한 디바이스 이야기는 나중에 더 살펴보겠습니다.\n",
    "\n",
    "지금은 `dtype=torch.float16`으로 텐서를 생성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKSuajld_09s",
    "outputId": "cbac29d9-3371-4fe1-b47c-3af4623b5fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16) # torch.half would also work\n",
    "\n",
    "float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUjkB2AX7Upz"
   },
   "source": [
    "## 텐서에서 정보 가져오기\n",
    "\n",
    "텐서를 생성한 후(또는 다른 사람이나 PyTorch 모듈이 생성한 후), 텐서에서 일부 정보를 가져오고 싶을 수 있습니다.\n",
    "\n",
    "이전에 본 것들이지만 텐서에 대해 알아보고 싶은 가장 흔한 속성 세 가지는 다음과 같습니다:\n",
    "* `shape` - 텐서의 shape는 무엇인가? (일부 연산은 특정 shape 규칙을 요구함)\n",
    "* `dtype` - 텐서 내 요소들이 어떤 데이터 타입으로 저장되어 있는가?\n",
    "* `device` - 텐서가 어떤 디바이스에 저장되어 있는가? (보통 GPU 또는 CPU)\n",
    "\n",
    "무작위 텐서를 생성하고 그에 대한 세부사항을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd_X4D0j7Umq",
    "outputId": "86045713-ab36-4c8e-840c-e788f80c5266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7799, 0.8140, 0.0893, 0.2062],\n",
      "        [0.7525, 0.3845, 0.8207, 0.4587],\n",
      "        [0.9277, 0.8166, 0.9052, 0.0953]])\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "some_tensor = torch.rand(3, 4)\n",
    "\n",
    "# Find out details about it\n",
    "print(some_tensor)\n",
    "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45K-E5uPg6cj"
   },
   "source": [
    "> **참고:** PyTorch에서 문제가 발생할 때, 위의 세 속성 중 하나와 관련된 경우가 매우 많습니다. 따라서 오류 메시지가 나타날 때 \"무엇, 무엇, 어디\"라는 작은 노래를 불러보세요: \n",
    "  * \"*내 텐서들의 shape는 무엇인가? 데이터 타입은 무엇이고 어디에 저장되어 있는가? 어떤 shape, 어떤 데이터 타입, 어디 어디 어디*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdiWvoAi7UjL"
   },
   "source": [
    "## 텐서 조작 (텐서 연산)\n",
    "\n",
    "딥러닝에서 데이터(이미지, 텍스트, 비디오, 오디오, 단백질 구조 등)는 텐서로 표현됩니다.\n",
    "\n",
    "모델은 이러한 텐서를 조사하고 입력 데이터의 패턴 표현을 생성하기 위해 텐서에 대한 일련의 연산(수백만 개 이상일 수 있음)을 수행하여 학습합니다.\n",
    "\n",
    "이러한 연산은 종종 다음과 같은 멋진 춤입니다:\n",
    "* 덧셈\n",
    "* 뺄셈\n",
    "* 곱셈 (요소별)\n",
    "* 나눗셈\n",
    "* 행렬 곱셈\n",
    "\n",
    "그리고 그것이 전부입니다. 물론 여기저기에 몇 개 더 있지만 이것들이 신경망의 기본 구성 요소입니다.\n",
    "\n",
    "이러한 구성 요소를 올바른 방식으로 쌓으면 가장 정교한 신경망을 만들 수 있습니다(레고처럼!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk_6Dd7L7Uce"
   },
   "source": [
    "### 기본 연산\n",
    "\n",
    "기본적인 연산 몇 가지로 시작해보겠습니다: 덧셈(`+`), 뺄셈(`-`), 곱셈(`*`).\n",
    "\n",
    "예상하신 대로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X71WpQoPD7a4",
    "outputId": "ab30f13e-fc67-4ae4-c5ce-1006410dba07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of values and add a number to it\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp4TlTWWEFeO",
    "outputId": "ce7d2296-881f-4eb3-802e-fd12bc25d6ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply it by 10\n",
    "tensor * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1VEHnuRkn8Q"
   },
   "source": [
    "위의 텐서 값들이 `tensor([110, 120, 130])`가 되지 않은 것을 주목하세요. 이는 텐서 내부의 값들이 재할당되지 않는 한 변경되지 않기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuB1UjCIEJIA",
    "outputId": "57cae862-c145-4681-d74b-fe6d77f2125a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors don't change unless reassigned\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYvqGpUTk1o6"
   },
   "source": [
    "숫자를 빼고 이번에는 `tensor` 변수를 재할당해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4iWKoLsENry",
    "outputId": "14d6771d-eb57-4b11-88a7-b1bb308ddc6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract and reassign\n",
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFgZY-PaFNXa",
    "outputId": "3536ea54-a056-444c-cd5d-6d438ddda965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add and reassign\n",
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYXDoIOzk-6I"
   },
   "source": [
    "PyTorch에는 [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul) (곱셈의 줄임말)과 [`torch.add()`](https://pytorch.org/docs/stable/generated/torch.add.html)와 같은 기본 연산을 수행하는 내장 함수들이 많이 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVysdk3kFWbY",
    "outputId": "3a5bf687-cf24-4224-9e76-975f84638ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use torch functions\n",
    "torch.multiply(tensor, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxuPJIpNFbqO",
    "outputId": "f04cafd9-eaea-4254-df1a-5ab3b524d74e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original tensor is still unchanged \n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70UNL33AlVQq"
   },
   "source": [
    "하지만 `torch.mul()` 대신 `*`와 같은 연산자 기호를 사용하는 것이 더 일반적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5v3RkR0F2Jq",
    "outputId": "0137caab-5ea1-4d95-f4c5-a0baa0fd652d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5fVuyu7q5z"
   },
   "source": [
    "### 행렬 곱셈 (필요한 전부)\n",
    "\n",
    "머신러닝과 딥러닝 알고리즘(신경망과 같은)에서 가장 흔한 연산 중 하나는 [행렬 곱셈](https://www.mathsisfun.com/algebra/matrix-multiplying.html)입니다.\n",
    "\n",
    "PyTorch는 [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) 메서드에서 행렬 곱셈 기능을 구현합니다.\n",
    "\n",
    "행렬 곱셈에서 기억해야 할 주요 규칙 두 가지는 다음과 같습니다:\n",
    "1. **내부 차원**이 일치해야 합니다:\n",
    "  * `(3, 2) @ (3, 2)`는 작동하지 않습니다\n",
    "  * `(2, 3) @ (3, 2)`는 작동합니다\n",
    "  * `(3, 2) @ (2, 3)`는 작동합니다\n",
    "2. 결과 행렬은 **외부 차원**의 shape를 가집니다:\n",
    " * `(2, 3) @ (3, 2)` -> `(2, 2)`\n",
    " * `(3, 2) @ (2, 3)` -> `(3, 3)`\n",
    "\n",
    "> **참고:** Python에서 \"`@`\"는 행렬 곱셈의 기호입니다.\n",
    "\n",
    "> **자료:** `torch.matmul()`을 사용한 행렬 곱셈의 모든 규칙을 [PyTorch 문서](https://pytorch.org/docs/stable/generated/torch.matmul.html)에서 볼 수 있습니다.\n",
    "\n",
    "텐서를 생성하고 요소별 곱셈과 행렬 곱셈을 수행해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZE7loucmDlEM",
    "outputId": "44032bf9-c1f7-42fc-c842-dbe7a5c1221a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUAZ3_b0vOKv"
   },
   "source": [
    "요소별 곱셈과 행렬 곱셈의 차이점은 값의 덧셈입니다.\n",
    "\n",
    "값이 `[1, 2, 3]`인 `tensor` 변수에 대해:\n",
    "\n",
    "| 연산 | 계산 | 코드 |\n",
    "| ----- | ----- | ----- |\n",
    "| **요소별 곱셈** | `[1*1, 2*2, 3*3]` = `[1, 4, 9]` | `tensor * tensor` |\n",
    "| **행렬 곱셈** | `[1*1 + 2*2 + 3*3]` = `[14]` | `tensor.matmul(tensor)` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i42gkUeHvI_1",
    "outputId": "18a630ce-bb56-4c40-81b4-9fdbb2ed7a4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise matrix mutlication\n",
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvCBiiTTDk8y",
    "outputId": "cf623247-8f1b-49f1-e788-16da3ed1e59c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4E_pROBDk2r",
    "outputId": "a09af00f-277b-479e-b0a2-ad6311ee5413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use the \"@\" symbol for matrix multiplication, though not recommended\n",
    "tensor @ tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obbginUMv43A"
   },
   "source": [
    "수동으로 행렬 곱셈을 할 수 있지만 권장하지 않습니다.\n",
    "\n",
    "내장된 `torch.matmul()` 메서드가 더 빠릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qMSaLOoJscL",
    "outputId": "8bcad8a2-c900-4966-e13c-ff2cc02b9207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 µs, sys: 38 µs, total: 184 µs\n",
      "Wall time: 227 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Matrix multiplication by hand \n",
    "# (avoid doing operations with for loops at all cost, they are computationally expensive)\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVWiKB0KwH74",
    "outputId": "fce58235-5c09-49ec-f34b-a90e5640281e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 µs, sys: 7 µs, total: 34 µs\n",
      "Wall time: 36.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ4DDmo1TGe-"
   },
   "source": [
    "## 딥러닝에서 가장 흔한 오류 중 하나 (shape 오류)\n",
    "\n",
    "딥러닝의 많은 부분이 행렬 곱셈과 행렬에 대한 연산 수행이며, 행렬은 어떤 shape와 크기를 결합할 수 있는지에 대한 엄격한 규칙이 있기 때문에, 딥러닝에서 마주칠 가장 흔한 오류 중 하나는 shape 불일치입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rN5RcoD4Jo6y",
    "outputId": "20f6c65b-86f4-4903-d253-f6cbf0583934"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-aceec990e652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                          [9, 12]], dtype=torch.float32)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_B\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shape가 올바른 방식이어야 합니다  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNA6MZEFxWVt"
   },
   "source": [
    "내부 차원을 일치시켜 `tensor_A`와 `tensor_B` 간의 행렬 곱셈을 작동시킬 수 있습니다.\n",
    "\n",
    "이를 수행하는 방법 중 하나는 **전치(transpose)**입니다(주어진 텐서의 차원을 바꾸는 것).\n",
    "\n",
    "PyTorch에서 전치를 수행하는 방법은 다음과 같습니다:\n",
    "* `torch.transpose(input, dim0, dim1)` - 여기서 `input`은 전치하고자 하는 텐서이고 `dim0`과 `dim1`은 바꿀 차원입니다.\n",
    "* `tensor.T` - 여기서 `tensor`는 전치하고자 하는 텐서입니다.\n",
    "\n",
    "후자를 시도해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUqgaANiy1wq",
    "outputId": "e48bbf0c-8008-434e-d372-caa658b2f36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7., 10.],\n",
      "        [ 8., 11.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B\n",
    "print(tensor_A)\n",
    "print(tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DveqxO7iy_Fi",
    "outputId": "1bd2e85b-ea4d-4948-c408-8eb46ef3534c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B.T\n",
    "print(tensor_A)\n",
    "print(tensor_B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35rEIu-NKtVE",
    "outputId": "0b32c7f1-556e-45d4-de22-388419e93dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The operation works when tensor_B is transposed\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfcFEqfLjN24"
   },
   "source": [
    "`torch.matmul()`의 줄임말인 [`torch.mm()`](https://pytorch.org/docs/stable/generated/torch.mm.html)도 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3rJvW_TTGe_",
    "outputId": "2c501972-20bf-4a83-ad4a-b5f1b2424097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mm is a shortcut for matmul\n",
    "torch.mm(tensor_A, tensor_B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXKozI4T0hFi"
   },
   "source": [
    "전치 없이는 행렬 곱셈의 규칙이 충족되지 않아 위와 같은 오류가 발생합니다.\n",
    "\n",
    "시각적으로는 어떨까요? \n",
    "\n",
    "![visual demo of matrix multiplication](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/00-matrix-multiply-crop.gif)\n",
    "\n",
    "http://matrixmultiplication.xyz/에서 이와 같은 행렬 곱셈 시각화를 직접 만들 수 있습니다.\n",
    "\n",
    "> **참고:** 이와 같은 행렬 곱셈은 두 행렬의 [**내적(dot product)**](https://www.mathsisfun.com/algebra/vectors-dot-product.html)이라고도 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA64Z4DmkB31"
   },
   "source": [
    "신경망은 행렬 곱셈과 내적으로 가득합니다.\n",
    "\n",
    "[`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) 모듈(나중에 실제로 볼 예정)은 피드포워드 레이어 또는 완전 연결 레이어라고도 하며, 입력 `x`와 가중치 행렬 `A` 간의 행렬 곱셈을 구현합니다.\n",
    "\n",
    "$$\n",
    "y = x\\cdot{A^T} + b\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "* `x`는 레이어의 입력입니다(딥러닝은 `torch.nn.Linear()`와 같은 레이어들이 서로 위에 쌓인 스택입니다).\n",
    "* `A`는 레이어가 생성한 가중치 행렬로, 신경망이 데이터의 패턴을 더 잘 표현하도록 학습하면서 조정되는 무작위 숫자로 시작합니다(\"`T`\"를 주목하세요, 가중치 행렬이 전치되기 때문입니다).\n",
    "  * **참고:** 가중치 행렬을 보여주기 위해 `W`나 `X`와 같은 다른 문자를 자주 볼 수도 있습니다.\n",
    "* `b`는 가중치와 입력을 약간 오프셋하는 데 사용되는 편향 항입니다.\n",
    "* `y`는 출력입니다(패턴을 발견하기 위해 입력을 조작한 것).\n",
    "\n",
    "이는 선형 함수입니다(고등학교나 다른 곳에서 $y = mx+b$와 같은 것을 본 적이 있을 것입니다), 그리고 직선을 그리는 데 사용할 수 있습니다!\n",
    "\n",
    "선형 레이어를 가지고 놀아보겠습니다.\n",
    "\n",
    "아래의 `in_features`와 `out_features` 값을 변경해보고 무엇이 일어나는지 확인해보세요.\n",
    "\n",
    "shape와 관련된 것을 눈치채셨나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC_MjKW1LX7T",
    "outputId": "768f75d2-c978-4df3-e18a-4684d46bdfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 선형 레이어가 무작위 가중치 행렬로 시작하므로 재현 가능하게 만들어보겠습니다(나중에 더 자세히 다룰 예정)\n",
    "torch.manual_seed(42)\n",
    "# 이는 행렬 곱셈을 사용합니다\n",
    "linear = torch.nn.Linear(in_features=2, # in_features = 입력의 내부 차원과 일치\n",
    "                         out_features=6) # out_features = 외부 값을 설명\n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"입력 shape: {x.shape}\\n\")\n",
    "print(f\"출력:\\n{output}\\n\\n출력 shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIGrP5j1pN7j"
   },
   "source": [
    "> **질문:** 위에서 `in_features`를 2에서 3으로 변경하면 어떻게 될까요? 오류가 발생하나요? 입력(`x`)의 shape를 어떻게 변경하여 오류를 해결할 수 있나요? 힌트: 위에서 `tensor_B`에 대해 무엇을 했었나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPNF0nMWoGEj"
   },
   "source": [
    "이전에 해본 적이 없다면, 행렬 곱셈은 처음에 혼란스러운 주제일 수 있습니다.\n",
    "\n",
    "하지만 몇 번 가지고 놀아보고 심지어 몇 개의 신경망을 열어보면, 어디에나 있다는 것을 알게 될 것입니다.\n",
    "\n",
    "기억하세요, 행렬 곱셈이 필요한 전부입니다.\n",
    "\n",
    "![matrix multiplication is all you need](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_matrix_multiplication_is_all_you_need.jpeg)\n",
    "\n",
    "*신경망 레이어를 파헤치고 직접 구축하기 시작하면, 행렬 곱셈이 어디에나 있다는 것을 발견할 것입니다. **출처:** https://marksaroufim.substack.com/p/working-class-deep-learner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjMmrJOOPv5e"
   },
   "source": [
    "### 최솟값, 최댓값, 평균, 합계 등 찾기 (집계)\n",
    "\n",
    "이제 텐서를 조작하는 몇 가지 방법을 보았으니, 텐서를 집계하는 몇 가지 방법을 살펴보겠습니다(더 많은 값에서 더 적은 값으로).\n",
    "\n",
    "먼저 텐서를 생성한 다음 최댓값, 최솟값, 평균, 합계를 찾아보겠습니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrFQbe5fP1Rk",
    "outputId": "034013c1-b384-4a0d-edf8-295ed3a456f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(0, 100, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J-wfMdlsEco"
   },
   "source": [
    "이제 일부 집계를 수행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5wSP9YKP3Lb",
    "outputId": "3aa238c7-646f-434f-a55c-292aabef7227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0\n",
      "Maximum: 90\n",
      "Mean: 45.0\n",
      "Sum: 450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "# print(f\"Mean: {x.mean()}\") # this will error\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHoKpsg3sKQE"
   },
   "source": [
    "> **참고:** `torch.mean()`과 같은 일부 메서드는 텐서가 `torch.float32`(가장 일반적) 또는 다른 특정 데이터 타입이어야 하며, 그렇지 않으면 연산이 실패할 수 있습니다.\n",
    "\n",
    "`torch` 메서드로도 위와 동일한 작업을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cr23Y9uP3HO",
    "outputId": "9c86d805-eef2-465c-e2c8-2bccd515e6d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(0), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7ApCaZjDkvp"
   },
   "source": [
    "### 위치별 최솟값/최댓값\n",
    "\n",
    "[`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html)와 [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html)을 사용하여 최댓값 또는 최솟값이 발생하는 텐서의 인덱스를 찾을 수도 있습니다.\n",
    "\n",
    "이는 실제 값 자체가 아닌 가장 높은(또는 가장 낮은) 값이 있는 위치만 원할 때 유용합니다([softmax 활성화 함수](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)를 사용하는 나중 섹션에서 이를 볼 것입니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzNBl9JSGlHi",
    "outputId": "01e0740e-c34f-469b-9c8f-9e6e5f0363af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Index where max value occurs: 8\n",
      "Index where min value occurs: 0\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "tensor = torch.arange(10, 100, 10)\n",
    "print(f\"텐서: {tensor}\")\n",
    "\n",
    "# 최댓값과 최솟값의 인덱스 반환\n",
    "print(f\"최댓값이 발생하는 인덱스: {tensor.argmax()}\")\n",
    "print(f\"최솟값이 발생하는 인덱스: {tensor.argmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBu33WihOXBk"
   },
   "source": [
    "### 텐서 데이터 타입 변경\n",
    "\n",
    "앞서 언급했듯이, 딥러닝 연산에서 흔한 문제는 텐서들이 서로 다른 데이터 타입을 가지는 것입니다.\n",
    "\n",
    "하나의 텐서가 `torch.float64`이고 다른 하나가 `torch.float32`라면, 일부 오류가 발생할 수 있습니다.\n",
    "\n",
    "하지만 해결 방법이 있습니다.\n",
    "\n",
    "[`torch.Tensor.type(dtype=None)`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html)을 사용하여 텐서의 데이터 타입을 변경할 수 있습니다. 여기서 `dtype` 매개변수는 사용하고자 하는 데이터 타입입니다.\n",
    "\n",
    "먼저 텐서를 생성하고 데이터 타입을 확인해보겠습니다(기본값은 `torch.float32`입니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rY2FEsCAOaLu",
    "outputId": "507f1ade-7c7a-4172-fa48-60c9ac4831c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor and check its datatype\n",
    "tensor = torch.arange(10., 100., 10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR30FHEc92of"
   },
   "source": [
    "이제 이전과 동일한 텐서를 생성하지만 데이터 타입을 `torch.float16`으로 변경해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cac8gRYjOeab",
    "outputId": "96e5ce12-bc29-4a2b-f81c-bfc89ea2d075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a float16 tensor\n",
    "tensor_float16 = tensor.type(torch.float16)\n",
    "tensor_float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndVlKJZ4-7_5"
   },
   "source": [
    "`torch.int8` 텐서를 만들기 위해 비슷한 작업을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Yqovld2Oj6s",
    "outputId": "667da17f-e38f-404a-bd2d-63683e45c99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a int8 tensor\n",
    "tensor_int8 = tensor.type(torch.int8)\n",
    "tensor_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44GxVabar-xe"
   },
   "source": [
    "> **참고:** 다양한 데이터 타입이 처음에는 혼란스러울 수 있습니다. 하지만 이렇게 생각해보세요. 숫자가 낮을수록(예: 32, 16, 8) 컴퓨터가 값을 저장하는 정밀도가 낮아집니다. 그리고 저장 공간이 적으면 일반적으로 더 빠른 계산과 더 작은 전체 모델이 됩니다. 모바일 기반 신경망은 종종 8비트 정수로 작동하며, float32 대응물보다 작고 빠르게 실행되지만 정확도는 낮습니다. 이에 대한 자세한 내용은 [컴퓨팅의 정밀도](https://en.wikipedia.org/wiki/Precision_(computer_science))에 대해 읽어보시기 바랍니다.\n",
    "\n",
    "> **연습:** 지금까지 상당한 수의 텐서 메서드를 다뤘지만 [`torch.Tensor` 문서](https://pytorch.org/docs/stable/tensors.html)에 더 많은 것들이 있습니다. 10분간 스크롤하면서 눈에 띄는 것들을 살펴보는 것을 권장합니다. 클릭해서 직접 코드로 작성해보고 무엇이 일어나는지 확인해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CkCtAYmGsHY"
   },
   "source": [
    "### 형태 변경, 스택, 압축 및 압축 해제\n",
    "\n",
    "종종 텐서 내부의 값을 실제로 변경하지 않고 텐서의 형태를 변경하거나 차원을 바꾸고 싶을 때가 있습니다.\n",
    "\n",
    "이를 위한 몇 가지 인기 있는 메서드는 다음과 같습니다:\n",
    "\n",
    "| 메서드 | 한 줄 설명 |\n",
    "| ----- | ----- |\n",
    "| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | `input`을 `shape`로 형태 변경(호환 가능한 경우), `torch.Tensor.reshape()`도 사용 가능. |\n",
    "| [`torch.Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) | 원본 텐서와 같은 데이터를 공유하지만 다른 `shape`로 원본 텐서의 뷰를 반환. |\n",
    "| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html) | 새로운 차원(`dim`)을 따라 `tensors` 시퀀스를 연결, 모든 `tensors`는 같은 크기여야 함. |\n",
    "| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) | 값이 `1`인 모든 차원을 제거하기 위해 `input`을 압축. |\n",
    "| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html) | `dim`에 차원 값 `1`이 추가된 `input`을 반환. | \n",
    "| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html) | 차원이 `dims`로 순열(재배열)된 원본 `input`의 *뷰*를 반환. | \n",
    "\n",
    "왜 이런 것들을 해야 할까요?\n",
    "\n",
    "딥러닝 모델(신경망)은 모두 어떤 방식으로든 텐서를 조작하는 것에 관한 것이기 때문입니다. 그리고 행렬 곱셈의 규칙 때문에 shape 불일치가 있으면 오류가 발생합니다. 이러한 메서드들은 텐서의 올바른 요소들이 다른 텐서의 올바른 요소들과 섞이도록 도와줍니다.\n",
    "\n",
    "시도해보겠습니다.\n",
    "\n",
    "먼저 텐서를 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYjRTLOzG4Ev",
    "outputId": "f7f2719c-15ce-406b-dc8f-4477046cd5d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "import torch\n",
    "x = torch.arange(1., 8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_VarMO9CoT8"
   },
   "source": [
    "이제 `torch.reshape()`로 추가 차원을 추가해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US4WjpQ3SG-8",
    "outputId": "c519d59e-85f1-4a10-eaaa-acb487028e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add an extra dimension\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "x_reshaped, x_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tig5xm0jCxuU"
   },
   "source": [
    "`torch.view()`로 뷰를 변경할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDN2BNe5TGfB",
    "outputId": "3df1b0d6-2548-4ecc-ca25-0c4e28a6e536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change view (keeps same data as original but changes view)\n",
    "# See more: https://stackoverflow.com/a/54507446/7900723\n",
    "z = x.view(1, 7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8joAaUEC2NX"
   },
   "source": [
    "하지만 `torch.view()`로 텐서의 뷰를 변경하는 것은 실제로 *동일한* 텐서의 새로운 뷰만 생성한다는 것을 기억하세요.\n",
    "\n",
    "따라서 뷰를 변경하면 원본 텐서도 변경됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DxURVvXTGfC",
    "outputId": "668d194d-dd0a-4db1-da00-9c3fd8849186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing z changes x\n",
    "z[:, 0] = 5\n",
    "z, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxnqDBlpDDJ_"
   },
   "source": [
    "새로운 텐서를 네 번 스택하고 싶다면 `torch.stack()`으로 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pX5Adf3ORiTK",
    "outputId": "703e8568-61df-4ebd-f4d3-a6366dc265c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET56QzNHDuOI"
   },
   "source": [
    "텐서에서 모든 단일 차원을 제거하는 것은 어떨까요?\n",
    "\n",
    "이를 위해 `torch.squeeze()`를 사용할 수 있습니다(1보다 큰 차원만 가지도록 텐서를 *압축*하는 것으로 기억합니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2Y2HEoDRxJZ",
    "outputId": "dd0645a6-1cdd-46bc-a3a2-433d9cd09336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "Previous shape: torch.Size([1, 7])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "New shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "# Remove extra dimension from x_reshaped\n",
    "x_squeezed = x_reshaped.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acjDLk8WD8NC"
   },
   "source": [
    "`torch.squeeze()`의 반대 작업을 하려면 `torch.unsqueeze()`를 사용하여 특정 인덱스에 차원 값 1을 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUC-DEEwSYv7",
    "outputId": "da60e019-3ea6-42f8-8e47-ba037ead737f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Previous shape: torch.Size([7])\n",
      "\n",
      "New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "New shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_squeezed}\")\n",
    "print(f\"Previous shape: {x_squeezed.shape}\")\n",
    "\n",
    "## Add an extra dimension with unsqueeze\n",
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"\\nNew tensor: {x_unsqueezed}\")\n",
    "print(f\"New shape: {x_unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9DuJzXgFbM5"
   },
   "source": [
    "`torch.permute(input, dims)`로 축 값의 순서를 재배열할 수도 있습니다. 여기서 `input`은 새로운 `dims`로 *뷰*가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCRGCX8DTGfC",
    "outputId": "6853328b-a1cf-4470-f366-106a231a189c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor with specific shape\n",
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "# Permute the original tensor to rearrange the axis order\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06LKaFemGBoE"
   },
   "source": [
    "> **참고**: 순열은 *뷰*를 반환하기 때문에(원본과 같은 데이터를 공유), 순열된 텐서의 값은 원본 텐서와 동일하며 뷰의 값을 변경하면 원본의 값도 변경됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEPqVL7fTGfC"
   },
   "source": [
    "## 인덱싱 (텐서에서 데이터 선택)\n",
    "\n",
    "때로는 텐서에서 특정 데이터를 선택하고 싶을 때가 있습니다(예: 첫 번째 열이나 두 번째 행만).\n",
    "\n",
    "이를 위해 인덱싱을 사용할 수 있습니다.\n",
    "\n",
    "Python 리스트나 NumPy 배열에서 인덱싱을 해본 적이 있다면, PyTorch에서 텐서로 인덱싱하는 것은 매우 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSXzdxCQTGfD",
    "outputId": "05a72c08-5f8c-433a-cd31-46065686f825"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]), torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor \n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQG5krnKG43B"
   },
   "source": [
    "값 인덱싱은 외부 차원 -> 내부 차원으로 갑니다(대괄호를 확인해보세요)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zv_Z3IAzTGfD",
    "outputId": "cf6c0936-7600-4af4-9b6f-f6b8ac9b4c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First square bracket:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Second square bracket: tensor([1, 2, 3])\n",
      "Third square bracket: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's index bracket by bracket\n",
    "print(f\"First square bracket:\\n{x[0]}\") \n",
    "print(f\"Second square bracket: {x[0][0]}\") \n",
    "print(f\"Third square bracket: {x[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaLjaIFxHe89"
   },
   "source": [
    "`:`를 사용하여 \"이 차원의 모든 값\"을 지정한 다음 쉼표(`,`)를 사용하여 다른 차원을 추가할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCT09pqeTGfD",
    "outputId": "a91f9b73-f8f0-476a-9c69-fcd03b042f6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwDx_gMsTGfD",
    "outputId": "8165cfd9-a88d-4212-8c45-1eb84ef5be83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiw3_1E3TGfD",
    "outputId": "12fa4749-cf52-4e88-c2c0-44d26aeb633c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFVEgrKhTGfD",
    "outputId": "69eadeb9-11b3-4b48-cb95-0b3305c1274c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ik0r11RIxtm"
   },
   "source": [
    "인덱싱은 처음에 꽤 혼란스러울 수 있으며, 특히 더 큰 텐서에서는 더욱 그렇습니다(저도 올바르게 하기 위해 여러 번 시도해야 합니다). 하지만 약간의 연습과 데이터 탐색가의 모토(***시각화, 시각화, 시각화***)를 따르면 익숙해질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8ZaW0Bq7rCm"
   },
   "source": [
    "## PyTorch 텐서와 NumPy\n",
    "\n",
    "NumPy는 인기 있는 Python 수치 계산 라이브러리이므로, PyTorch는 NumPy와 잘 상호작용할 수 있는 기능을 가지고 있습니다.\n",
    "\n",
    "NumPy에서 PyTorch로(그리고 다시 돌아가기) 사용하고 싶은 두 가지 주요 메서드는 다음과 같습니다:\n",
    "* [`torch.from_numpy(ndarray)`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - NumPy 배열 -> PyTorch 텐서.\n",
    "* [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) - PyTorch 텐서 -> NumPy 배열.\n",
    "\n",
    "시도해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDrDCnvY7rKS",
    "outputId": "86155a63-01f9-4372-e889-61a65ebf0fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy array to tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16JG6cONLPnO"
   },
   "source": [
    "> **참고:** 기본적으로 NumPy 배열은 데이터 타입 `float64`로 생성되며, PyTorch 텐서로 변환하면 동일한 데이터 타입을 유지합니다(위와 같이).\n",
    ">\n",
    "> 하지만 많은 PyTorch 계산은 기본적으로 `float32`를 사용합니다.\n",
    "> \n",
    "> 따라서 NumPy 배열(float64) -> PyTorch 텐서(float64) -> PyTorch 텐서(float32)로 변환하고 싶다면 `tensor = torch.from_numpy(array).type(torch.float32)`를 사용할 수 있습니다.\n",
    "\n",
    "위에서 `tensor`를 재할당했기 때문에, 텐서를 변경해도 배열은 동일하게 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovwl7VCREv8L",
    "outputId": "efd21eb9-0010-436a-dc29-f851e3d7d77a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the array, keep the tensor\n",
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geVvu1p0MTWc"
   },
   "source": [
    "PyTorch 텐서에서 NumPy 배열로 가고 싶다면 `tensor.numpy()`를 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xw_7ZyVaTKxQ",
    "outputId": "54d6f347-d3f6-44df-9155-83d980c31780"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to NumPy array\n",
    "tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n",
    "numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt8yEV1jMfi2"
   },
   "source": [
    "위와 동일한 규칙이 적용되어, 원본 `tensor`를 변경해도 새로운 `numpy_tensor`는 동일하게 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMp6ZSkET4_Y",
    "outputId": "100678a4-c220-4a44-e4a5-0542359cb9de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the tensor, keep the array the same\n",
    "tensor = tensor + 1\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gU3ubCrUkI-"
   },
   "source": [
    "## 재현성 (랜덤에서 랜덤을 제거하려고 시도)\n",
    "\n",
    "신경망과 머신러닝에 대해 더 많이 배우면서, 랜덤성이 얼마나 중요한 역할을 하는지 발견하게 될 것입니다.\n",
    "\n",
    "정확히는 의사랜덤성입니다. 결국 설계된 대로 컴퓨터는 근본적으로 결정론적이기 때문에(각 단계가 예측 가능), 생성하는 랜덤성은 시뮬레이션된 랜덤성입니다(이에 대한 논쟁도 있지만, 저는 컴퓨터 과학자가 아니므로 더 자세한 내용은 직접 찾아보시기 바랍니다).\n",
    "\n",
    "그렇다면 이것이 신경망과 딥러닝과 어떤 관련이 있을까요?\n",
    "\n",
    "신경망이 데이터의 패턴을 설명하기 위해 랜덤 숫자로 시작한다고 논의했습니다(이 숫자들은 좋지 않은 설명입니다). 그리고 텐서 연산(아직 논의하지 않은 몇 가지 다른 것들)을 사용하여 이러한 랜덤 숫자를 개선하여 데이터의 패턴을 더 잘 설명하려고 시도합니다.\n",
    "\n",
    "간단히 말하면:\n",
    "\n",
    "``랜덤 숫자로 시작 -> 텐서 연산 -> 더 좋게 만들려고 시도 (계속 반복)``\n",
    "\n",
    "랜덤성은 좋고 강력하지만, 때로는 랜덤성이 조금 덜했으면 좋겠을 때가 있습니다.\n",
    "\n",
    "왜일까요?\n",
    "\n",
    "반복 가능한 실험을 수행할 수 있기 때문입니다.\n",
    "\n",
    "예를 들어, X 성능을 달성할 수 있는 알고리즘을 만듭니다.\n",
    "\n",
    "그리고 친구가 당신이 미치지 않았는지 확인하기 위해 시도해봅니다.\n",
    "\n",
    "그들은 어떻게 그런 일을 할 수 있을까요?\n",
    "\n",
    "바로 **재현성**이 필요한 곳입니다.\n",
    "\n",
    "다시 말해, 같은 코드를 실행했을 때 내 컴퓨터에서 얻는 것과 동일한(또는 매우 유사한) 결과를 당신의 컴퓨터에서 얻을 수 있을까요?\n",
    "\n",
    "PyTorch에서 재현성의 간단한 예를 살펴보겠습니다.\n",
    "\n",
    "두 개의 무작위 텐서를 생성하는 것으로 시작하겠습니다. 무작위이므로 다를 것으로 예상하시겠죠? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSwxnwEbTGfF",
    "outputId": "73b34154-734f-496f-9b55-b6aaa137e854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8016, 0.3649, 0.6286, 0.9663],\n",
      "        [0.7687, 0.4566, 0.5745, 0.9200],\n",
      "        [0.3230, 0.8613, 0.0919, 0.3102]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.9536, 0.6002, 0.0351, 0.6826],\n",
      "        [0.3743, 0.5220, 0.1336, 0.9666],\n",
      "        [0.9754, 0.8474, 0.8988, 0.1105]])\n",
      "\n",
      "Does Tensor A equal Tensor B? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two random tensors\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPU6mDKJnr8M"
   },
   "source": [
    "예상하신 대로, 텐서들은 서로 다른 값으로 나옵니다.\n",
    "\n",
    "하지만 *동일한* 값을 가진 두 개의 무작위 텐서를 생성하고 싶다면 어떨까요?\n",
    "\n",
    "즉, 텐서는 여전히 무작위 값을 포함하지만 같은 맛을 가질 것입니다.\n",
    "\n",
    "바로 [`torch.manual_seed(seed)`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html)가 필요한 곳입니다. 여기서 `seed`는 무작위성에 맛을 더하는 정수(예: `42`이지만 무엇이든 될 수 있음)입니다.\n",
    "\n",
    "더 *맛있는* 무작위 텐서를 생성하여 시도해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sB6d1GfYTGfF",
    "outputId": "4d11d38e-4406-4aff-9a81-cf13aa89ee5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# # Set the random seed\n",
    "RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n",
    "torch.manual_seed(seed=RANDOM_SEED) \n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# Have to reset the seed every time a new rand() is called \n",
    "# Without this, tensor_D would be different to tensor_C \n",
    "torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uct53Xr5QRC_"
   },
   "source": [
    "훌륭합니다!\n",
    "\n",
    "시드를 설정하는 것이 작동한 것 같습니다.\n",
    "\n",
    "> **자료:** 방금 다룬 내용은 PyTorch에서 재현성의 표면만 긁어 본 것입니다. 재현성 일반과 랜덤 시드에 대한 더 자세한 내용은 다음을 확인해보세요:\n",
    "> * [PyTorch 재현성 문서](https://pytorch.org/docs/stable/notes/randomness.html) (좋은 연습은 10분간 이 내용을 읽어보는 것이며, 지금 이해하지 못하더라도 알고 있는 것이 중요합니다).\n",
    "> * [Wikipedia 랜덤 시드 페이지](https://en.wikipedia.org/wiki/Random_seed) (이것은 랜덤 시드와 의사랜덤성에 대한 좋은 개요를 제공합니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxIIM7t27rQ-"
   },
   "source": [
    "## GPU에서 텐서 실행 (더 빠른 계산 만들기)\n",
    "\n",
    "딥러닝 알고리즘은 많은 수치 연산을 필요로 합니다.\n",
    "\n",
    "그리고 기본적으로 이러한 연산은 종종 CPU(컴퓨터 처리 장치)에서 수행됩니다.\n",
    "\n",
    "하지만 GPU(그래픽 처리 장치)라고 불리는 또 다른 일반적인 하드웨어가 있으며, 이는 신경망이 필요로 하는 특정 유형의 연산(행렬 곱셈)을 CPU보다 훨씬 빠르게 수행하는 경우가 많습니다.\n",
    "\n",
    "여러분의 컴퓨터에도 하나가 있을 수 있습니다.\n",
    "\n",
    "만약 있다면, 신경망을 학습할 때마다 사용해야 합니다. 학습 시간을 극적으로 단축시킬 가능성이 높기 때문입니다.\n",
    "\n",
    "먼저 GPU에 액세스하고 두 번째로 PyTorch가 GPU를 사용하도록 하는 몇 가지 방법이 있습니다.\n",
    "\n",
    "> **참고:** 이 코스에서 \"GPU\"를 언급할 때는 별도로 명시하지 않는 한 [CUDA가 활성화된 Nvidia GPU](https://developer.nvidia.com/cuda-gpus)를 참조합니다(CUDA는 GPU가 그래픽뿐만 아니라 범용 컴퓨팅에도 사용될 수 있도록 도와주는 컴퓨팅 플랫폼 및 API입니다).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UiR6QpoYQH_"
   },
   "source": [
    "\n",
    "### 1. GPU 얻기\n",
    "\n",
    "GPU라고 할 때 무슨 일이 일어나는지 이미 알고 계실 수도 있습니다. 하지만 그렇지 않다면, GPU에 액세스하는 몇 가지 방법이 있습니다.\n",
    "\n",
    "| **방법** | **설정 난이도** | **장점** | **단점** | **설정 방법** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| Google Colab | 쉬움 | 무료 사용, 거의 제로 설정 필요, 링크만으로 다른 사람과 작업 공유 가능 | 데이터 출력을 저장하지 않음, 제한된 컴퓨팅, 타임아웃 대상 | [Google Colab 가이드 따르기](https://colab.research.google.com/notebooks/gpu.ipynb) |\n",
    "| 자체 사용 | 보통 | 자신의 컴퓨터에서 모든 것을 로컬로 실행 | GPU는 무료가 아님, 초기 비용 필요 | [PyTorch 설치 가이드라인](https://pytorch.org/get-started/locally/) 따르기 |\n",
    "| 클라우드 컴퓨팅 (AWS, GCP, Azure) | 보통-어려움 | 적은 초기 비용, 거의 무한한 컴퓨팅 액세스 | 지속적으로 실행하면 비용이 많이 들 수 있음, 올바른 설정에 시간이 걸림 | [PyTorch 설치 가이드라인](https://pytorch.org/get-started/cloud-partners/) 따르기 |\n",
    "\n",
    "GPU를 사용하는 더 많은 옵션이 있지만 위의 세 가지가 지금은 충분할 것입니다.\n",
    "\n",
    "개인적으로 저는 소규모 실험(그리고 이 코스 만들기)에는 Google Colab과 개인 컴퓨터의 조합을 사용하고, 더 많은 컴퓨팅 파워가 필요할 때는 클라우드 리소스를 이용합니다.\n",
    "\n",
    "> **자료:** 자체 GPU를 구매하려고 하지만 무엇을 얻어야 할지 확실하지 않다면, [Tim Dettmers의 훌륭한 가이드](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)가 있습니다.\n",
    "\n",
    "Nvidia GPU에 액세스할 수 있는지 확인하려면 `!nvidia-smi`를 실행할 수 있습니다. 여기서 `!`(bang이라고도 함)는 \"명령줄에서 이것을 실행\"을 의미합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEMcO-9zYc-w",
    "outputId": "77405db7-3494-4add-cfc7-8415e52a0412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 10 02:09:18 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvkB9p5zYf8E"
   },
   "source": [
    "Nvidia GPU에 액세스할 수 없다면, 위의 명령은 다음과 같은 것을 출력할 것입니다:\n",
    "\n",
    "```\n",
    "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
    "```\n",
    "\n",
    "그런 경우에는 위로 돌아가서 설치 단계를 따르세요.\n",
    "\n",
    "GPU가 있다면, 위의 명령은 다음과 같은 것을 출력할 것입니다:\n",
    "\n",
    "```\n",
    "Wed Jan 19 22:09:08 2022       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvibZ6e0YcDk"
   },
   "source": [
    "\n",
    "\n",
    "### 2. PyTorch가 GPU에서 실행되도록 설정\n",
    "\n",
    "GPU에 액세스할 준비가 되면, 다음 단계는 PyTorch가 데이터 저장(텐서)과 데이터 계산(텐서에 대한 연산 수행)에 사용하도록 하는 것입니다.\n",
    "\n",
    "이를 위해 [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) 패키지를 사용할 수 있습니다.\n",
    "\n",
    "이에 대해 이야기하는 대신, 시도해보겠습니다.\n",
    "\n",
    "[`torch.cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available)을 사용하여 PyTorch가 GPU에 액세스할 수 있는지 테스트할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OweDLgwjEvZ2",
    "outputId": "3a278a24-3ec3-4b1f-8f96-298086fa6ea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jedZcx2PZFpL"
   },
   "source": [
    "위에서 `True`를 출력하면 PyTorch가 GPU를 보고 사용할 수 있다는 의미이고, `False`를 출력하면 GPU를 볼 수 없으며 그런 경우에는 설치 단계를 다시 거쳐야 합니다.\n",
    "\n",
    "이제 코드가 CPU *또는* 사용 가능한 경우 GPU에서 실행되도록 설정하고 싶다고 가정해보겠습니다.\n",
    "\n",
    "그렇게 하면 여러분이나 다른 사람이 코드를 실행하기로 결정하더라도 사용하는 컴퓨팅 디바이스에 관계없이 작동할 것입니다.\n",
    "\n",
    "사용 가능한 디바이스 유형을 저장할 `device` 변수를 생성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j92HBCKB7rYa",
    "outputId": "8cca1643-645c-4b67-f1f5-37066f6b9549"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjFyPP2WaCch"
   },
   "source": [
    "위에서 `\"cuda\"`를 출력하면 사용 가능한 CUDA 디바이스(GPU)를 사용하도록 모든 PyTorch 코드를 설정할 수 있다는 의미이고, `\"cpu\"`를 출력하면 PyTorch 코드가 CPU를 계속 사용한다는 의미입니다.\n",
    "\n",
    "> **참고:** PyTorch에서는 [**디바이스 무관 코드**](https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code)를 작성하는 것이 모범 사례입니다. 이는 CPU(항상 사용 가능) 또는 GPU(사용 가능한 경우)에서 실행될 코드를 의미합니다.\n",
    "\n",
    "더 빠른 계산을 원한다면 GPU를 사용할 수 있지만, *훨씬* 빠른 계산을 원한다면 여러 GPU를 사용할 수 있습니다.\n",
    "\n",
    "[`torch.cuda.device_count()`](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count)를 사용하여 PyTorch가 액세스할 수 있는 GPU의 개수를 셀 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MArsn0DFTGfG",
    "outputId": "de717df5-bb67-4900-805e-a6f00ad0b409"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of devices\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVNf1hiqa-gO"
   },
   "source": [
    "PyTorch가 액세스할 수 있는 GPU의 개수를 아는 것은 하나의 GPU에서 특정 프로세스를 실행하고 다른 GPU에서 다른 프로세스를 실행하고 싶을 때 유용합니다(PyTorch는 *모든* GPU에 걸쳐 프로세스를 실행할 수 있는 기능도 있습니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqQLcuj68OA-"
   },
   "source": [
    "### 3. 텐서(및 모델)를 GPU에 배치\n",
    "\n",
    "[`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html)를 호출하여 텐서(및 모델, 나중에 볼 예정)를 특정 디바이스에 배치할 수 있습니다. 여기서 `device`는 텐서(또는 모델)가 이동할 대상 디바이스입니다.\n",
    "\n",
    "왜 이렇게 할까요?\n",
    "\n",
    "GPU는 CPU보다 훨씬 빠른 수치 계산을 제공하며, GPU를 사용할 수 없는 경우 **디바이스 무관 코드**(위 참조) 때문에 CPU에서 실행됩니다.\n",
    "\n",
    "> **참고:** `to(device)`를 사용하여 텐서를 GPU에 배치하는 것(예: `some_tensor.to(device)`)은 해당 텐서의 복사본을 반환합니다. 즉, 동일한 텐서가 CPU와 GPU에 모두 있을 것입니다. 텐서를 덮어쓰려면 재할당하세요:\n",
    ">\n",
    "> `some_tensor = some_tensor.to(device)`\n",
    "\n",
    "텐서를 생성하고 GPU에 배치해보겠습니다(사용 가능한 경우)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhI3srFXEHfP",
    "outputId": "2f4f6435-fdc4-4e99-e87c-9421c2100f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor (default on CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxXeRKO0TGfG"
   },
   "source": [
    "GPU를 사용할 수 있다면, 위의 코드는 다음과 같은 것을 출력할 것입니다:\n",
    "\n",
    "```\n",
    "tensor([1, 2, 3]) cpu\n",
    "tensor([1, 2, 3], device='cuda:0')\n",
    "```\n",
    "\n",
    "두 번째 텐서에 `device='cuda:0'`이 있는 것을 주목하세요. 이는 사용 가능한 0번째 GPU에 저장되어 있다는 의미입니다(GPU는 0부터 인덱싱되며, 두 개의 GPU가 사용 가능하다면 각각 `'cuda:0'`과 `'cuda:1'`이 될 것입니다. 최대 `'cuda:n'`까지).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4puyUX4Bci5D"
   },
   "source": [
    "### 4. 텐서를 CPU로 다시 이동\n",
    "\n",
    "텐서를 CPU로 다시 이동하고 싶다면 어떨까요?\n",
    "\n",
    "예를 들어, NumPy와 함께 텐서와 상호작용하고 싶다면 이렇게 하고 싶을 것입니다(NumPy는 GPU를 활용하지 않습니다).\n",
    "\n",
    "`tensor_on_gpu`에서 [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) 메서드를 사용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "3ChSLJgPTGfG",
    "outputId": "32e92f62-db28-4dc7-ce93-c2ab33229252"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-53175578f49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhymtkRDTGfG"
   },
   "source": [
    "대신, 텐서를 CPU로 다시 가져와서 NumPy와 함께 사용할 수 있도록 [`Tensor.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html)를 사용할 수 있습니다.\n",
    "\n",
    "이것은 텐서를 CPU 메모리로 복사하여 CPU와 함께 사용할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN15s-NdTGfG",
    "outputId": "9fffb6f2-c200-4f9c-d987-d9ab5d9cba49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyzNH5lrTGfH"
   },
   "source": [
    "위의 코드는 GPU 텐서의 복사본을 CPU 메모리에 반환하므로 원본 텐서는 여전히 GPU에 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5u83PCRTGfH",
    "outputId": "4cb931e2-7c8d-49b9-a7de-db3d3c6589b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlmBpnuPTGfH"
   },
   "source": [
    "## 연습 문제\n",
    "\n",
    "1. 문서 읽기 - 딥러닝(그리고 일반적으로 코딩 학습)의 큰 부분은 사용하는 특정 프레임워크의 문서에 익숙해지는 것입니다. 이 코스의 나머지 부분에서 PyTorch 문서를 많이 사용할 것입니다. 따라서 다음을 10분간 읽어보는 것을 권장합니다(지금 일부 내용을 이해하지 못해도 괜찮습니다. 완전한 이해가 목표가 아니라 인식이 목표입니다):\n",
    "  * [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) 문서.\n",
    "  * [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics) 문서.\n",
    "2. shape가 `(7, 7)`인 무작위 텐서를 생성하세요.\n",
    "3. 2번의 텐서와 shape가 `(1, 7)`인 다른 무작위 텐서로 행렬 곱셈을 수행하세요(힌트: 두 번째 텐서를 전치해야 할 수도 있습니다).\n",
    "4. 랜덤 시드를 `0`으로 설정하고 2번과 3번을 다시 수행하세요. 출력은 다음과 같아야 합니다:\n",
    "\n",
    "```\n",
    "(tensor([[1.8542],\n",
    "         [1.9611],\n",
    "         [2.2884],\n",
    "         [3.0481],\n",
    "         [1.7067],\n",
    "         [2.5290],\n",
    "         [1.7989]]), torch.Size([7, 1]))\n",
    "```\n",
    "\n",
    "5. 랜덤 시드에 대해 말하자면, `torch.manual_seed()`로 설정하는 방법을 보았지만 GPU에 해당하는 것이 있나요? (힌트: 이에 대해서는 `torch.cuda` 문서를 살펴봐야 합니다)\n",
    "  * 만약 있다면, GPU 랜덤 시드를 `1234`로 설정하세요.\n",
    "6. shape가 `(2, 3)`인 두 개의 무작위 텐서를 생성하고 둘 다 GPU로 보내세요(이를 위해서는 GPU 액세스가 필요합니다). 텐서를 생성할 때 `torch.manual_seed(1234)`를 설정하세요(이것은 GPU 랜덤 시드일 필요는 없습니다). 출력은 다음과 같아야 합니다:\n",
    "\n",
    "```\n",
    "Device: cuda\n",
    "(tensor([[0.0290, 0.4019, 0.2598],\n",
    "         [0.3666, 0.0583, 0.7006]], device='cuda:0'),\n",
    " tensor([[0.0518, 0.4681, 0.6738],\n",
    "         [0.3315, 0.7837, 0.5631]], device='cuda:0'))\n",
    "```\n",
    "7. 6번에서 생성한 텐서들로 행렬 곱셈을 수행하세요(다시 한번, 텐서 중 하나의 shape를 조정해야 할 수도 있습니다).\n",
    "8. 7번 출력의 최댓값과 최솟값을 찾으세요.\n",
    "9. 7번 출력의 최댓값과 최솟값 인덱스를 찾으세요.\n",
    "10. shape가 `(1, 1, 1, 10)`인 무작위 텐서를 만들고 모든 `1` 차원이 제거된 shape가 `(10)`인 텐서를 생성하세요. 생성할 때 시드를 `7`로 설정하고 첫 번째 텐서와 그 shape, 두 번째 텐서와 그 shape를 출력하세요. 출력은 다음과 같아야 합니다:\n",
    "\n",
    "```\n",
    "tensor([[[[0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297,\n",
    "           0.3653, 0.8513]]]]) torch.Size([1, 1, 1, 10])\n",
    "tensor([0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297, 0.3653,\n",
    "        0.8513]) torch.Size([10])\n",
    "```\n",
    "\n",
    "> **자료:** 이 연습문제를 완료하려면 코스 GitHub의 [연습 노트북 템플릿](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/exercises)과 가능한 [해설/해답](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)을 참고하세요.\n",
    "\n",
    "## 추가 학습\n",
    "\n",
    "* [PyTorch 기초 튜토리얼](https://pytorch.org/tutorials/beginner/basics/intro.html)을 1시간 정도 살펴보세요(특히 [Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)와 [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) 섹션을 권장합니다).\n",
    "* 텐서가 데이터를 어떻게 표현하는지 더 알고 싶다면 다음 영상을 참고하세요: [What's a tensor?](https://youtu.be/f5liqUk0ZTw)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "00_pytorch_fundamentals.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
